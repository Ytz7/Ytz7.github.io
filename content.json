{"meta":{"title":"七号zz の Blog","subtitle":"愿为未来做牛马。","description":"废材研究生🤦‍♂️","author":"七号zz","url":"https://Ytz7.github.io","root":"/"},"pages":[{"title":"","date":"2023-05-12T13:56:54.059Z","updated":"2023-05-12T13:56:54.059Z","comments":true,"path":"manifest.json","permalink":"https://ytz7.github.io/manifest.json","excerpt":"","text":"{\"name\":\"七号zz の Blog\",\"short_name\":\"七号zz\",\"theme_color\":\"#eedeab\",\"background_color\":\"#eedeab\",\"display\":\"standalone\",\"scope\":\"/\",\"start_url\":\"/\",\"icons\":[{\"src\":\"/img/siteicon/16.png\",\"sizes\":\"16x16\",\"type\":\"image/png\"},{\"src\":\"/img/siteicon/32.png\",\"sizes\":\"32x32\",\"type\":\"image/png\"},{\"src\":\"/img/siteicon/48.png\",\"sizes\":\"48x48\",\"type\":\"image/png\"},{\"src\":\"/img/siteicon/64.png\",\"sizes\":\"64x64\",\"type\":\"image/png\"},{\"src\":\"/img/siteicon/128.png\",\"sizes\":\"128x128\",\"type\":\"image/png\"},{\"src\":\"/img/siteicon/144.png\",\"sizes\":\"144x144\",\"type\":\"image/png\"},{\"src\":\"/img/siteicon/512.png\",\"sizes\":\"512x512\",\"type\":\"image/png\"}],\"splash_pages\":null}"},{"title":"about","date":"2023-04-24T14:13:26.000Z","updated":"2023-05-12T13:56:54.059Z","comments":true,"path":"about/index.html","permalink":"https://ytz7.github.io/about/index.html","excerpt":"","text":"关于我一个废材研究生ISFJ/守护者让我在抑郁的边缘徘徊"},{"title":"分类","date":"2023-04-24T14:13:14.000Z","updated":"2023-05-12T13:56:54.059Z","comments":true,"path":"categories/index.html","permalink":"https://ytz7.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2023-05-12T13:56:54.059Z","updated":"2023-05-12T13:56:54.059Z","comments":true,"path":"js/ali_font.js","permalink":"https://ytz7.github.io/js/ali_font.js","excerpt":"","text":"!(function (c) { var l, h, a, t, i, v = '', o = (o = document.getElementsByTagName(\"script\"))[o.length - 1].getAttribute(\"data-injectcss\"), p = function (c, l) { l.parentNode.insertBefore(c, l); }; if (o &amp;&amp; !c.__iconfont__svg__cssinject__) { c.__iconfont__svg__cssinject__ = !0; try { document.write( \".svgfont {display: inline-block;width: 1em;height: 1em;fill: currentColor;vertical-align: -0.1em;font-size:16px;}\" ); } catch (c) { console &amp;&amp; console.log(c); } } function d() { i || ((i = !0), a()); } function m() { try { t.documentElement.doScroll(\"left\"); } catch (c) { return void setTimeout(m, 50); } d(); } (l = function () { var c, l = document.createElement(\"div\"); (l.innerHTML = v), (v = null), (l = l.getElementsByTagName(\"svg\")[0]) &amp;&amp; (l.setAttribute(\"aria-hidden\", \"true\"), (l.style.position = \"absolute\"), (l.style.width = 0), (l.style.height = 0), (l.style.overflow = \"hidden\"), (l = l), (c = document.body).firstChild ? p(l, c.firstChild) : c.appendChild(l)); }), document.addEventListener ? ~[\"complete\", \"loaded\", \"interactive\"].indexOf(document.readyState) ? setTimeout(l, 0) : ((h = function () { document.removeEventListener(\"DOMContentLoaded\", h, !1), l(); }), document.addEventListener(\"DOMContentLoaded\", h, !1)) : document.attachEvent &amp;&amp; ((a = l), (t = c.document), (i = !1), m(), (t.onreadystatechange = function () { \"complete\" == t.readyState &amp;&amp; ((t.onreadystatechange = null), d()); })); })(window);"},{"title":"友情链接","date":"2023-04-24T14:25:31.000Z","updated":"2023-05-12T13:56:54.059Z","comments":true,"path":"link/index.html","permalink":"https://ytz7.github.io/link/index.html","excerpt":"","text":""},{"title":"标签","date":"2023-04-24T14:12:42.000Z","updated":"2023-05-12T13:56:54.059Z","comments":true,"path":"tags/index.html","permalink":"https://ytz7.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hello World","slug":"hello-world","date":"2023-05-12T13:56:54.059Z","updated":"2023-05-12T13:56:54.059Z","comments":true,"path":"posts/16107.html","link":"","permalink":"https://ytz7.github.io/posts/16107.html","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"关于深度学习中遇到的知识盲区","slug":"deep-learning-some-questions","date":"2023-05-11T11:47:01.000Z","updated":"2023-05-12T13:56:54.059Z","comments":true,"path":"posts/eaa3f406.html","link":"","permalink":"https://ytz7.github.io/posts/eaa3f406.html","excerpt":"","text":"关于深度学习的一些问题与知识盲区此篇用于记录在阅读论文以及博客时遇到的一些问题和知识盲区，方便日后进行复盘。 激活函数为什么需要激活函数？通常激活函数都是非线性的，它能够帮助我们引入非线性因素，使得神经网络能够更好地解决更加复杂地问题，在简单的二分类问题中，如果不使用激活函数，使用简单的**逻辑回归**，那么该模型只能作简单的线性分类，而不能作复杂的非线性划分，如下图所示： 值得一提的是，如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，效果与单个神经元无异。另外，如果是拟合问题而不是分类问题，输出层的激活函数可以使用线性函数。 Sigmoid 函数饱和带来的问题Sigmoid 函数的取值范围在 (0,1) 之间，单调连续，求导容易，一般用于二分类神经网络的输出层。 Sigmoid 函数饱和区范围广，容易造成梯度消失。饱和区如下图所示，图中红色椭圆标注的饱和区曲线平缓，梯度的值很小，近似为零，且 Sigmoid 函数的饱和区范围很广，除了 [-5,5]，其余区域都可以认为是饱和区，这种情况很容易造成梯度消失，梯度消失会增大神经网络训练难度，影响神经网络模型的性能。 降噪自编码器 Denoising Auto-Encoder在神经网络模型训练阶段开始前，通过 Auto-encoder 对模型进行预训练可确定编码器 W 的初始参数值。然而，受模型复杂度、训练集数据量以及数据噪音等问题的影响，通过 Auto-encoder 得到的初始模型往往存在过拟合的风险。 简单理解，在人类的感知过程中，某些模态的信息对结果的判断影响并不大。举个例子，一块圆形的饼干和一块方形的饼干，在认知中同属于饼干这一类，因此形状对我们判断是否是饼干没有太大作用，也就是噪声。如果不能将形状数据去除掉，可能会产生“圆饼干是饼干，方饼干就不是饼干”的问题（过拟合）。 当采用无监督的方法分层预训练深度网络的权值时，为了学习到较鲁棒的特征，可以**在网络的可视层（即数据的输入层）引入随机噪声**，这种方法称为降噪自编码器（Denoising Auto-Encoder， DAE）。 降噪自编码器：一个模型，能够从有噪音的原始数据作为输入，而能够恢复出真正的原始数据。这样的模型，更具有鲁棒性。 以下是以经典的 MNIST 手写数字识别为例，对于输入的数据引入了变换角度、随机噪点、添加背景图像等噪音。模型通过训练后可以对有噪音图像更加鲁棒，而这也更符合实际使用的需求。 对于有噪音的输入数据，区别于一般自编码机，降噪自编码机要做的就是数据的降噪。关于降噪的过程如下图所示： 对于输入层 $x$，以一定概率将其节点置 0，得到 $\\hat{x}$,用 $\\hat{x}$ 去计算 $y$,计算 $z$ ，并将 $z$ 与原始 $x$ 做误差迭代，对结果误差较小的节点可以认为是噪声。每层处理重复上述工作。 自编码器的本质是学习一个相等函数，即网络的输入和重构后的输出相等，这种相等函数的表示有个缺点就是当测试样本和训练样本不符合同一分布，效果不好，而降噪自编码器在这方面的处理有所进步。 随机深度 Stochastic Depth针对于残差模块的优化，由于很深的 ResNet 通常需要很长时间的训练(也就是训练很慢)，作者引入了一种类似于 dropout 的方法，在训练过程中随机丢弃子图层（randomly drop a subset of layers），而在推断时正常使用完整的图。 ResNet 网络是由一个接一个的残差模块(ResBlock)串联起来的，可以视为ResBlock的集合。在训练时，对每个 ResBlock 随机 drop（按伯努利分布），drop 就是将上一个 ResBlock 直接输出到下一个 ResBlock，被 drop 的 ResBlock 什么都不做也不更新。另外，网络的输入被视为第一层，是不会 drop 的。 与 Dropout 的不同之处在于，该方法 drop 整个 ResBlock，而 Dropout 在训练期间只 drop 一部分神经元节点。这种方法大大降低了训练时间，甚至在训练完成后删除部分layer，还能不影响精度。 最小化信息熵 Entropy Minimization参考文章：最小熵原理（一） 在半监督学习中，有标签（分类完全准确）的数据样本通常相对较少，通过训练模型对未标记数据样本进行预测，选择出高置信度的样本，作为标记样本同有标签样本作为下一次训练的数据样本。 Entropy Minimization 是一种在半监督学习中使用的技术，它的目的是最小化信息熵，从而使模型在分类时的不确定性最小。在半监督学习中，我们希望模型尽可能地利用未标记数据来学习，但是这些数据并不带有正确的标签，因此我们需要利用某些技术来帮助我们学习这些数据。 加快模型学习进度的唯一方法就是降低学习目标的冗余信息量，所提到的“去冗余”，可以理解为“省去没必要的学习成本”。 也就是通常所使用到的技巧：过滤掉低置信度的未标记样本，保留高置信度样本。 Top-1 and Top-5 AccuracyTop-1： 在多分类问题中，一般认为在经过全连接层后得到的概率分布中概率最大的类别为我们模型的预测类别，就判断为正确。 Top-5： 在多分类问题中，一般认为在经过全连接层后得到的概率分布中概率最大的全五个类别中有我们模型的预测类别，就判断为正确。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://ytz7.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://ytz7.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"https://ytz7.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"test-blog","slug":"test-blog","date":"2023-05-11T07:42:17.000Z","updated":"2023-05-12T13:56:54.059Z","comments":true,"path":"posts/467ec823.html","link":"","permalink":"https://ytz7.github.io/posts/467ec823.html","excerpt":"","text":"这是一篇测试博客修改一下在上传试试…在测试测试","categories":[],"tags":[{"name":"test","slug":"test","permalink":"https://ytz7.github.io/tags/test/"}]},{"title":"半监督学习学习过程","slug":"semi-suprised-learning-start","date":"2023-05-11T07:08:40.000Z","updated":"2023-05-12T13:56:54.059Z","comments":true,"path":"posts/52ee1a8c.html","link":"","permalink":"https://ytz7.github.io/posts/52ee1a8c.html","excerpt":"","text":"半监督学习写在前面学习了快一个学期的 Java 程序开发，让我感觉开发的过程真的好空洞…但是我并不知道下一步到底要做什么，又处于想找实习又怕找不到的情况下，刷了几天 LeetCode，然后又变成了“小🐏人”，紧接着被隔离😅，真的挺痛苦，所幸在第三天就“出狱”了，在蹲“监狱”期间，受到一个“狱友”的启发，“还是要做好职业规划的，若是没有做好职业规划，像一只无头苍蝇一样，很可能在毕业之际还是找不到工作。”在思考了一天后，我决定重新开始在“半监督学习”这个领域开始零基础学习了，为什么会有这个想法？是因为我觉得我起初考研的目的是为了能够在人工智能这个领域继续深入了解，目的很单纯，那为什么我不坚持以下呢？于是我决定重新开始这个计划，管他呢，起码是自己喜欢做的事情，哪怕未来找不到工作什么的，起码现在是由试错的资本的。 以下将会记录我在学习时候遇到的问题/解决方式/新思路/感想。 1. Self-Training自训练方法，模型基于已标记好的训练集进行训练，得到一个基础模型，利用该基础模型对未标记的数据集进行预测一个伪标签，然后将两个数据集整合训练，得到一个新的模型，从而迭代更新模型参数，生成一个最优模型。 1.1. [Pseudo-Label](https://github.com/emintham/Papers/blob/master/Lee-Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks.pdf)伪标签技术适用于小样本学习，实际上在样本极其珍贵的金融、医疗图像、安全等领域，伪标签学习有时候很有效。 伪标签的定义来自于半监督学习，半监督学习的核心思想是通过借助无标签的数据来提升有监督过程中的模型性能。 粗略来讲，伪标签技术就是利用在已标注数据所训练的模型在未标注的数据上进行预测，根据预测结果对样本进行筛选，再次输入模型中进行训练的一个过程。 如下图所示，利用有标签的数据集训练出一个模型，运用训练出的模型给予无标签数据一个伪标签。如何定义所属类别？利用训练好的模型对无标签数据进行预测，以概率最高的类别作为无标签数据的伪标签。 entropy regularization：用于防止模型过拟合，通过在损失函数中加入熵（entropy）项来实现 利用 entropy regularization 思想，将无监督数据转为目标数据的正则项，即将拥有伪标签的无标签数据视为有标签的数据，利用交叉熵（与最初训练模型一致）来评估误差大小。 模型整体的目标函数如下： 其中左边一项为交叉熵，用来评估有标签数据的误差，右边一项即为 entropy regularization 项，用来从无标签的数据中获取训练信号。 为了平衡有标签数据和无标签数据的信号强度，如上所示，算法在目标函数中引入了时变参数 α(t)，其数学形式如下，其中 T1 和 T2 都为超参数： 因此，随着训练时间的增加，α(t) 将会从零开始线性增长至某个饱和值，对应无标签数据的信号也将逐渐释放出来。背后的核心想法也很直观，早期模型预测效果不佳，因此 entropy regularization 产生信号的误差也较大，因而 α(t) 应该从零开始，由小逐渐增大。 存在不足：只在训练时间这个维度上，采用了退火思想，即采用时变系数α(t)。而在伪标签这个维度，对于模型给予的预测标签一视同仁，这种方法在实际中存在明显问题。若模型在对伪标签的数据预测后， 10 个类别预测概率值都接近于 0.1，以最大概率这一原则选择对应的标签，那么这个标签对模型的训练会造成一定的副作用。 设想如何突破这一不足？ 也许可以设定一个阈值，抛弃那些预测最大概率值小于该阈值的未标记样本，将满足条件的未标记样本分配伪标签，并加入模型评估当中，之后再迭代训练。 1.2. Noisy Student论文的关键 idea 是训练两个模型，“teacher”和“student”，强调的是在student模型中加入噪声，teacher 模型和 student 模型可以用不同的模型训练，也可以使用相同的模型。 在有标签数据中训练“teacher”模型，并利用该模型对未标记数据进行推断伪标签，这些伪标签可以是软标签，也可以取其最大概率的类别将其转换为硬标签。 然后将标记数据与为标记数据（带有伪标签）置入“student”进行训练，在训练之前数据增强使用 RandAugment，待“student”模型训练好后，使用最新的模型作为新的“teacher”，进行下一次迭代，此过程会重复几次（通常为 3 次）。 总结该篇论文的流程思路： 首先将在 ImageNet 上训练好的模型作为 Teacher Network 来训练 Student Network 再使用训练好的 T 网络（无噪音）来对另一个数据集 [JFT dataset] 生成尽可能准确的伪标签 之后使用生成伪标签的数据集 [JFT dataset] 和 ImageNet 一起训练 Student Network Student Network中增加了模型噪音 Dropout 随机深度 Stochastic Depth 数据噪音：对图片进行数据增广（RandAugment） 对 Student 模型添加噪音的作用： 数据噪音：提高泛化能力 模型噪音：提高模型鲁棒性和泛化能力 具体参数设置： Stochastic Depth：幸存概率因子为 0.8 Dropout：分类层（final layer）引入 0.5 的丢弃率 RandAugment：应用两个随机计算，其震级设置为 27 其他 Tricks： 数据过滤：将教师模型中置信度不高的图片过滤，因为这通常代表着域外图像（out-of-domain data） 数据平衡：平衡不同类别的图片数量，当一个类别所对应的图片数量不是很充足时，会采取随机复制的方法来扩充样本量 软标签（Soft Pseudo-Label）：在消融实验中表示，软标签对域外图像有更强的指导作用 消融实验1.噪音是否对模型有影响？（The Importance of Noise in Self-training） 从结果可以看出，噪音、随机深度、数据扩充起着重要的作用使学生模型胜过教师模型，对此有人提出是不是对未标记数据加入正则项以防止过拟合来代替噪音，作者在实验中说明这是不对的。因为在去噪的情况下，未标记图像的训练损失并没有下降多少以此说明模型并没有对未标记数据过拟合。 2.对于迭代训练的消融实验（A Study of Iterative Training） 作者先在标记数据上训练了 EfficientNet-B7 作为 Teacher，然后再训练 EfficientNet-L2 作为 Student，然后让 Student 作为 Teacher 依次迭代三轮，作者表明，迭代训练提高了准确度，并且，给出再最后通过调整未标记图像和标记图像的比为 ==1 : 28== 时达到最优 Top-1 Acc.。 3.能力强的教师模型会不会对学生模型造成的影响 4.无标签的数据量大小 作者按照比例分别从整个数据集中均匀采样（uniformly sampling），会发现在数据量减少至 1/16 中，模型的性能大都相似，在数据量达到 1/32 或更小后，模型性能有了显著的下降（可能 .3 个点就可以算是显著的下降了吧…），所以，使用大量未标记的数据会产生更好的性能，但作者指出：对于大模型来说，数据量越多越好，而小模型由于容量限制则很容易饱和。 5.硬标签和软标签对域外图像的影响 作者将预测置信度高（high-confidence）的图像视作域内图像（in-domain images），反之，将预测置信度低（low-confidence）的图像视作域外图像（out-of-domain images），作者表明：对于域内图像，软伪标签（ soft pseudo labels）和硬伪标签（hard pseudo labels）都对模型有一定的帮助；而对于域外图像，软伪标签使得模型对域外图像的判断有着一定的帮助，而硬标签则会对模型的精度有一定的损害。 剩下的消融实验就不写了，有点过于冗长了。","categories":[{"name":"半监督学习","slug":"半监督学习","permalink":"https://ytz7.github.io/categories/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://ytz7.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"个人随笔","slug":"个人随笔","permalink":"https://ytz7.github.io/tags/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/"},{"name":"Semi-Suprised-Learning","slug":"Semi-Suprised-Learning","permalink":"https://ytz7.github.io/tags/Semi-Suprised-Learning/"}]}],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://ytz7.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"半监督学习","slug":"半监督学习","permalink":"https://ytz7.github.io/categories/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://ytz7.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"https://ytz7.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"test","slug":"test","permalink":"https://ytz7.github.io/tags/test/"},{"name":"个人随笔","slug":"个人随笔","permalink":"https://ytz7.github.io/tags/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/"},{"name":"Semi-Suprised-Learning","slug":"Semi-Suprised-Learning","permalink":"https://ytz7.github.io/tags/Semi-Suprised-Learning/"}]}