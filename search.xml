<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>SimMatch</title>
      <link href="/posts/697723e2.html"/>
      <url>/posts/697723e2.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>SimMatch: Semi-supervised Learning with Similarity Matching - CVPR 2022</p></blockquote><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230902201326255.png" alt="image-20230902201326255"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165633855.png" alt="image-20230913165633855"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165645656.png" alt="image-20230913165645656"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165654047.png" alt="image-20230913165654047"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165704315.png" alt="image-20230913165704315"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165712290.png" alt="image-20230913165712290"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165722851.png" alt="image-20230913165722851"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165737444.png" alt="image-20230913165737444"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165746130.png" alt="image-20230913165746130"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165758187.png" alt="image-20230913165758187"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165808546.png" alt="image-20230913165808546"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165818149.png" alt="image-20230913165818149"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165826742.png" alt="image-20230913165826742"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165834876.png" alt="image-20230913165834876"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165843838.png" alt="image-20230913165843838"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165851812.png" alt="image-20230913165851812"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165904250.png" alt="image-20230913165904250"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165931934.png" alt="image-20230913165931934"></p>]]></content>
      
      
      <categories>
          
          <category> 半监督学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Semi-Surprised-Learning </tag>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MMDetection</title>
      <link href="/posts/73d20edf.html"/>
      <url>/posts/73d20edf.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>MMDetection 踩坑之路</p><p>每天都在配环境，每时每刻都在烦…</p></blockquote><p>参考：<a href="https://mmdetection.readthedocs.io/zh_CN/v2.17.0/get_started.html#id6">依赖 — MMDetection 2.17.0 文档</a></p><h1 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h1><h2 id="安装-Pytorch"><a href="#安装-Pytorch" class="headerlink" title="安装 Pytorch"></a>安装 Pytorch</h2><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">更新 pip</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">python -m pip install --upgrade pip</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html</span></span><br></pre></td></tr></tbody></table></figure><h2 id="安装-mmcv-full"><a href="#安装-mmcv-full" class="headerlink" title="安装 mmcv-full"></a>安装 mmcv-full</h2><p>推荐安装 <code>mmcv-full</code>，包含所有功能，而简版为 <code>mmcv</code>，并且，下载时必须从 openmmlab 官网下载，下载源格式为：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://download.openmmlab.com/mmcv/dist/cu${CUDA_VERSION }/torch{TORCH_VERSION }/index.html</span><br></pre></td></tr></tbody></table></figure><blockquote><p>当 cuda 版本为 10.1 时，则为 <code>cu101</code>；11.1 时，为 <code>cu111</code></p><p>torch 的版本不管为 1.9.5 还是 1.9.1，小版本都归零，即  <code>1.9.0</code></p></blockquote><p>在 CUDA 11.1，torch 版本为 1.9.1 的版本下，下载地址为：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html</span><br></pre></td></tr></tbody></table></figure><p>完整的命令为：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">pip install mmcv-full==1.3.17 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html</span></span><br></pre></td></tr></tbody></table></figure><h3 id="安装-mmdetection"><a href="#安装-mmdetection" class="headerlink" title="安装 mmdetection"></a>安装 mmdetection</h3><p>需要注意依赖，参考首行的官方文档：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">pip install mmdet==2.25</span></span><br></pre></td></tr></tbody></table></figure><h1 id="学习-MMDetection"><a href="#学习-MMDetection" class="headerlink" title="学习 MMDetection"></a>学习 MMDetection</h1><h2 id="对数据做增强"><a href="#对数据做增强" class="headerlink" title="对数据做增强"></a>对数据做增强</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 强增强 - SoftTeacher</span></span><br><span class="line">strong_pipeline = [</span><br><span class="line">    <span class="built_in">dict</span>(</span><br><span class="line">        <span class="built_in">type</span>=<span class="string">"Sequential"</span>,</span><br><span class="line">        transforms=[</span><br><span class="line">            <span class="built_in">dict</span>(</span><br><span class="line">                <span class="built_in">type</span>=<span class="string">"RandResize"</span>,</span><br><span class="line">                img_scale=[(<span class="number">1333</span>, <span class="number">400</span>), (<span class="number">1333</span>, <span class="number">1200</span>)],</span><br><span class="line">                multiscale_mode=<span class="string">"range"</span>,</span><br><span class="line">                keep_ratio=<span class="literal">True</span>,</span><br><span class="line">            ),</span><br><span class="line">            <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandFlip"</span>, flip_ratio=<span class="number">0.5</span>),</span><br><span class="line">            <span class="built_in">dict</span>(</span><br><span class="line">                <span class="built_in">type</span>=<span class="string">"ShuffledSequential"</span>,</span><br><span class="line">                transforms=[</span><br><span class="line">                    <span class="built_in">dict</span>(</span><br><span class="line">                        <span class="built_in">type</span>=<span class="string">"OneOf"</span>,</span><br><span class="line">                        transforms=[</span><br><span class="line">                            <span class="built_in">dict</span>(<span class="built_in">type</span>=k)</span><br><span class="line">                            <span class="keyword">for</span> k <span class="keyword">in</span> [</span><br><span class="line">                                <span class="string">"Identity"</span>,</span><br><span class="line">                                <span class="string">"AutoContrast"</span>,</span><br><span class="line">                                <span class="string">"RandEqualize"</span>,</span><br><span class="line">                                <span class="string">"RandSolarize"</span>,</span><br><span class="line">                                <span class="string">"RandColor"</span>,</span><br><span class="line">                                <span class="string">"RandContrast"</span>,</span><br><span class="line">                                <span class="string">"RandBrightness"</span>,</span><br><span class="line">                                <span class="string">"RandSharpness"</span>,</span><br><span class="line">                                <span class="string">"RandPosterize"</span>,</span><br><span class="line">                            ]</span><br><span class="line">                        ],</span><br><span class="line">                    ),</span><br><span class="line">                    <span class="built_in">dict</span>(</span><br><span class="line">                        <span class="built_in">type</span>=<span class="string">"OneOf"</span>,</span><br><span class="line">                        transforms=[</span><br><span class="line">                            <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandTranslate"</span>, x=(-<span class="number">0.1</span>, <span class="number">0.1</span>)),</span><br><span class="line">                            <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandTranslate"</span>, y=(-<span class="number">0.1</span>, <span class="number">0.1</span>)),</span><br><span class="line">                            <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandRotate"</span>, angle=(-<span class="number">30</span>, <span class="number">30</span>)),</span><br><span class="line">                            [</span><br><span class="line">                                <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandShear"</span>, x=(-<span class="number">30</span>, <span class="number">30</span>)),</span><br><span class="line">                                <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandShear"</span>, y=(-<span class="number">30</span>, <span class="number">30</span>)),</span><br><span class="line">                            ],</span><br><span class="line">                        ],</span><br><span class="line">                    ),</span><br><span class="line">                ],</span><br><span class="line">            ),</span><br><span class="line">            <span class="built_in">dict</span>(</span><br><span class="line">                <span class="built_in">type</span>=<span class="string">"RandErase"</span>,</span><br><span class="line">                n_iterations=(<span class="number">1</span>, <span class="number">5</span>),</span><br><span class="line">                size=[<span class="number">0</span>, <span class="number">0.2</span>],</span><br><span class="line">                squared=<span class="literal">True</span>,</span><br><span class="line">            ),</span><br><span class="line">        ],</span><br><span class="line">        record=<span class="literal">True</span>,</span><br><span class="line">    ),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"Pad"</span>, size_divisor=<span class="number">32</span>),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"Normalize"</span>, **img_norm_cfg),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"ExtraAttrs"</span>, tag=<span class="string">"unsup_student"</span>),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"DefaultFormatBundle"</span>),</span><br><span class="line">    <span class="built_in">dict</span>(</span><br><span class="line">        <span class="built_in">type</span>=<span class="string">"Collect"</span>,</span><br><span class="line">        keys=[<span class="string">"img"</span>, <span class="string">"gt_bboxes"</span>, <span class="string">"gt_labels"</span>],</span><br><span class="line">        meta_keys=(</span><br><span class="line">            <span class="string">"filename"</span>,</span><br><span class="line">            <span class="string">"ori_shape"</span>,</span><br><span class="line">            <span class="string">"img_shape"</span>,</span><br><span class="line">            <span class="string">"img_norm_cfg"</span>,</span><br><span class="line">            <span class="string">"pad_shape"</span>,</span><br><span class="line">            <span class="string">"scale_factor"</span>,</span><br><span class="line">            <span class="string">"tag"</span>,</span><br><span class="line">            <span class="string">"transform_matrix"</span>,</span><br><span class="line">        ),</span><br><span class="line">    ),</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure><p>这段代码中定义了一个强增强的 pipeline，包括以下几个部分：</p><ol><li><p><code>RandResize</code>：随机对图像进行缩放，可以生成不同尺度的图像来增加训练数据的多样性。</p></li><li><p><code>RandFlip</code>：随机对图像进行水平或垂直翻转，可以帮助模型学习不同角度的物体。</p></li><li><p><code>ShuffledSequential</code>：将后续的增强操作随机打乱，以增加数据的多样性。</p></li><li><p><code>OneOf</code>：从一组增强操作中随机选择一个进行操作，包括：</p><ul><li><code>Identity</code>：原图不变</li><li><code>AutoContrast</code>：自动调整图像对比度</li><li><code>RandEqualize</code>：随机增强直方图均衡化</li><li><code>RandSolarize</code>：随机增强 Solarization 效果</li><li><code>RandColor</code>：随机改变图像颜色平衡</li><li><code>RandContrast</code>：随机增强图像对比度</li><li><code>RandBrightness</code>：随机改变图像亮度</li><li><code>RandSharpness</code>：随机增强图像锐利度</li><li><code>RandPosterize</code>：随机降低图像位深度</li></ul></li><li><p><code>OneOf</code>：从一组增强操作中随机选择一个进行操作，包括：</p><ul><li><code>RandTranslate</code>：随机对图像进行平移</li><li><code>RandRotate</code>：随机对图像进行旋转</li><li><code>RandShear</code>：随机对图像进行剪切</li></ul></li><li><p><code>RandErase</code>：随机擦除图像中的像素，增加模型对图像噪声和遮挡的鲁棒性。</p></li><li><p><code>Pad</code>：将图像填充至指定大小的倍数，用于处理不同大小的图像以及特定的 GPU 加速要求。</p></li><li><p><code>Normalize</code>：对图像进行标准化处理，将像素值映射到均值为 0，方差为 1 的分布中。</p></li><li><p><code>ExtraAttrs</code>：添加一个额外的属性，用于标记数据被传入模型的来源（这里标记为 “unsup_student”）。</p></li><li><p><code>DefaultFormatBundle</code>：将图像数据打包为默认格式。</p></li><li><p><code>Collect</code>：将图像及其标注信息收集到一个字典中，以供后续处理。</p></li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 弱增强 - SoftTeacher</span></span><br><span class="line">weak_pipeline = [</span><br><span class="line">    <span class="built_in">dict</span>(</span><br><span class="line">        <span class="built_in">type</span>=<span class="string">"Sequential"</span>,</span><br><span class="line">        transforms=[</span><br><span class="line">            <span class="built_in">dict</span>(</span><br><span class="line">                <span class="built_in">type</span>=<span class="string">"RandResize"</span>,</span><br><span class="line">                img_scale=[(<span class="number">1333</span>, <span class="number">400</span>), (<span class="number">1333</span>, <span class="number">1200</span>)],</span><br><span class="line">                multiscale_mode=<span class="string">"range"</span>,</span><br><span class="line">                keep_ratio=<span class="literal">True</span>,</span><br><span class="line">            ),</span><br><span class="line">            <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandFlip"</span>, flip_ratio=<span class="number">0.5</span>),</span><br><span class="line">        ],</span><br><span class="line">        record=<span class="literal">True</span>,</span><br><span class="line">    ),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"Pad"</span>, size_divisor=<span class="number">32</span>),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"Normalize"</span>, **img_norm_cfg),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"ExtraAttrs"</span>, tag=<span class="string">"unsup_teacher"</span>),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"DefaultFormatBundle"</span>),</span><br><span class="line">    <span class="built_in">dict</span>(</span><br><span class="line">        <span class="built_in">type</span>=<span class="string">"Collect"</span>,</span><br><span class="line">        keys=[<span class="string">"img"</span>, <span class="string">"gt_bboxes"</span>, <span class="string">"gt_labels"</span>],</span><br><span class="line">        meta_keys=(</span><br><span class="line">            <span class="string">"filename"</span>,</span><br><span class="line">            <span class="string">"ori_shape"</span>,</span><br><span class="line">            <span class="string">"img_shape"</span>,</span><br><span class="line">            <span class="string">"img_norm_cfg"</span>,</span><br><span class="line">            <span class="string">"pad_shape"</span>,</span><br><span class="line">            <span class="string">"scale_factor"</span>,</span><br><span class="line">            <span class="string">"tag"</span>,</span><br><span class="line">            <span class="string">"transform_matrix"</span>,</span><br><span class="line">        ),</span><br><span class="line">    ),</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure><p>这是一段弱增强的代码，主要包括以下几个部分：</p><ol><li><p><code>RandResize</code>：随机对图像进行缩放，可以生成不同尺度的图像来增加训练数据的多样性。</p></li><li><p><code>RandFlip</code>：随机对图像进行水平或垂直翻转，可以帮助模型学习不同角度的物体。</p></li><li><p><code>Pad</code>：将图像填充至指定大小的倍数，用于处理不同大小的图像以及特定的 GPU 加速要求。</p></li><li><p><code>Normalize</code>：对图像进行标准化处理，将像素值映射到均值为 0，方差为 1 的分布中。</p></li><li><p><code>ExtraAttrs</code>：添加一个额外的属性，用于标记数据被传入模型的来源（这里标记为 “unsup_teacher”）。</p></li><li><p><code>DefaultFormatBundle</code>：将图像数据打包为默认格式。</p></li><li><p><code>Collect</code>：将图像及其标注信息收集到一个字典中，以供后续处理。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 环境配置 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Faster-RCNN</title>
      <link href="/posts/c018b3e5.html"/>
      <url>/posts/c018b3e5.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</p></blockquote><blockquote><p>记一次过度饮酒后的第二天…真的痛苦</p><p>看了好几篇在目标检测算法应用半监督学习的文章，感觉好多都是基于 Faster RCNN 作为其主干网络来实现，但是对于目标检测算法，目前只对于 YOLOv3（One-Stage）有所了解，所以说，还是需要对其他目标检测的算法有所了解才好。</p></blockquote><p>论文地址：<a href="https://arxiv.org/abs/1506.01497"> Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p><p>参考文章：</p><ul><li><a href="https://blog.csdn.net/weixin_44791964/article/details/105739918">Pytorch搭建Faster R-CNN目标检测平台</a></li><li><a href="https://blog.csdn.net/STATEABC/article/details/123881710">RCNN、Fast-RCNN、Faster-RCNN网络详解</a></li><li><a href="https://zhuanlan.zhihu.com/p/60895929?utm_id=0">Faster RCNN的一些笔记</a></li><li><a href="https://blog.csdn.net/lanran2/article/details/54376126?spm=1001.2014.3001.5502">RPN 解析</a></li></ul><p>参考视频：</p><ul><li><a href="https://www.bilibili.com/video/BV1af4y1m7iL?p=3&amp;spm_id_from=pageDriver&amp;vd_source=7801e0201bc8ddcb3d951c7388a08c3e">霹雳吧啦Wz</a></li><li><a href="https://www.bilibili.com/video/BV1BK41157Vs?p=1&amp;vd_source=7801e0201bc8ddcb3d951c7388a08c3e">Bubbliiiing</a></li></ul><h1 id="RCNN-算法流程"><a href="#RCNN-算法流程" class="headerlink" title="RCNN 算法流程"></a>RCNN 算法流程</h1><ol><li>一张图片生成1k~2k个候选区域(使用Selective Search方法)；</li><li>对每个候选区域，使用深度网络(即图片分类网络)提取特征；</li><li>特征送入每一类的SVM分类器，判别是否属于该类；</li><li>使用回归器精细修正候选框位置。</li></ol><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/56ef4502976d4a25a8c5db087638bbba.png" alt="在这里插入图片描述"></p><p>Selective Search 方法是一种通过图像分割方法来生成一些原始区域的方法，使用一些合并策略将这些区域合并，得到一个层次化的区域结构，这些结构就包含着可能需要的物体。简而言之，<u><strong>Selective Search 方法用于生成候选框</strong></u>。</p><p><strong>对每个候选区域，使用深度网络提取特征</strong>后，将 2000 个候选区域分别缩放到 <u><strong>227 * 227</strong></u> 的大小（AlexNet CNN 的输入大小），通过 CNN 提取特征。获取 <u><strong>4096 维</strong></u>的特征，得到 <u><strong>2000 * 4096 维</strong></u>特征矩阵：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/353f2eb486744aa1aef24d69a1e8d53d.png" alt="在这里插入图片描述"></p><p>然后将<strong>特征送入每一类的SVM分类器来判定类别</strong>，将 2000 * 4096 维特征与 20 个SVM（Pascal VOC 有 20 个类别）组成的<u><strong>权值矩阵</strong></u> 4096 * 20 相乘，获得 2000 * 20 维矩阵表示每一个建议框是某个目标类别的得分，即在矩阵中 [i, j] 表示第 i 个建议框中是第 j 类的概率大小。</p><p>分别对上述 2000 * 20 维矩阵中每一列即每一类进行<u><strong>非极大值抑制提出重叠建议框</strong></u>，得到该列（即该类中）得分最高的一些建议框。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/c533db30aeb54d288d7ac3ac38e9789b.png" alt="在这里插入图片描述"></p><p><strong>使用回归器精细修正候选框位置</strong>，分别用 20 个回归器对上述 20 个类别中剩余的建议框进行回归操作，最终得到每个类别的修正后得分最高的 bounding box。</p><p>如下图所示，黄色窗口 P 表示建议框 Region Proposal，绿色窗口 G 表示实际框Ground Truth，红色窗口 G^ 表示 Region Proposal 进行回归后的预测窗口。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/25b3a317002b43818e22f4744519c948.png" alt="在这里插入图片描述"></p><h1 id="Fast-RCNN-算法流程"><a href="#Fast-RCNN-算法流程" class="headerlink" title="Fast-RCNN 算法流程"></a>Fast-RCNN 算法流程</h1><p>具体流程如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/f6f2b89d4e7d4a13acc6c068406add95.png" alt="在这里插入图片描述"></p><h2 id="SPPNet-x2F-ROI-pooling-layer"><a href="#SPPNet-x2F-ROI-pooling-layer" class="headerlink" title="SPPNet / ROI pooling layer"></a>SPPNet / ROI pooling layer</h2><p>在 RCNN 的基础上对于提取特征的步骤做了改进：</p><ul><li>将整个图像输入到 CNN 中得到特征图，再将 Selective Search 方法得到的<u><strong>候选框投影到特征图</strong></u>上，得到<u><strong>特征矩阵</strong></u></li></ul><p>即 SPPNet，与 RCNN 的对比如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529172148060.png" alt="image-20230529172148060"></p><p>根据每个候选区域原图与特征图的映射关系，就能在特征图中直接获取特征矩阵，这些候选区域的特征不需要再重复计算。</p><p>对于<u><strong>候选区域的 size 各异的限制</strong></u>，RCNN 通过对候选区域做变换得到，即<u><strong>放缩</strong></u>，在 SPPNet 中由 <code>spatial pyramid pooling</code> 解决，将原始网络中最后一个池化层改进成了空间金字塔池化层，它可以将任意尺寸的特征图转化为固定大小的特征向量从而不同尺寸的候选区域的不同大小特征图样切片也可以转化成相同大小的特征向量以进行后续判决：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/49a24a3963224bf5a3ede9915c0041cd.png" alt="img"></p><h2 id="softmax-分类器"><a href="#softmax-分类器" class="headerlink" title="softmax 分类器"></a>softmax 分类器</h2><p>输出 N+1 个类别的概率（目标总类别 + 背景），共 N+1 个节点：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/6b33b1e4c7f34f7bb10615a41d7410c4.png" alt="在这里插入图片描述"></p><h2 id="bbox-regressor-边界框回归器"><a href="#bbox-regressor-边界框回归器" class="headerlink" title="bbox regressor 边界框回归器"></a>bbox regressor 边界框回归器</h2><p>输出对应 N+1 个类别的候选边界框回归参数（dx, dy, dw, dh），共 （N+1） *  4 个节点：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/dbd6bbb853f34471bb0b6b4344e8e8ee.png" alt="在这里插入图片描述"></p><p>利用回归得到的参数得到边界框：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/d9880b1884e541b99313fb39a10c0896.png" alt="在这里插入图片描述"></p><h1 id="Faster-RCNN-算法流程"><a href="#Faster-RCNN-算法流程" class="headerlink" title="Faster-RCNN 算法流程"></a>Faster-RCNN 算法流程</h1><ol><li>将图像输入网络得到相应的特征图；</li><li>使用 RPN 结构生成候选框，将 RPN 生成的候选框投影到特征图上获得相应的特征矩阵；</li><li>将每个特征矩阵通过 ROI(Region of Interest) pooling 层缩放到 7x7 大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果。</li></ol><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529174255504.png" alt="image-20230529174255504" style="zoom: 67%;"><p>Faster-RCNN 可以理解为 RPN + Fast-RCNN，右半部分即为 Fast-RCNN。</p><h2 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h2><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529192038186.png" alt="image-20230529192038186"></p><blockquote><p>RPN，全称为 Region Proposal Network，Region Proposal 为区域提取，RPN 就是用来提取候选框的网络</p></blockquote><p>利用深度卷积神经网络来计算初始的候选区域，一个特征图通过<u><strong>滑动窗口</strong></u>，得到256 维特征，其中一个 <code>grid</code> 生成 <code>k</code> 个 <code>anchor boxes</code>，通过两次全连接得到 <code>2k</code> 个分数和 <code>4k</code> 个坐标，如下图所示，其中 <code>cls</code> 代表类别预测，分别预测<u><strong>背景概率</strong></u>和<u><strong>前景概率</strong></u>，前景概率即为区域中是否带有对象的概率，可以视作<u><strong>二分类问题</strong></u>，而 <code>reg</code> 则为 k 个 boxes 的四个参数的预测，分别为：中心点坐标和宽高。</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529193318163.png" alt="image-20230529193318163" style="zoom: 67%;"><p>在论文中提出了三种尺度（）和三种比例作为 anchor，每个位置在<u><strong>原图</strong></u>上都对于有 9 个 anchor，对于 RPN 生成的候选框之间存在大量重叠，基于候选框的 <u><strong>cls 得分</strong></u>，采用<u><strong>非极大抑制</strong></u>筛选候选框。最后对应于原图的候选框生成结果可以由下图简略概括：</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529193915964.png" alt="image-20230529193915964" style="zoom:50%;"><p>RPN 的<u><strong>输入特征图</strong></u>指的是哪个特征图？</p><p>RPN 的输入特征图就是 Faster-RCNN 中的公共 feature map，主要用以 RPN 和 RoI Pooling 共享。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529194033385.png" alt="image-20230529194033385"></p><p>其中，<code>p</code> 表示的是 anchor 为一个 object 的概率，<code>p*</code> 为 1 时则表示该 anchor 为正样本（positive），反之，则为负样本（negative）。</p><p>判断 anchor 是否为正样本：</p><ul><li>与 Grounding Truth 的重叠区域 IoU 超过 0.7</li><li>为了防止条件 1 筛选过后不存在正样本，故使 IoU 最大的 anchor 也为正样本</li></ul><h2 id="Training-RPNs"><a href="#Training-RPNs" class="headerlink" title="Training RPNs"></a>Training RPNs</h2><ol><li><p>RPN 通过反向传播和随机梯度下降法进行端到端的训练</p></li><li><p>随机从一张图片中采样 256 个 anchors，计算一个 mini-batch 的损失函数</p></li><li><p>采样的 positive anchors 和 negative anchors 的比例是 1 : 1，若正样本少于 128，则从负样本中采样填充</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529201003539.png" alt="image-20230529201003539"></p></li><li><p>所有层通过 ImageNet 预训练模型进行初始化</p></li></ol><h2 id="Sharing-Features-for-RPN-and-Fast-R-CNN"><a href="#Sharing-Features-for-RPN-and-Fast-R-CNN" class="headerlink" title="Sharing Features for RPN and Fast R-CNN"></a>Sharing Features for RPN and Fast R-CNN</h2><blockquote><p>这一个地方看得不是很懂…</p></blockquote><p><u><strong>RPN和Fast R-CNN共享特征</strong></u>，论文中采用的是：<u><strong>Alternating training</strong></u>：</p><p>首先训练 RPN 层，然后用所得到的 proposals 来训练 Fast-RCNN（此时设置 RPN层不更新），然后得到的 Fast-RCNN，再来初始化并训练 RPN 层，然后再迭代往复。</p><p>目的：为了使两个不同任务能够<u><strong>共享卷积层</strong></u>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529202144896.png" alt="image-20230529202144896"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529202200709.png" alt="image-20230529202200709"></p><p>具体训练细节：</p><ol><li>训练 RPN，使用 ImageNet 预训练权重，并且在 region proposal 任务上进行训练微调</li><li>使用生成的 region proposal 在 Fast R-CNN 上进行训练，同意使用 ImageNet 预训练权重</li><li>使用所训练的目标检测网络（Fast R-CNN）来初始化 RPN 训练权重，但固定共享的卷积层，仅微调 RPN 独有的层</li><li>让共享的卷积层继续固定，训练微调 Fast R-CNN 独有的层</li></ol><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529203002756.png" alt="image-20230529203002756" style="zoom:67%;"><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529203015920.png" alt="image-20230529203015920" style="zoom:67%;"><blockquote><p>现在是 2023/05/29 22：38，看代码看得有点昏，走咯走咯，下班…</p><p>git push 一哈🤦‍♂️</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Object-Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023 年下半年计划</title>
      <link href="/posts/68092092.html"/>
      <url>/posts/68092092.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>打算做半监督目标检测方向，理清以下下半年的计划，该学习什么，该怎样学习，不然像个无头苍蝇一样四处乱撞…</p><p>定一个计划把，看看自己是否能够坚持下来，学完一篇就在这边打卡，但我不知道这样子做会不会太慢？…</p><p>不过还是要处理一下前几天看的 FixMatch 的代码细节还没看完，先把这个东西收尾收掉把。</p><p>2023-05-24 记录：FixMatch 代码收尾结束，果然看代码对于 Coding 也会有些帮助，尤其是在上次看完 YOLOv3 的代码之后，之前感觉还挺害怕看代码的…</p><p>Ps： chatGPT 真好用🤗</p></blockquote><h1 id="Coding-能力"><a href="#Coding-能力" class="headerlink" title="Coding 能力"></a>Coding 能力</h1><p>感觉自己的 Python 基础相当的差啊，对 Pytorch 也不这么熟悉，或者说只会看代码和运行别人的代码🤦‍♂️，那么该如何解决这一个棘手的问题呢？</p><ul><li><a href="https://space.bilibili.com/18161609/channel/collectiondetail?sid=48290">霹雳吧啦Wz的个人空间_哔哩哔哩_bilibili</a></li></ul><p>学习这个博主关于图像分类方面的代码，尽量要试着阅读和手写代码</p><h1 id="目标检测方面"><a href="#目标检测方面" class="headerlink" title="目标检测方面"></a>目标检测方面</h1><p>对于目标检测的基础只是还是需要了解的，具体可以看下面这个博主的视频学习，也是能阅读代码就阅读代码，（手写代码）</p><ul><li><a href="https://space.bilibili.com/472467171/?spm_id_from=333.999.0.0">Bubbliiiing的个人空间_哔哩哔哩_bilibili</a></li></ul><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230525120054286.png" alt="image-20230525120054286"></p><h1 id="半监督目标检测方向"><a href="#半监督目标检测方向" class="headerlink" title="半监督目标检测方向"></a>半监督目标检测方向</h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&amp;mid=2247602689&amp;idx=3&amp;sn=608057273347f3109b0a6e65212ed3c4&amp;chksm=fb54b0edcc2339fb809d96e89c98a6a023d9f4d19185c875ce70f348d2265ddf620d91fba086&amp;scene=27">一文梳理目标检测的半监督学习 (qq.com)</a></p><p>首先肯定是了解先前的相关经典论文，不过不需要精读，需要精度的是这两年以内的论文，然后再去复现论文。</p>]]></content>
      
      
      <categories>
          
          <category> 学习计划 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch 学习</title>
      <link href="/posts/9f542fbf.html"/>
      <url>/posts/9f542fbf.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>好久没有写 python了，对于 pytorch 的语法什么的都忘光光了，写着一篇博客来记录我学习的过程，以及途中遇到的 bug 和解决方法，希望我的 coding 能力能够有所提升…</p></blockquote><h1 id="环境安装配置问题"><a href="#环境安装配置问题" class="headerlink" title="环境安装配置问题"></a>环境安装配置问题</h1><h2 id="安装-CUDA（Linux-环境）"><a href="#安装-CUDA（Linux-环境）" class="headerlink" title="安装 CUDA（Linux 环境）"></a>安装 CUDA（Linux 环境）</h2><h3 id="查看-CUDA-状态"><a href="#查看-CUDA-状态" class="headerlink" title="查看 CUDA 状态"></a>查看 CUDA 状态</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="comment"># 查看 cuda</span></span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">naidia-smi</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">nvcc -V</span></span><br></pre></td></tr></tbody></table></figure><h2 id="conda-create-无法使用"><a href="#conda-create-无法使用" class="headerlink" title="conda create 无法使用"></a>conda create 无法使用</h2><p>记录一下 conda create 命令无法使用的问题：</p><p>原因在于配置文件好像解析不了，于是将配置文件 <code>.condarc</code> 中的内容替换为：</p><figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">channel_alias: https://mirrors.tuna.tsinghua.edu.cn/anaconda</span><br><span class="line">default_channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">ssl_verify: true</span><br></pre></td></tr></tbody></table></figure><p>更改完成后，在命令行输入以下命令完成配置的更新：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></tbody></table></figure><h2 id="Linux-中-bash-执行-sh-文件"><a href="#Linux-中-bash-执行-sh-文件" class="headerlink" title="Linux 中 bash 执行 .sh 文件"></a>Linux 中 bash 执行 .sh 文件</h2><p>当出现 <code>$'\r': command not found</code>，是因为 <u><strong>Windows</strong></u> 和 <u><strong>Linux</strong></u> 的 <u><strong>sh</strong></u> 一些文件格式不同：</p><p>以换行为例，Windows 中是 <code>\r\n</code>，而 Linux 中则是 <code>\n</code>，所以才会报 <code>\r’: command not found</code></p><p>如何解决？</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="comment"># 利用 vi 打开文件 xx.sh</span></span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="comment"># 打开分别输入指令</span></span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="comment"># 换行符转换</span></span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">:<span class="built_in">set</span> ff=unix</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="comment"># 保存退出</span></span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">:wq</span></span><br></pre></td></tr></tbody></table></figure><h2 id="安装-Pytorch"><a href="#安装-Pytorch" class="headerlink" title="安装 Pytorch"></a>安装 Pytorch</h2><p>就拿最近看的论文 FixMatch 的环境失手，代码需要：</p><ul><li>python 3.6+</li><li>torch 1.4</li><li>torchvision 0.5</li><li>tensorboard</li><li>numpy</li><li>tqdm</li><li>apex (optional)</li></ul><p>创建虚拟环境 fixmatch，这里我选择 python 版本为 3.7：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n fixmatch python=3.7</span><br><span class="line">conda activate fixmatch</span><br></pre></td></tr></tbody></table></figure><p>访问 Pytorch 的官网选择对应版本的 torch 和 torchvision 即可：</p><p><a href="https://pytorch.org/get-started/previous-versions/">Previous PyTorch Versions | PyTorch</a></p><p>下载 <code>torch==1.4.0</code> 和<code>torchvision==0.5.0</code>，这里需要加上后缀 <code>-f https://download.pytorch.org/whl/torch_stable.html</code>，因为配置下载源为清华源，好像在清华源中找不到这个版本，所以以后都直接从 Pytorch 官网下载即可：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==1.4.0 torchvision==0.5.0 -f https://download.pytorch.org/whl/torch_stable.html</span><br><span class="line">pip install tensorboard</span><br><span class="line">pip install tqdm</span><br></pre></td></tr></tbody></table></figure><p>测试是否可以使用，加入 python 环境，显示以下信息就算安装成功：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; import torch</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; torch.cuda.is_available()</span></span><br><span class="line">True</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; torch.__version__</span></span><br><span class="line">'1.4.0+cu92'</span><br></pre></td></tr></tbody></table></figure><h2 id="安装-Pytorch-和-mmdetection"><a href="#安装-Pytorch-和-mmdetection" class="headerlink" title="安装 Pytorch 和 mmdetection"></a>安装 Pytorch 和 mmdetection</h2><h1 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h1><h2 id="torch-函数使用"><a href="#torch-函数使用" class="headerlink" title="torch 函数使用"></a>torch 函数使用</h2><h3 id="torch-manual-seed"><a href="#torch-manual-seed" class="headerlink" title="torch.manual_seed()"></a>torch.manual_seed()</h3><p>在神经网络中，参数默认是进行随机初始化的。如果不设置的话每次训练时的初始化都是随机的，导致结果不确定。</p><p>如果设置初始化，则每次初始化都是固定的。</p><p>实际上，计算机并不能产生真正的随机数，而是已经编写好的一些无规则排列的数字存储在电脑里，把这些数字划分为若干相等的 N 份，并为每份加上一个编号，编号固定的时候，获得的随机数也是固定的。</p><p><code>torch.manual_seed(1)</code> 用于设置随机初始化的种子，即上述的编号，编号固定，每次获取的随机数固定。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      random.seed(args.seed) </span><br><span class="line">    　torch.manual_seed(args.seed)  <span class="comment">#为CPU设置种子用于生成随机数，以使结果是确定的 </span></span><br><span class="line">    　torch.cuda.manual_seed(args.seed) <span class="comment">#为当前GPU设置随机种子；</span></span><br><span class="line">    　cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    　</span><br><span class="line"><span class="comment"># 如果使用多个GPU，应该使用torch.cuda.manual_seed_all()为所有的GPU设置种子。</span></span><br></pre></td></tr></tbody></table></figure><h3 id="torch-distributed-barrier"><a href="#torch-distributed-barrier" class="headerlink" title="torch.distributed.barrier()"></a>torch.distributed.barrier()</h3><p>这是示例代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.local_rank <span class="keyword">not</span> <span class="keyword">in</span> [-<span class="number">1</span>, <span class="number">0</span>]:</span><br><span class="line">    torch.distributed.barrier()</span><br><span class="line">    </span><br><span class="line">labeled_dataset, unlabeled_dataset, test_dataset = DATASET_GETTERS[args.dataset](</span><br><span class="line">    args, <span class="string">'./data'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.local_rank == <span class="number">0</span>:</span><br><span class="line">    torch.distributed.barrier()</span><br></pre></td></tr></tbody></table></figure><p>关于参数 <code>local_rank</code> 的理解：</p><p>在使用多进程处理任务时，在 Python 中通常设置等级 0 是第一个进程或基本进程，然后对其他进程进行不同的排序，例如 1、2、3 加上基本进程 0，总共 4 个进程。</p><p>使用单进程时，往往会设置进程号为 <code>-1</code>，总而言之，当 <code>local_rank</code> 为 0/-1 时，认为它是主进程。</p><p>对于多进程任务，往往只需要一个进程<strong>预处理数据或者读取数据</strong>，而为了与其他进程共享数据，需要在主进程处理数据的时候让其他进程全部停下来等待。</p><p>上述的代码示例，实际的功能如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 询问当前进程是否为主进程</span></span><br><span class="line"><span class="keyword">if</span> args.local_rank <span class="keyword">not</span> <span class="keyword">in</span> [-<span class="number">1</span>, <span class="number">0</span>]:</span><br><span class="line">    <span class="comment"># 不是主进程，阻塞</span></span><br><span class="line">    <span class="comment"># 让该进程加入 barrier()</span></span><br><span class="line">    torch.distributed.barrier()</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 主进程读取数据</span></span><br><span class="line">labeled_dataset, unlabeled_dataset, test_dataset = DATASET_GETTERS[args.dataset](</span><br><span class="line">    args, <span class="string">'./data'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主进程读取数据完毕，主进程加入 barrier()</span></span><br><span class="line"><span class="keyword">if</span> args.local_rank == <span class="number">0</span>:</span><br><span class="line">    torch.distributed.barrier()</span><br></pre></td></tr></tbody></table></figure><p>当所有的进程都进入了 barrier() 后，Pytorch 就会打开所有的 barrier()，所有的进程都可以继续进行相关操作。</p><p>当需要进行阻塞操作时，模板即为：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.local_rank <span class="keyword">not</span> <span class="keyword">in</span> [-<span class="number">1</span>, <span class="number">0</span>]:</span><br><span class="line">    torch.distributed.barrier()</span><br><span class="line"></span><br><span class="line"><span class="string">'''主进程进行操作</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.local_rank == <span class="number">0</span>:</span><br><span class="line">    torch.distributed.barrier()</span><br></pre></td></tr></tbody></table></figure><p>总结：</p><ul><li>当进程遇到障碍时，它将阻塞</li><li>屏障的位置并不重要（例如，并非所有进程都必须输入相同的 if 语句）</li><li>一个进程被一个屏障阻塞，直到所有进程都遇到一个屏障，在这个屏障上为所有进程解除这些屏障</li></ul><h3 id="model-named-parameters"><a href="#model-named-parameters" class="headerlink" title="model.named_parameters()"></a>model.named_parameters()</h3><p><code>model.named_parameters()</code> 将会打印每一次迭代元素的名字和 <code>param</code>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = DarkNet([<span class="number">1</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">4</span>])</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name,param.requires_grad)</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">conv1.weight <span class="literal">True</span></span><br><span class="line">bn1.weight <span class="literal">True</span></span><br><span class="line">bn1.bias <span class="literal">True</span></span><br><span class="line">layer1.ds_conv.weight <span class="literal">True</span></span><br><span class="line">layer1.ds_bn.weight <span class="literal">True</span></span><br><span class="line">layer1.ds_bn.bias <span class="literal">True</span></span><br><span class="line">layer1.residual_0.conv1.weight <span class="literal">True</span></span><br><span class="line">layer1.residual_0.bn1.weight <span class="literal">True</span></span><br><span class="line">layer1.residual_0.bn1.bias <span class="literal">True</span></span><br><span class="line">layer1.residual_0.conv2.weight <span class="literal">True</span></span><br><span class="line">layer1.residual_0.bn2.weight <span class="literal">True</span></span><br><span class="line">layer1.residual_0.bn2.bias <span class="literal">True</span></span><br><span class="line">layer2.ds_conv.weight <span class="literal">True</span></span><br><span class="line">layer2.ds_bn.weight <span class="literal">True</span></span><br><span class="line">layer2.ds_bn.bias <span class="literal">True</span></span><br><span class="line">layer2.residual_0.conv1.weight <span class="literal">True</span></span><br><span class="line">layer2.residual_0.bn1.weight <span class="literal">True</span></span><br><span class="line">layer2.residual_0.bn1.bias <span class="literal">True</span></span><br><span class="line">....</span><br></pre></td></tr></tbody></table></figure><h3 id="torch-nn-parallel-DistributedDataParallel"><a href="#torch-nn-parallel-DistributedDataParallel" class="headerlink" title="torch.nn.parallel.DistributedDataParallel"></a>torch.nn.parallel.DistributedDataParallel</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""在分布环境中实现数据并行训练</span></span><br><span class="line"><span class="string">    torch.nn.parallel.DistributedDataParallel</span></span><br><span class="line"><span class="string">    - model 需要进行数据并行训练的模型对象</span></span><br><span class="line"><span class="string">    - device_ids=[args.local_rank] 指定在哪些设备上进行数据并行训练</span></span><br><span class="line"><span class="string">    - output_device=args.local_rank 指定输出的设备</span></span><br><span class="line"><span class="string">    - find_unused_parameters=True 指定是否查找未使用的参数。</span></span><br><span class="line"><span class="string">      当模型具有不同分支或子模块具有不同输入形状时，可能会出现未使用的参数。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">model = torch.nn.parallel.DistributedDataParallel(</span><br><span class="line">    model, device_ids=[args.local_rank],</span><br><span class="line">    output_device=args.local_rank, find_unused_parameters=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="tensor-chunk"><a href="#tensor-chunk" class="headerlink" title="tensor.chunk()"></a>tensor.chunk()</h3><p><code>.chunk()</code>是PyTorch张量的一个方法，用于将张量沿着指定维度进行均匀切分成多个子张量。</p><p>具体来说，<code>.chunk()</code>的功能如下：</p><ul><li>语法：<code>chunk(chunks, dim=0)</code></li><li>参数：<ul><li><code>chunks</code>：表示要切分的块数。</li><li><code>dim</code>：表示要在哪个维度上进行切分，默认为0（第一个维度）。</li></ul></li><li>返回值：返回一个元组，包含切分后的子张量。</li></ul><p><code>.chunk()</code>方法会将原始张量沿着指定维度进行均匀切分，每个子张量的大小相等（除非原始张量的大小无法被整除）。返回的结果是一个元组，其中包含切分后的子张量。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logits_u_w, logits_u_s = logits[batch_size:].chunk(<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure><p>在给定的代码中，<code>logits[batch_size:].chunk(2)</code>将张量<code>logits</code>从索引<code>batch_size</code>开始切分成两个子张量。这样可以将模型输出的结果划分为两个部分，分别赋值给<code>logits_u_w</code>和<code>logits_u_s</code>。</p><h3 id="tensor-detach"><a href="#tensor-detach" class="headerlink" title="tensor.detach()"></a>tensor.detach()</h3><p><code>.detach()</code>是PyTorch张量的一个方法，用于创建一个新的张量，其中包含了原始张量的数据，但没有建立与计算图的连接。</p><p>具体来说，<code>.detach()</code>的功能如下：</p><ul><li><code>.detach()</code>会返回一个新的张量，其中包含了原始张量的数据，但<u><strong>没有梯度信息和计算图的连接</strong></u>。这意味着通过<code>.detach()</code>得到的张量<u><strong>不会参与反向传播过程</strong></u>，<u><strong>不会对梯度进行求导</strong></u>。</li></ul><p>常见的使用场景包括：</p><ul><li>当你只对部分张量进行梯度计算时，可以使用<code>.detach()</code>将其与其他需要梯度的张量分离。</li><li>当你需要在不影响原始张量梯度的情况下，对张量进行操作或传递给其他函数。</li></ul><p>总而言之，<code>.detach()</code>方法可以用于生成不需要梯度的张量，并且在某些情况下，可以提高效率并避免梯度传播的影响。</p><h3 id="tensor-flatten-start-dim-x3D-1"><a href="#tensor-flatten-start-dim-x3D-1" class="headerlink" title="tensor.flatten(start_dim=1)"></a>tensor.flatten(start_dim=1)</h3><p><code>x.flatten(start_dim=1)</code> 表示将 <code>x</code> 沿着从 <code>start_dim</code> 开始的所有维度拉平成一个一维张量，返回的结果是一个新的张量。</p><p>具体来说，假设 <code>x</code> 的形状为 <code>(B, C, H, W)</code>，其中 <code>B</code> 表示 batch size，<code>C</code> 表示通道数，<code>H</code> 表示高度，<code>W</code> 表示宽度，那么 <code>x.flatten(start_dim=1)</code> 的结果是一个形状为 <code>(B, C*H*W)</code> 的张量，即将除了 <code>B</code> 以外的所有维度都拉平成一个维度。</p><p>如果 <code>start_dim</code> 是默认值 0，则相当于将整个张量拉平成一个一维张量，即形状为 <code>(B*C*H*W,)</code>。</p><p>如果 <code>start_dim</code> 是 2，假设 <code>x</code> 的形状为 <code>(B, C, H, W)</code>，则表示从第二个维度 <code>C</code> 开始将 <code>x</code> 沿着后面的维度拉平成一个一维张量，返回的结果是一个形状为 <code>(B, C, H*W)</code> 的张量。</p><p>具体来说，对于每个样本数据，张量中第二个维度 <code>C</code> 之后的所有维度都被拉平成为一个维度，而前两个维度 <code>B</code> 和 <code>C</code> 保持不变，因此结果的形状是 <code>(B, C, H*W)</code>。</p><p>举个例子，如果 <code>x</code> 的形状是 <code>(2, 3, 4, 5)</code>，那么 <code>x.flatten(start_dim=2)</code> 的结果将是一个形状为 <code>(2, 3, 20)</code> 的张量。其中，第 1 个样本的维度为 <code>(3, 4, 5)</code>，被拉平成了 <code>(3, 20)</code>；第 2 个样本的维度也是 <code>(3, 4, 5)</code>，被拉平成了 <code>(3, 20)</code>。因此结果的形状是 <code>(2, 3, 20)</code>。</p><h3 id="zip"><a href="#zip" class="headerlink" title="zip()"></a>zip()</h3><p>Python 中使用 <code>zip()</code> 函数同时循环遍历多个列表，例如：</p><p><code>for s, a in zip(self.sizes, self.aspect_ratios)</code>，</p><p>其中：</p><ul><li><code>self.sizes</code> 是一个包含 3 个元素的列表，对应 3 个预定义的尺度（以像素值的平方根为度量）</li></ul><p>  sizes=(128, 256, 512)</p><ul><li><p><code>self.aspect_ratios</code> 是一个包含 3 个元素的列表，对应 3 种预定义的宽高比</p><p>aspect_ratios=(0.5, 1.0, 2.0)</p></li></ul><p><code>zip(self.sizes, self.aspect_ratios)</code> 返回的是一个可迭代对象，每次迭代会输出 <code>self.sizes</code> 和 <code>self.aspect_ratios</code> 中<u><strong>对应位置</strong></u>的元素，即 <code>(s₁, a₁), (s₂, a₂), ...</code>。</p><h3 id="model-parameters-和-model-buffers"><a href="#model-parameters-和-model-buffers" class="headerlink" title="model.parameters() 和 model.buffers()"></a>model.parameters() 和 model.buffers()</h3><p>在 PyTorch 中，<code>model.parameters()</code>和<code>model.buffers()</code>都是用于获取模型中的参数和缓冲区的函数，但是它们获取的内容不同。</p><p><code>model.parameters()</code>函数返回模型中需要训练的参数。这些参数通常对应着网络的权重和偏置值，以及其他可更新的变量。调用<code>model.parameters()</code>函数通常用于定义优化器，从而在训练网络时更新这些参数。</p><p><code>model.buffers()</code>函数返回模型中所有不需要训练的缓冲区，比如BN层和移动平均层的移动平均值和方差。缓冲区也需要在模型定义时被注册，但是它们不需要参与训练，因此也不会影响梯度的计算和更新过程。调用<code>model.buffers()</code>函数通常用于模型的保存和恢复，以及其他需要获取缓冲区的操作。</p><h3 id="nn-BatchNorm2d"><a href="#nn-BatchNorm2d" class="headerlink" title="nn.BatchNorm2d"></a>nn.BatchNorm2d</h3><p>在 PyTorch 中，Batch Normalization 层（BN 层）是通过在模型中添加一个特殊的 nn.BatchNorm2d 类来实现的。其缓冲区包括：</p><ol><li>running_mean：用于计算训练过程中的均值，每次更新一批数据后就会更新。</li><li>running_var：用于计算训练过程中的方差，每次更新一批数据后就会更新。</li><li>weight：会被模型学习到的尺度因子。</li><li>bias：会被模型学习到的偏移参数。</li></ol><p>当模型向前传播时，BN 层会将输入数据按维度计算出均值和方差，然后使用这些均值和方差对数据进行归一化操作。归一化后的数据会受到 weight 和 bias 的影响，然后被传递给下一层作为输入。</p><p>注意到在添加了 BN 层后，我们在<u><strong>传递数据到下一层之前需要使用 ReLU 激活函数</strong></u>。</p><p>nn.BatchNorm2d 的实现细节如下：</p><p>假设输入的形状是 <code>(batch_size, channels, height, width)</code>，则 BatchNorm2d 层会按照 <code>channels</code> 维度计算每一个元素对应的均值和方差，接着使用这些均值和方差对输入数据进行归一化。归一化后的数据会受到权重参数 <code>weight</code>（形状为 <code>(channels,)</code>）和偏置参数 <code>bias</code>（形状也是 <code>(channels,)</code>）的影响，最终输出形状也是 <code>(batch_size, channels, height, width)</code>。</p><h3 id="torch-unique-consecutive-args-kwargs"><a href="#torch-unique-consecutive-args-kwargs" class="headerlink" title="torch.unique_consecutive(*args, **kwargs)"></a>torch.unique_consecutive(*args, **kwargs)</h3><p>参数：</p><ul><li><strong>input</strong>(<a href="https://vimsky.com/cache/index.php?source=https://pytorch.org/docs/stable/tensors.html%23torch.Tensor"><em>Tensor</em></a>) -输入张量</li><li><strong>return_inverse</strong>(<a href="https://vimsky.com/cache/index.php?source=https://docs.python.org/3/library/functions.html%23bool"><em>bool</em></a>) -是否还返回原始输入中的元素在返回的唯一列表中结束的位置的索引。</li><li><strong>return_counts</strong>(<a href="https://vimsky.com/cache/index.php?source=https://docs.python.org/3/library/functions.html%23bool"><em>bool</em></a>) -是否还返回每个唯一元素的计数。</li><li><strong>dim</strong>(<a href="https://vimsky.com/cache/index.php?source=https://docs.python.org/3/library/functions.html%23int"><em>int</em></a>) -要应用唯一的维度。如果 <code>None</code> ，则返回展平输入的唯一性。默认值：<code>None</code></li></ul><p>返回：</p><p>一个张量或一个张量元组包含</p><ul><li>输出(张量)：唯一标量元素的输出列表。</li><li><strong>inverse_indices</strong>(张量)：(可选)如果<code>return_inverse</code>为 True，将有一个额外的返回张量(与输入的形状相同)表示原始输入中的元素映射到输出中的位置的索引；否则，此函数将仅返回一个张量。</li><li><strong>计数</strong>(张量)：(可选)如果<code>return_counts</code>为 True，将有一个额外的返回张量(与 output 或 output.size(dim) 的形状相同，如果指定了 dim )表示每个唯一值或张量的出现次数。</li></ul><p>例子：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = torch.unique_consecutive(x)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, inverse_indices = torch.unique_consecutive(x, return_inverse=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>inverse_indices</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, counts = torch.unique_consecutive(x, return_counts=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>counts</span><br><span class="line">tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></tbody></table></figure><h3 id="torch-cumsum"><a href="#torch-cumsum" class="headerlink" title="torch.cumsum"></a>torch.cumsum</h3><p><code>torch.cumsum</code> 是 PyTorch 中的一个函数，用于计算张量元素的累加和。</p><p>具体来说，对于一个一维张量（长度为 n），<code>torch.cumsum(input, dim=0)</code> 返回一个同样长度的张量，其中每个位置的元素为原张量中该位置及之前位置的元素的累加和。例如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">b = torch.cumsum(a, dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># output: tensor([ 1,  3,  6, 10, 15])</span></span><br></pre></td></tr></tbody></table></figure><p>对于一个多维张量，<code>torch.cumsum(input, dim)</code> 会沿着指定维度 dim 进行累加。例如，对于一个二维张量（shape 为 (m, n)），<code>torch.cumsum(input, dim=0)</code> 会返回一个与原张量尺寸相同的张量，其中每列都沿着行方向进行元素累加。</p><p>具体实现可以参考 PyTorch 文档中的说明：<a href="https://pytorch.org/docs/stable/generated/torch.cumsum.html">torch.cumsum</a>。</p><h2 id="统计模型参数"><a href="#统计模型参数" class="headerlink" title="统计模型参数"></a>统计模型参数</h2><p>首先对于计算模型的FLOPs而言，fvcore是一个易用的工具。fvcore是Facebook开源的一个轻量级的核心库，它提供了各种计算机视觉框架中常见且基本的功能。其中就包括了统计模型的参数以及FLOPs等。fvcore项目的开源地址是：</p><ul><li><a href="https://github.com/facebookresearch/fvcore">facebookresearch/fvcore: Collection of common code that’s shared among different research projects in FAIR computer vision team. (github.com)</a></li></ul><p>如果需要使用fvcore，首先需要安装：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install fvcore</span><br></pre></td></tr></tbody></table></figure><p>对于FlOPs，我们先解释一下其概念：</p><ul><li>FLOPS：注意全大写，是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。</li><li>FLOPs：注意s小写，是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。</li><li><strong>注意，模型的参数量少不代表FLOPs低，理论的FLOPs低也不代表实际的推理速度快</strong>。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet50</span><br><span class="line"><span class="keyword">from</span> fvcore.nn <span class="keyword">import</span> FlopCountAnalysis, parameter_count_table</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_models_param_nums</span>(<span class="params">model</span>):</span><br><span class="line">  <span class="string">"""counts and prints the number of models parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">      model (_type_): model</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  total = <span class="built_in">sum</span>([param.numel() <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters()])</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">'  + Number of params: %.2fM'</span> % (total / <span class="number">1e6</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_models_param_nums</span>(<span class="params">model</span>):</span><br><span class="line">  <span class="string">"""counts and the number of models parameters"""</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()) / <span class="number">1e6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  <span class="comment"># 创建 resnet50 网络</span></span><br><span class="line">  model = resnet50(num_classes=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 创建输入网络的 tensor</span></span><br><span class="line">  tensor = (torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>),)</span><br><span class="line">  <span class="comment"># 分析 FLOPS</span></span><br><span class="line">  flops = FlopCountAnalysis(model, tensor)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">"Flops: {}"</span>.<span class="built_in">format</span>(flops.total()))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 计算模型的参数量</span></span><br><span class="line">  <span class="built_in">print</span>(parameter_count_table(model))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 自定义函数统计模型的参数量</span></span><br><span class="line">  count_models_param_nums(model)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 输出模型的总参数</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">"Total params: {:.2f}M"</span>.<span class="built_in">format</span>(count_models_param_nums(model)))</span><br></pre></td></tr></tbody></table></figure><h2 id="封装数据集"><a href="#封装数据集" class="headerlink" title="封装数据集"></a>封装数据集</h2><h2 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h2><h3 id="多卡训练"><a href="#多卡训练" class="headerlink" title="多卡训练"></a>多卡训练</h3><p>使用多卡训练的方式有很多，当然前提是我们的设备中存在两个及以上的GPU：使用命令 <code>nvidia-smi</code> 查看当前Ubuntu平台的GPU数量，在我们设备中确实存在多卡的条件下，最简单的方法是直接使用 <code>torch.nn.DataParallel</code> 将模型wrap一下即可：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = torch.nn.DataParallel(model)</span><br></pre></td></tr></tbody></table></figure><p>这时，默认所有存在的显卡都会被使用。</p><p>如果机子中有很多显卡，但只想使用0、1、2号显卡，那么可以：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">','</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]))</span><br><span class="line">net = torch.nn.DataParallel(model)</span><br><span class="line"><span class="comment"># CUDA_VISIBLE_DEVICES 表示当前可以被python环境程序检测到的显卡</span></span><br></pre></td></tr></tbody></table></figure><h3 id="权重衰退（weight-decay）"><a href="#权重衰退（weight-decay）" class="headerlink" title="权重衰退（weight_decay）"></a>权重衰退（weight_decay）</h3><blockquote><p>正则化：凡事可以减少泛化误差而不是减少训练误差的方法，都可以称作正则化方法。</p><p>权重衰退是一种最常见的处理过拟合的方法，通常也被称为 L2 正则化</p></blockquote><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/20210709122114223.png" alt="img"></p><p><strong>权值衰减</strong> 是一直以来经常被使用的一种抑制过拟合的方法。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。为损失函数加上权重的平方范数（L2 范数），就可以抑制权重变大，λ 是控制正则化强度的超参数，λ 设置得越大，对大的权重施加的惩罚就越重。</p><p>以下是一段示例代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">no_decay = [<span class="string">'bias'</span>, <span class="string">'bn'</span>]</span><br><span class="line">grouped_parameters = [</span><br><span class="line">    {<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(</span><br><span class="line">        nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">'weight_decay'</span>: args.wdecay},</span><br><span class="line">    {<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> <span class="built_in">any</span>(</span><br><span class="line">        nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">'weight_decay'</span>: <span class="number">0.0</span>}</span><br><span class="line">]</span><br><span class="line">optimizer = optim.SGD(grouped_parameters, lr=args.lr,</span><br><span class="line">                      momentum=<span class="number">0.9</span>, nesterov=args.nesterov)</span><br></pre></td></tr></tbody></table></figure><ol><li><p><code>no_decay</code> 表示不应用权重衰减（weight decay）的参数，其中，偏置（bias）和批归一化（batch normalization）参数不会受到权重衰减的影响</p></li><li><p><code>grouped_parameters</code> 中：</p><ul><li><code>params</code> 一个包含参数（parameter）的列表，根据特定的条件选择模型中的参数，第一个字典中选择的是受权重衰减影响的参数，第二个字典中选择的是不受权重衰减影响的参数</li><li><code>weight_decay</code> 权重衰减的值</li></ul></li><li><p><code>optimizer</code> 是一个使用随机梯度下降（Stochastic Gradient Descent, SGD）算法进行优化的对象，接受以下参数：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SGD</span>(</span><br><span class="line">    params: _params_t,</span><br><span class="line">    lr: <span class="built_in">float</span>,</span><br><span class="line">    momentum: <span class="built_in">float</span> = ...,</span><br><span class="line">    dampening: <span class="built_in">float</span> = ...,</span><br><span class="line">    weight_decay: <span class="built_in">float</span> = ...,</span><br><span class="line">    nesterov: <span class="built_in">bool</span> = ...</span><br><span class="line">)</span><br><span class="line"><span class="string">"""Implements stochastic gradient descent (optionally with momentum).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Nesterov momentum is based on the formula from On the importance of initialization and momentum in deep learning__.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">                parameter groups</span></span><br><span class="line"><span class="string">            lr (float): learning rate</span></span><br><span class="line"><span class="string">            momentum (float, optional): momentum factor (default: 0)</span></span><br><span class="line"><span class="string">            weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string">            dampening (float, optional): dampening for momentum (default: 0)</span></span><br><span class="line"><span class="string">            nesterov (bool, optional): enables Nesterov momentum (default: False)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Example:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.zero_grad()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss_fn(model(input), target).backward()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.step()</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure></li></ol><h3 id="虚拟机后台训练"><a href="#虚拟机后台训练" class="headerlink" title="虚拟机后台训练"></a>虚拟机后台训练</h3><p>只需要在以前训练的指令前增加 nohup 命令，同时在结尾加上 &amp; 符号即可：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nohup python -m visdom.server &amp;  # 后台运行visdom.server</span><br><span class="line">nohup python train.py &amp;  # 使模型在后台训练</span><br></pre></td></tr></tbody></table></figure><p>我们可以通过 jobs -l 来查看进程，在训练结束后通过 kill 指令关闭后台进程：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jobs -l  # 查看进程</span><br><span class="line">kill -9 PID  # 通过进程的PID关闭进程</span><br></pre></td></tr></tbody></table></figure><p>当终端链接断开，重新链接后无法通过 jobs 命令查看后台进程，此时需要通过 ps ux 指令查看所有的进程的 PID，然后通过 kill 指令关闭进程：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps ux  # 查看所有进程的PID</span><br><span class="line">kill -9 PID  # 关闭特定进程</span><br></pre></td></tr></tbody></table></figure><p>另外一点需要注意的时，通过后台运行程序的所有输入都会存储到 nohup.out 的文件中，如果不清理的话，该文件会不断增加，nohup.out 文件默认存放在当前执行脚本所在的目录中，也可以同过指令修改存放位置：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改nohup.out文件存放位置</span></span><br><span class="line">nohup python train.py &gt; /path/to/custom.out &amp;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在不停止进程的情况下清空nohup.out文件的指令（以下两个指令任选一个即可）</span></span><br><span class="line">cp /dev/null nohup.out</span><br><span class="line">cat /dev/null &gt; nohup.out</span><br></pre></td></tr></tbody></table></figure><p>例如：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(fixmatch) root@be6fec86789a:~/Tz/FixMatch-pytorch# nohup python train.py --dataset cifar10 --num-labeled 4000 --arch wideresnet --batch-size 64 --lr 0.03 --expand-labels --seed 5 --out results/cifar10@4000.5 &gt; /root/Tz/FixMatch-pytorch/log/fixmatch_cifar10_4000_5.out &amp;</span><br><span class="line">[1] 3913</span><br><span class="line">nohup: ignoring input and redirecting stderr to stdout</span><br><span class="line">(fixmatch) root@be6fec86789a:~/Tz/FixMatch-pytorch# jobs -l</span><br><span class="line">[1]+  3913 Running                 nohup python train.py --dataset cifar10 --num-labeled 4000 --arch wideresnet --batch-size 64 --lr 0.03 --expand-labels --seed 5 --out results/cifar10@4000.5 &gt; /root/Tz/FixMatch-pytorch/log/fixmatch_cifar10_4000_5.out &amp;</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 环境配置 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FixMatch</title>
      <link href="/posts/e378f85.html"/>
      <url>/posts/e378f85.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>Simplifying Semi-Supervised Learning with Consistency and Confidence</p></blockquote><p>会议：NeurIPS 2020</p><p>论文地址：<a href="https://arxiv.org/abs/2001.07685">FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</a></p><p>Pytorch 复现：</p><ul><li><a href="https://github.com/kekmodel/FixMatch-pytorch">kekmodel/FixMatch-pytorch</a></li><li><a href="https://github.com/CoinCheung/fixmatch-pytorch">CoinCheung/fixmatch-pytorch</a></li><li><a href="https://github.com/valencebond/FixMatch_pytorch">valencebond/FixMatch_pytorch:</a></li></ul><p>参考文章：</p><ul><li><a href="https://amitness.com/2020/07/semi-supervised-learning/">Semi-Supervised Learning in Computer Vision</a></li></ul><h2 id="主要工作"><a href="#主要工作" class="headerlink" title="主要工作"></a>主要工作</h2><p>FixMatch 结合了半监督学习之前的工作，结合<u><strong>伪标签（ pseudo-label）</strong></u>和 <u><strong>一致性正则（Consistency Regularization）</strong></u>，并且它更简单，相较于之前在半监督学习的工作，比如：UDA 和 ReMixMatch，如下图所示，FixMatch 在有标签数据上训练，将预测所得到的概率分布与真实标签计算损失 <code>L1</code>；</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522192955482.png" alt="image-20230522192955482"></p><p>对于无标签数据，FixMatch 做了两次数据增强，这是一个相较于其他工作特别的地方，FixMatch 分别对无标签数据做了<code>弱增强（Weakly Augmented）</code>和<code>强增强（Strongly Augmented）</code>，对于弱增强后的数据通过模型预测得到一个概率分布，并且，对这一概率分布做一特殊处理，最概率分布中最大的概率，设置一个阈值，若该概率大于该阈值，那么则将其纳入损失函数之中，并且，使用 <code>arg max</code>使其分布变为 <code>One-Hot</code>形式，接着与强增强后的数据通过模型预测得到的分布作交叉熵得到损失<code>L2</code>，最后通过加权和得到最终的损失。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522192925406.png" alt="image-20230522192925406"></p><p>其中 L1、L2 以及最终的损失如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522193829744.png" alt="image-20230522193829744"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522193843142.png" alt="image-20230522193843142"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522193917729.png" alt="image-20230522193917729"></p><p>这里提一下，在计算有标签数据，将对对其作一些<u><strong>弱增强</strong></u>的操作，一个很有意思的结果，对于权重系数 <code>λ</code>，在先前的工作中都显示对权重系数慢慢提升是一个很重要的部分，但在 FixMatch 中，提升权重系数这一部分以及包含在算法当中，就不需要额外进行一些计算将权重系数慢慢增大：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522194843295.png" alt="image-20230522194843295"></p><p>不需要添加这个权重系数的原因是：在损失函数中 <code>max(qb)</code>在早期训练中绝大部分数据通过模型预测得到的最大概率值都会比阈值<code>τ</code>小，在训练的过程中，模型的预测输出将会越来越“自信”，而将会有越来越多的数据样本的 <code>max(qb) &gt; τ </code>。</p><h2 id="数据增强方式"><a href="#数据增强方式" class="headerlink" title="数据增强方式"></a>数据增强方式</h2><h3 id="弱增强（Weak-Augmentation）"><a href="#弱增强（Weak-Augmentation）" class="headerlink" title="弱增强（Weak Augmentation）"></a>弱增强（Weak Augmentation）</h3><p>论文中使用了简单的<u><strong>翻转和平移（flip-and-shift）</strong></u>策略：</p><ul><li><p><strong>Random Horizontal Flip</strong></p><p><img src="https://amitness.com/images/fixmatch-horizontal-flip-gif" alt="Example of Random Horizontal Flip"></p><p>随机水平翻转，论文中使用了设置其概率为 0.5，对于 SVHN 数据集不适用该增强方式，因为 <u><strong>SVHN 数据集为都是数字</strong></u>，对数字进行翻转，没有意义，以下是基于 Pytorch 实现：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">im = Image.<span class="built_in">open</span>(<span class="string">'dog.png'</span>)</span><br><span class="line">weak_im = transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>)(im)</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>Random Vertical and Horizontal Translation</strong></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-translate.gif" alt="Example of Random Vertical and Horizontal Translation"></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">im = Image.<span class="built_in">open</span>(<span class="string">'dog.png'</span>)</span><br><span class="line">resized_im = transforms.Resize(<span class="number">32</span>)(im)</span><br><span class="line">translated = transforms.RandomCrop(size=<span class="number">32</span>, </span><br><span class="line">                                 padding=<span class="built_in">int</span>(<span class="number">32</span>*<span class="number">0.125</span>), </span><br><span class="line">                                 padding_mode=<span class="string">'reflect'</span>)(resized_im)</span><br></pre></td></tr></tbody></table></figure></li></ul><h3 id="强增强（Strong-Augmentation）"><a href="#强增强（Strong-Augmentation）" class="headerlink" title="强增强（Strong Augmentation）"></a>强增强（Strong Augmentation）</h3><p>FixMatch 对数据样本运用了 <code>RandAugment </code> 和 <code>CTAugment </code>后，在对其使用 <code>CutOut </code>增强方式。</p><ul><li><p>Cutout</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-cutout.gif" alt="Example of Cutout Augmentation"></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># Image of 520*520</span></span><br><span class="line">im = torch.rand(<span class="number">3</span>, <span class="number">520</span>, <span class="number">520</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fill cutout with gray color</span></span><br><span class="line">gray_code = <span class="number">127</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ratio=(1, 1) to set aspect ratio of square</span></span><br><span class="line"><span class="comment"># p=1 means probability is 1, so always apply cutout</span></span><br><span class="line"><span class="comment"># scale=(0.01, 0.01) means we want to get cutout of 1% of image area</span></span><br><span class="line"><span class="comment"># Hence: Cuts out gray square of 52*52</span></span><br><span class="line">cutout_im = transforms.RandomErasing(p=<span class="number">1</span>, </span><br><span class="line">                                 ratio=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">                                 scale=(<span class="number">0.01</span>, <span class="number">0.01</span>), </span><br><span class="line">                                 value=gray_code)(im)</span><br></pre></td></tr></tbody></table></figure></li><li><p>AutoAugment</p></li><li><p>RandAugment</p></li></ul><p>后面两个增强方式具体还没有看他的论文，就不介绍了…</p><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>对有标签的数据使用的 Batch 大小为 <code>B</code>，对于无标签的数据所使用的 Batch 大小为 <code>μB</code>，其中 <code>μ</code> 为超参数，作者使用 <code>μ = 7</code> 用于模型的训练：</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522201443131.png" alt="image-20230522201443131" style="zoom:80%;"><p>有监督学习部分，使用交叉熵计算损失函数：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-supervised-loss.png" alt="Supervised Part of FixMatch"></p><p>对未标记数据制作伪标签：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-pseudolabel.png" alt="Generating Pseudolabels in FixMatch"></p><p>计算一致性损失，所使用的损失函数也为交叉熵：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-strong-aug-loss.png" alt="Consistency Regularization in FixMatch"></p><p>对于上述所提到的权重系数 <code>λ</code>，在训练过程中，应用与训练的无标签数据将会越来越多，如下图所示，模型就好像是小孩子学习一般，从简单在到复杂：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-curriculum-learning.png" alt="Free Curriculum Learning in FixMatch"></p><p>算法如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522201915468.png" alt="image-20230522201915468"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522204054364.png" alt="image-20230522204054364"></p><p>在论文中，作者做了一个很有意思的实验，即只取 10 个有标签的数据（在 CIFAR-10 中，即每一个类别<u><strong>只取 1 张图片</strong></u>），作者随机挑选了 4 次，构建了 4 个不同的数据集，模型的准确率（中位数）为 64.28%（准确率范围为：48.58% - 85.32%）。</p><p>并且，训练数据集之间的差异并不是很大，理应得到的准确率应该相差不大，但作者发现了，4 个模型在第一个数据集中训练得到的准确率都达到了 61% - 67%，而在第二个数据集中准确率在 68% - 75%。</p><p>作者认为：训练的不稳定性与<u><strong>每个数据集中 10 个标记数据的质量</strong></u>有关，采样得到的低质量的标记数据，可能会让模型很难有效的学习信息，为了验证这一个观点，作者构建了 8 个新的训练数据集（各个类别中最有代表性的有标签数据 -&gt; … -&gt; 各个类别中最没有代表性的有标签数据）进行训练，结果如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-1-label-example.png" alt="Learning with just 1 image per class"></p><p>结果和假设一样，基于最有代表性的有标签数据组成的数据集训练达到了 78% 的准确率，而最没有代表性的有标签数据则为 10%，因此，可以验证，<u><strong>标记数据的质量好坏</strong></u>决定了模型的好坏。</p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p>由于 FixMatch 模型很简单，很容易就能实现（指代码量少），作者对与先前的工作未提到的 优化器（Optimizer）和 学习率更新的方式（ Learning Rate Schedule）花了很多的功夫研究。</p><h3 id="Ablation-Study-on-Optimizer"><a href="#Ablation-Study-on-Optimizer" class="headerlink" title="Ablation Study on Optimizer"></a>Ablation Study on Optimizer</h3><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522204944126.png" alt="image-20230522204944126"></p><p>作者在实验中发现：<u><strong>不同的优化器以及优化器中超参数的选择</strong></u>对性能有着很大的影响，并且，Adam 的性能并不比 momentum SGD 好，原因可能在于模型<u><strong>对于学习率的变化很敏感</strong></u>，如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522205017378.png" alt="image-20230522205017378"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522205036867.png" alt="image-20230522205036867"></p><h3 id="Ablation-Study-on-Learning-Rate-Schedule"><a href="#Ablation-Study-on-Learning-Rate-Schedule" class="headerlink" title="Ablation Study on Learning Rate Schedule"></a>Ablation Study on Learning Rate Schedule</h3><p>上文中所提到，作者发现模型对于学习率的变化很敏感，作者发现，在选择合适的衰减方式的同时，选择合适的衰减率也很重要：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522205424281.png" alt="image-20230522205424281"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522205529762.png" alt="image-20230522205529762"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上是我觉得比较有意思的地方，具体可细看论文。</p>]]></content>
      
      
      <categories>
          
          <category> 半监督学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Semi-Surprised-Learning </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> Hybrid Methods </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unsupervised-Data-Augmentation</title>
      <link href="/posts/77eb0ac8.html"/>
      <url>/posts/77eb0ac8.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>Unsupervised Data Augmentation for Consistency Training</p></blockquote><p>会议：NeurIPS 2019</p><p>论文地址：<a href="https://arxiv.org/abs/1904.12848v2"> Unsupervised Data Augmentation for Consistency Training</a></p><p>Pytorch 复现：<a href="https://github.com/sndnyang/vat_pytorch">sndnyang/vat_pytorch</a></p><p>参考文章：</p><ul><li><a href="https://amitness.com/2020/07/semi-supervised-learning/">Semi-Supervised Learning in Computer Vision</a></li><li><a href="https://blog.csdn.net/by6671715/article/details/122706003">码侯烧酒的博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/369023559">Unsupervised Data Augmentation for Consistency Training </a></li></ul><p>参考视频：</p><ul><li><a href="https://www.bilibili.com/video/BV1Ab4y1S7i5/?spm_id_from=333.999.0.0&amp;vd_source=7801e0201bc8ddcb3d951c7388a08c3e">数最后一名</a></li></ul><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>最近的半监督学习方式都是基于一致性学习（consistency training）的，尽管成果不错，但是它们添加噪声的方式都比较初级，大多数都是基于 Gaussian noise， dropout noise 或者是 adversarial noise。因此，作者想引入一些更高级的数据增强方式，尤其是那些在监督学习中已经证明有效的方法，看看它们在半监督学习中是否也是有效的。</p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>少样本标注情况下，充分利用无标注数据，使其尽可能达到有充分标注的效果。</p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>根据一致性训练的原则，认为对一个样本进行较好的数据增强后，预测标签不应该发生变化，因此一般来说会有两个部分的 loss：（1）标注样本的损失，一般采用<code>交叉熵</code>；（2）未标注样本的损失，<code>KL 散度</code>等。</p><p>流程图如下所示，对有标签样本计算损失 L1，对无标签样本执行一次预测得到一个结果，再对改变本做一次特殊的数据增强后在进行预测得到一个结果，将两个结果计算一致性损失 L2，将 L1 与 L2 计算加权和。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519192319207.png" alt="image-20230519192319207"></p><p>数学形式说明该流程，输入 <code>x</code>，计算输入分布 <code>p_θ(y|x)</code> 和带噪声的分布 <code>p_θ(y|x, ɛ)</code>，然后最小化两个分布之间的距离：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519193105506.png" alt="image-20230519193105506"></p><p>上述图片对应的损失函数如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519193157993.png" alt="image-20230519193157993"></p><p>其中，<code>λ</code> 为平衡有监督学习和一致性训练之间的损失，<code>CE</code> 为交叉熵，<code>f*(.)</code> 为索要训练的最终目标模型。</p><p><u><strong>但有一点很奇怪</strong></u>，作者在之前的版本中计算一致性损失所采用的计量方式为 KL 散度，但是在最新版本又换回了 Cross-Entropy，不懂这是为什么…可能看效果而异吧。</p><p>不过有的人说：</p><blockquote><p>MSE 和 KL 散度各有优劣，二者的选取与数据集的实际分布特征关系很大，在实践中不妨进行对比测试</p></blockquote><p>在论文中，作者正对计算机视觉和自然语言任务提出了不同的对应增强方法，其中对于图片的增强方式，作者使用了一个增强方法 <u><strong>RandAugment</strong></u>，（只了解了大概，具体论文没看，好像是从强化学习中的 AutoAugment 衍生而来的）。不同任务的效果如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519193829228.png" alt="image-20230519193829228"></p><h2 id="训练技巧（Tricks）"><a href="#训练技巧（Tricks）" class="headerlink" title="训练技巧（Tricks）"></a>训练技巧（Tricks）</h2><h3 id="数据层面"><a href="#数据层面" class="headerlink" title="数据层面"></a>数据层面</h3><ol><li><p>基于置信度的掩码（Confidence-based masking）</p><p>对预测效果不好的样本（指针对一致性预测的原始样本），即置信度（最大的样本的概率）小于一定阈值数据，不计入一致性损失。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519194350155.png" alt="image-20230519194350155"></p></li><li><p>输出分布锐化（Sharpening Predictions）</p><p>降低预测结果的熵对训练有好处，所以作者对无标签的预测见过做<u><strong>锐化</strong></u>，采用的方法为 <u><strong>low Softmax temperature τ</strong></u>，与方法一相结合，在 batch 大小为 B 的情况下，损失为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519194803301.png" alt="image-20230519194803301"></p></li><li><p>域外相关数据的选择（Domain-relevance Data Filtering）</p><p>无标签数据量大，其中的数据分布往往是不均衡且有大量任务无关数据，为了解决这个问题，论文提出一种通用的无标签数据分布均衡化策略。首先利用有标签数据训练一个初始化模型，然后去预测所有无标签数据，根据置信度均衡选择各类别数据。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519194958974.png" alt="image-20230519194958974"></p></li></ol><h3 id="训练层面"><a href="#训练层面" class="headerlink" title="训练层面"></a>训练层面</h3><p>Training Signal Annealing（训练信号退火）</p><p>主要是针对标签数据与未标签数据不平衡时的场景，由于有大量的未标签数据需要UDA 处理，所以需要一个较大模型，但是由于较大模型很容易在少量标签数据下过拟合，所以，提出了本方法用于解决该问题。</p><p>基本原理就是在训练过程中，随着未标签数据的增加，逐步去除带标签数据，从而避免模型过拟合到带标签的训练数据。具体而言，就是在训练的 <code>t</code> 时刻，设置一个阈值 <code>ηt</code>，当 <code>1/K ≤ ηt ≤ 1</code>，其中，<code>K</code> 是类别数，当某个标签数据计算的 <code>p_θ(y∗| x)</code>大于阈值<code>ηt</code>，将该标签数据移除出计算损失的过程，而只计算 miniBatch 里面的其余数据。</p><p>具体策略有 3 种：</p><ul><li>log-schedule</li><li>linear-schedule</li><li>exp-schedule</li></ul><p>对于 labeled 数据量少，容易过拟合情况，选择最后一种；对于 labeled 数据量较多，过拟合不严重，可视情况选择前两种。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519195814680.png" alt="image-20230519195814680"></p><h2 id="理论方面的探讨（Theoretical-Analysis）"><a href="#理论方面的探讨（Theoretical-Analysis）" class="headerlink" title="理论方面的探讨（Theoretical Analysis）"></a>理论方面的探讨（Theoretical Analysis）</h2><blockquote><p>理论渣渣只能意会不可用文字整理出来，具体看论文吧…</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>给人的感觉这一篇好“工程”😂，在数据方面做了很多文章，但半监督领域又好像都是在对数据这一方面做很多“动作”，不管了，接着看，接着读。</p>]]></content>
      
      
      <categories>
          
          <category> 半监督学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Semi-Surprised-Learning </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> Data Augmentation </tag>
            
            <tag> Consistency Regularization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Virtual Adversarial Training</title>
      <link href="/posts/cc87ae2e.html"/>
      <url>/posts/cc87ae2e.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>A Regularization Method for Supervised and Semi-Supervised Learning</p></blockquote><p>期刊：IEEE Transactions on Pattern Analysis and Machine Intelligence (2018)</p><p>论文地址：<a href="https://arxiv.org/abs/1704.03976"> Virtual Adversarial Training</a></p><p>Pytorch 复现：<a href="https://github.com/sndnyang/vat_pytorch">sndnyang/vat_pytorch</a></p><p>参考文章：</p><ul><li><a href="https://amitness.com/2020/07/semi-supervised-learning/">Semi-Supervised Learning in Computer Vision</a></li><li><a href="https://blog.csdn.net/by6671715/article/details/122723657"> 码侯烧酒的博客</a></li><li><a href="https://facico.blog.csdn.net/article/details/114634056?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-114634056-blog-119420855.235%5Ev36%5Epc_relevant_default_base&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-114634056-blog-119420855.235%5Ev36%5Epc_relevant_default_base&amp;utm_relevant_index=1">Facico的博客</a></li></ul><p>参考视频：</p><ul><li><a href="https://www.bilibili.com/video/BV1j64y1v7p4?p=1&amp;vd_source=7801e0201bc8ddcb3d951c7388a08c3e">数最后一名</a></li></ul><h2 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h2><p>论文的关键想法同样保持着<u><strong>一致性正则化</strong></u>的思想，只不过将图像的增强方式以生成<u><strong>对抗样本（Adversarial Example）</strong></u>来代替之，有标签的损失采用<u><strong>交叉熵（Cross-Entropy）</strong></u>计算，无标签的损失使用 <u><strong>KL 散度（KL Divergence）</strong></u>来计算，最后施以对应的权重得到最终的损失，大致流程图如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517202628300.png" alt="image-20230517202628300"></p><h2 id="一致性正则化"><a href="#一致性正则化" class="headerlink" title="一致性正则化"></a>一致性正则化</h2><p>具体来说，基于平滑假设和聚类假设，具有不同标签的数据点在低密度区域分离，并且相似的数据点具有相似的输出。如果对一个未标记的数据应用实际的扰动，其预测结果不应该发生显著变化，也就是输出具有一致性。其数学表达如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517203120942.png" alt="image-20230517203120942"></p><p>其中，<code>D</code> 为度量函数，一般采用 <code>KL 散度</code> 或者 <code>JS 散度</code>，当然也可以使用交叉熵或者平方误差等，<code>Augment(·)</code> 是指数据增强函数，当采用数据增强是，视为对样本/模型添加一些噪声扰动，<code>θ</code> 为模型参数。</p><p>常见的数据增强有以下：</p><ul><li>常规的数据增强：平移旋转，随机 dropout等</li><li>时序移动平均：Temporal Ensembling，Mean-Teacher 中使用的方法</li><li>对抗样本扰动：VAT</li><li>高级数据增强：<a href="https://arxiv.org/abs/1904.12848v2">UDA</a></li></ul><h2 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h2><p>对抗训练（Adversarial Training）是增强神经网络鲁棒性的重要方式，在对抗训练的过程中，样本会被混合一些微小的扰动（哪怕改变很小，但很有可能会造成错误分类），使神经网络适应这种改变，从而对对抗样本具有鲁棒性。</p><p>生成对抗样本有以下几种常见方式：</p><ul><li><p>基于梯度法: <a href="https://arxiv.org/abs/1412.6572">Explaining and Harnessing Adversarial Examples</a></p></li><li><p>基于超平面分类: <a href="https://arxiv.org/abs/1511.04599">DeepFool: a simple and accurate method to fool deep neural networks</a></p></li><li><p>对抗攻击（Adversarial Attack）：在模型原始输入上添加对抗扰动</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/2020101817001880.png" alt="img"></p></li></ul><p>Goodfellow 对对抗训练损失函数定义如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517204302784.png" alt="image-20230517204302784"></p><p>其中，<code>D</code> 是衡量两个分布相似度的函数，<code>q(y|xl)</code> 是样本的真实分布，<code>p(y|xl, θ)</code> 是由参数和 <code>xl</code> 生成的预测分布，通过增加扰动 <code>radv</code> 来使得两个分布尽量相似。</p><p>对抗为什么可行？</p><ul><li>因为很多网络被设计得十分“线性”，像 LSTM 这样的，对x的每个维度都做微小扰动，当x的维度变大的时候，会对网络造成较大的影响</li><li>网络的线性，使得高阶导近似0，Taylor 展开后占主导的是线性的部分，所以用来干扰的主要就是对抗样本中线性的部分</li></ul><p>通常，我们无法获得精确对抗性扰动的<u><strong>闭式解</strong></u>，不过可以通过上式中的度量 D 来线性近似 r，使用 <code>L2</code> 正则是，对抗扰动可以通过下面的式子近似：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517205056447.png" alt="image-20230517205056447"></p><p>使用 <code>L∞</code> 正则时，可通过下面的式子近似：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517205330497.png" alt="image-20230517205330497"></p><p>其中 <code>g</code> 为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517205353604.png" alt="image-20230517205353604"></p><p>可通过<u><strong>反向传播</strong></u>进行计算，对抗方法得到的扰动方向，比随机找一个扰动更好。</p><p>对抗训练存在的缺点：仅仅只能适配于有监督学习，当样本没有标签就不能进行，故作者提出了一种可以运用于<u><strong>半监督学习</strong></u>的对抗训练方法。</p><h2 id="Virtual-Adversarial-Training-VAT"><a href="#Virtual-Adversarial-Training-VAT" class="headerlink" title="Virtual Adversarial Training, VAT"></a>Virtual Adversarial Training, VAT</h2><p>Adversarial Direction，对抗性方向，其概念是指能够最大程度减少准确分类的概率的方向，如下图所示，寻找扰动项 <code>radv</code>，其投影点与准确分类的方向为对抗性方向。</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/1684329811138.png" alt="1684329811138" style="zoom: 50%;"><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517212606646.png" alt="image-20230517212606646"></p><p>local distributional smoothness（LDS），定义为衡量当前基于每个输入数据的模型的平滑度的负函数。</p><h3 id="推导流程"><a href="#推导流程" class="headerlink" title="推导流程"></a>推导流程</h3><blockquote><p>由于不知道为什么公式渲染不出来，我就不手打公式了，等有时间再去把这个问题弄了，我就直接套用现成已有的公式，或是直接手写出来，Sorry…</p></blockquote><p>文中的符号定义如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518145424158.png" alt="image-20230518145424158"></p><p>在上面，我们提到，<u><strong>对抗训练</strong></u>只适用于带标签的有监督问题，在半监督学习中并不是很适用，所以，作者就提出了<u><strong>虚拟对抗训练</strong></u>，<code>VAT</code> 的损失函数如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518145654097.png" alt="image-20230518145654097"></p><p>其中，<code>x⁎</code>可以表示为有标签或者无标签数据，正如上文中所提到的，在实际中，并没有关于 <code>q(y, xul)</code> 的直接信息，因此可以使用<code>p(y|x, θ)</code>来替换之，如果带标签的样本比较多时，<code>p</code>就会逼近<code>q</code>，利用<code>p</code>生成的虚拟标签代替不知道的标签，并根据虚拟标签计算对抗方向，此时的虚拟标签是用上一步训练之后得到的模型进行估计，故损失函数更新如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518150409362.png" alt="image-20230518150409362"></p><p>将损失函数求平均，得到正则项（<strong>regularization term</strong>）：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518150651440.png" alt="image-20230518150651440"></p><p>完整的目标函数为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518150735667.png" alt="image-20230518150735667"></p><p>其中第一项为<u><strong>带标签数据的负对数似然函数</strong></u>（针对有标签数据），VAT 的一个优点是：只有两个超参数：（1）对抗方向的限制参数（norm constraint）<code>ɛ &gt; 0</code>；（2）控制目标函数的两个目标项的相对平衡的正则化系数（regularization coefficient）<code>α &gt; 0</code>，事实上，作者指出，两个超参数的作用大抵相同，于是 VAT 中只微调了超参数 <code>ɛ</code>，而将 <code>α</code> 固定为 1。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518151554425.png" alt="image-20230518151554425"></p><p>对于上述目标函数，观察到，当 r = 0 时，D 永远为 0，所以需要对 D 进行二阶泰勒展开，这里的 D 为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518151854028.png" alt="image-20230518151854028"></p><p>二阶展开后得到：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518151914984.png" alt="image-20230518151914984"></p><p>其中 H 为海森矩阵（Hessian Matrix），具体如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518152200457.png" alt="image-20230518152200457"></p><p>故 <code>rvadv</code> 可以近似为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518152332268.png" alt="image-20230518152332268"></p><p>其中：u 是 H 的第一特征主向量，上划线表示同方向单位向量（算了我写出来.）</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/1684394786562.png" alt="1684394786562"></p><p>在计算 H 的特征值/特征向量时，时间复杂度需要 <code>O(I³)</code>，论文中提到使用幂迭代法（ power iteration method）和有限差分法（finite difference method）来近似求解，假设 <code>d</code> 为与特征向量不垂直的一个随机单位向量，迭代计算如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153132095.png" alt="image-20230518153132095"></p><p>注意到 <code>Hd</code> 可以被有限差分来近似计算：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153216356.png" alt="image-20230518153216356"></p><p>由于 D 的一介导还是很可能为 0，所以 r 用如下方式迭代更新：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153419621.png" alt="image-20230518153419621"></p><p><u><strong>迭代的次数越多效果越好</strong></u>，甚至很多数据集上迭代次数为 1 都取得了较好的效果，迭代次数如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153804154.png" alt="image-20230518153804154"></p><p>伪代码如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153918543.png" alt="image-20230518153918543"></p><p>效果如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518154205948.png" alt="image-20230518154205948"></p><p>迭代次数较少的情况下，有大量的<u><strong>未标记点（灰色点）</strong></u>会有较高的<u><strong>LDS（深蓝色）</strong></u>，即不平滑，因为模型一开始对相同类别的 point 预测了不同的 label，VAT 会让 LDS 较高的数据点更大的压力，使得数据点间边界更平滑。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>文章的亮点就是对于二阶导以及之后的各种近似优化，理论方面很充足，实验也很充足，理论方面对于我这个渣硕来说真的挺艰难，我在去年 12 月份就准备看这一篇，结果因为理论被劝退，终于时隔数月，我算是啃下来了…不过也还是云里雾里的，还是得补充一下数学理论知识啊，果然我导说的没错，理论知识都不懂，给我发论文看我都看不懂，那还搞啥呢，哈哈🤦‍♂️，就这样子，如果有机会再复盘这篇论文，有问题在更新。</p><p>最后附上我得手写版公式（惨不忍睹的被公式薄纱了）：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518155255116.png" alt="image-20230518155255116"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518155319698.png" alt="image-20230518155319698"></p>]]></content>
      
      
      <categories>
          
          <category> 半监督学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Semi-Surprised-Learning </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> Consistency Regularization </tag>
            
            <tag> Adversarial Example </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于深度学习中遇到的知识盲区</title>
      <link href="/posts/eaa3f406.html"/>
      <url>/posts/eaa3f406.html</url>
      
        <content type="html"><![CDATA[<h1 id="关于深度学习的一些问题与知识盲区"><a href="#关于深度学习的一些问题与知识盲区" class="headerlink" title="关于深度学习的一些问题与知识盲区"></a>关于深度学习的一些问题与知识盲区</h1><p>此篇用于记录在阅读论文以及博客时遇到的一些问题和知识盲区，方便日后进行复盘。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="为什么需要激活函数？"><a href="#为什么需要激活函数？" class="headerlink" title="为什么需要激活函数？"></a><strong>为什么需要激活函数？</strong></h3><p>通常激活函数都是非线性的，它能够帮助我们引入非线性因素，使得神经网络能够更好地解决更加复杂地问题，在简单的二分类问题中，如果不使用激活函数，使用简单的**<u>逻辑回归</u>**，那么该模型只能作简单的线性分类，而不能作复杂的非线性划分，如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511195806300.png" alt="image-20230511195806300"></p><p>值得一提的是，如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，效果与单个神经元无异。另外，如果是拟合问题而不是分类问题，输出层的激活函数可以使用线性函数。</p><h3 id="Sigmoid-函数饱和带来的问题"><a href="#Sigmoid-函数饱和带来的问题" class="headerlink" title="Sigmoid 函数饱和带来的问题"></a>Sigmoid 函数饱和带来的问题</h3><p>Sigmoid 函数的取值范围在 (0,1) 之间，单调连续，求导容易，一般用于二分类神经网络的输出层。</p><p>Sigmoid 函数饱和区范围广，容易造成梯度消失。饱和区如下图所示，图中红色椭圆标注的饱和区曲线平缓，梯度的值很小，近似为零，且 Sigmoid 函数的饱和区范围很广，除了 [-5,5]，其余区域都可以认为是饱和区，这种情况很容易造成梯度消失，梯度消失会增大神经网络训练难度，影响神经网络模型的性能。</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511200333796.png" alt="image-20230511200333796" style="zoom:50%;"><h2 id="降噪自编码器-Denoising-Auto-Encoder"><a href="#降噪自编码器-Denoising-Auto-Encoder" class="headerlink" title="降噪自编码器 Denoising Auto-Encoder"></a>降噪自编码器 Denoising Auto-Encoder</h2><p>在神经网络模型训练阶段开始前，通过 Auto-encoder 对模型进行预训练可确定编码器 W 的初始参数值。然而，受模型复杂度、训练集数据量以及数据噪音等问题的影响，通过 Auto-encoder 得到的初始模型往往存在过拟合的风险。</p><p>简单理解，在人类的感知过程中，某些模态的信息对结果的判断影响并不大。举个例子，一块圆形的饼干和一块方形的饼干，在认知中同属于饼干这一类，因此形状对我们判断是否是饼干没有太大作用，也就是<strong>噪声</strong>。如果不能将形状数据去除掉，可能会产生“圆饼干是饼干，方饼干就不是饼干”的问题（过拟合）。</p><p>当采用无监督的方法分层预训练深度网络的权值时，为了学习到较鲁棒的特征，可以**<u>在网络的可视层（即数据的输入层）引入随机噪声</u>**，这种方法称为降噪自编码器（<a href="https://dl.acm.org/doi/abs/10.1145/1390156.1390294">Denoising Auto-Encoder， DAE</a>）。</p><blockquote><p>降噪自编码器：一个模型，能够从有噪音的原始数据作为输入，而能够恢复出真正的原始数据。这样的模型，更具有鲁棒性。</p></blockquote><p>以下是以经典的 MNIST 手写数字识别为例，对于输入的数据引入了变换角度、随机噪点、添加背景图像等噪音。模型通过训练后可以对有噪音图像更加鲁棒，而这也更符合实际使用的需求。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-2b0c0b5855f0c98b44fbeaf75d6a72dd_r.jpg" alt="img"></p><p>对于有噪音的输入数据，区别于一般自编码机，降噪自编码机要做的就是数据的降噪。关于降噪的过程如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-9ab89078a4630eb52347244a61410c2f_r.jpg" alt="img"></p><p>对于输入层 $x$，以一定概率将其节点置 0，得到 $\hat{x}$,用 $\hat{x}$ 去计算 $y$,计算 $z$ ，并将 $z$ 与原始 $x$ 做误差迭代，对结果误差较小的节点可以认为是噪声。每层处理重复上述工作。</p><p>自编码器的本质是学习一个相等函数，即网络的输入和重构后的输出相等，这种相等函数的表示有个缺点就是当测试样本和训练样本不符合同一分布，效果不好，而降噪自编码器在这方面的处理有所进步。</p><h2 id="随机深度-Stochastic-Depth"><a href="#随机深度-Stochastic-Depth" class="headerlink" title="随机深度 Stochastic Depth"></a>随机深度 Stochastic Depth</h2><p>针对于残差模块的优化，由于很深的 ResNet 通常需要很长时间的训练(也就是训练很慢)，作者引入了一种类似于 dropout 的方法，在训练过程中随机丢弃子图层（randomly drop a subset of layers），而在推断时正常使用完整的图。</p><p>ResNet 网络是由一个接一个的残差模块(ResBlock)串联起来的，可以视为ResBlock的集合。在训练时，对每个 ResBlock 随机 drop（按伯努利分布），drop 就是将上一个 ResBlock 直接输出到下一个 ResBlock，被 drop 的 ResBlock 什么都不做也不更新。另外，网络的输入被视为第一层，是不会 drop 的。</p><p>与 Dropout 的不同之处在于，该方法 drop 整个 ResBlock，而 Dropout 在训练期间只 drop 一部分神经元节点。这种方法大大降低了训练时间，甚至在训练完成后删除部分layer，还能不影响精度。</p><h2 id="最小化信息熵-Entropy-Minimization"><a href="#最小化信息熵-Entropy-Minimization" class="headerlink" title="最小化信息熵 Entropy Minimization"></a>最小化信息熵 Entropy Minimization</h2><p>参考文章：<a href="https://kexue.fm/archives/5448">最小熵原理（一）</a></p><p>在半监督学习中，有标签（分类完全准确）的数据样本通常相对较少，通过训练模型对未标记数据样本进行预测，选择出高置信度的样本，作为标记样本同有标签样本作为下一次训练的数据样本。</p><p><u><strong>Entropy Minimization</strong></u> 是一种在半监督学习中使用的技术，它的目的是最小化信息熵，从而使模型在分类时的不确定性最小。在半监督学习中，我们希望模型尽可能地利用未标记数据来学习，但是这些数据并不带有正确的标签，因此我们需要利用某些技术来帮助我们学习这些数据。</p><p><u><strong>加快模型学习进度的唯一方法就是降低学习目标的冗余信息量</strong></u>，所提到的“去冗余”，可以理解为“省去没必要的学习成本”。</p><p>也就是通常所使用到的技巧：过滤掉低置信度的未标记样本，保留高置信度样本。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512164646319.png" alt="image-20230512164646319"></p><h2 id="Top-1-and-Top-5-Accuracy"><a href="#Top-1-and-Top-5-Accuracy" class="headerlink" title="Top-1 and Top-5 Accuracy"></a>Top-1 and Top-5 Accuracy</h2><p>Top-1：</p><p>在多分类问题中，一般认为在经过全连接层后得到的概率分布中概率最大的类别为我们模型的预测类别，就判断为正确。</p><p>Top-5：</p><p>在多分类问题中，一般认为在经过全连接层后得到的概率分布中概率最大的全五个类别中有我们模型的预测类别，就判断为正确。</p><h2 id="Dark-Knowledge"><a href="#Dark-Knowledge" class="headerlink" title="Dark Knowledge"></a>Dark Knowledge</h2><p>可以看作是经过 Softmax 函数后得到的各类类别的可能性，其包含着类别之间的相关性，比如，猫和狗的相似性，要远远大于猫和船的相似性，而这种相似性，会在概率值中有所体现，而这部分信息一致没有被很好的利用，所以称之为 Dark Knowledge。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>半监督学习学习过程</title>
      <link href="/posts/52ee1a8c.html"/>
      <url>/posts/52ee1a8c.html</url>
      
        <content type="html"><![CDATA[<h1 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h1><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>学习了快一个学期的 Java 程序开发，让我感觉开发的过程真的好空洞…但是我并不知道下一步到底要做什么，又处于想找实习又怕找不到的情况下，刷了几天 LeetCode，然后又变成了“小🐏人”，紧接着被隔离😅，真的挺痛苦，所幸在第三天就“出狱”了，在蹲“监狱”期间，受到一个“狱友”的启发，“还是要做好职业规划的，若是没有做好职业规划，像一只无头苍蝇一样，很可能在毕业之际还是找不到工作。”在思考了一天后，我决定重新开始在“半监督学习”这个领域开始零基础学习了，为什么会有这个想法？是因为我觉得我起初考研的目的是为了能够在人工智能这个领域继续深入了解，目的很单纯，那为什么我不坚持一下呢？于是我决定重新开始这个计划，管他呢，起码是自己喜欢做的事情，哪怕未来找不到工作什么的，起码现在是由试错的资本的。</p><p>以下将会记录我在学习时候遇到的问题/解决方式/新思路/感想。</p><h2 id="Self-Training"><a href="#Self-Training" class="headerlink" title="Self-Training"></a>Self-Training</h2><p>自训练方法，模型基于已标记好的训练集进行训练，得到一个基础模型，利用该基础模型对未标记的数据集进行预测一个<strong>伪标签</strong>，然后将两个数据集整合训练，得到一个新的模型，从而迭代更新模型参数，生成一个最优模型。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161304502.png" alt="image-20230511161304502"></p><h3 id="Pseudo-Label"><a href="#Pseudo-Label" class="headerlink" title="Pseudo-Label"></a><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.664.3543&amp;rep=rep1&amp;type=pdf">Pseudo-Label</a></h3><p>伪标签技术适用于小样本学习，实际上在样本极其珍贵的金融、医疗图像、安全等领域，伪标签学习有时候很有效。</p><p>伪标签的定义来自于半监督学习，半监督学习的核心思想是<u><strong>通过借助无标签的数据来提升有监督过程中的模型性能</strong></u>。</p><p>粗略来讲，伪标签技术就是<u><strong>利用在已标注数据所训练的模型在未标注的数据上进行预测，根据预测结果对样本进行筛选，再次输入模型中进行训练的一个过程</strong></u>。</p><p>如下图所示，利用有标签的数据集训练出一个模型，运用训练出的模型给予无标签数据一个<u><strong>伪标签</strong></u>。如何定义所属类别？利用训练好的模型对无标签数据进行预测，以概率最高的类别作为无标签数据的伪标签。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161425308.png" alt="image-20230511161425308"></p><blockquote><p>entropy regularization：用于防止模型过拟合，通过在损失函数中加入熵（entropy）项来实现</p></blockquote><p>利用 <em>entropy regularization</em> 思想，将无监督数据转为目标数据的正则项，即将拥有伪标签的无标签数据视为有标签的数据，利用交叉熵（与最初训练模型一致）来评估误差大小。</p><p>模型整体的目标函数如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-103a30551a86aa99222f8f35129088d7_r.jpg" alt="img"></p><p>其中左边一项为交叉熵，用来评估有标签数据的误差，右边一项即为 <em>entropy regularization</em> 项，用来从无标签的数据中获取训练信号。</p><p>为了平衡有标签数据和无标签数据的信号强度，如上所示，算法在目标函数中引入了时变参数 α(t)，其数学形式如下，其中 T1 和 T2 都为超参数：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-4d5a8a1d8aa4b9037859ab19dd346c0b_r.jpg" alt="img"></p><p>因此，随着训练时间的增加，α(t) 将会从零开始线性增长至某个饱和值，对应无标签数据的信号也将逐渐释放出来。背后的核心想法也很直观，早期模型预测效果不佳，因此 <em>entropy regularization</em> 产生信号的误差也较大，因而 α(t) 应该从零开始，由小逐渐增大。</p><p><strong>存在不足：</strong>只在训练时间这个维度上，采用了退火思想，即采用时变系数α(t)。而在伪标签这个维度，对于模型给予的预测标签一视同仁，这种方法在实际中存在明显问题。若模型在对伪标签的数据预测后， 10 个类别预测概率值都接近于 0.1，以最大概率这一原则选择对应的标签，那么这个标签对模型的训练会造成一定的<u><strong>副作用</strong></u>。</p><p><strong>设想如何突破这一不足？</strong></p><p>也许可以设定一个阈值，抛弃那些预测最大概率值小于该阈值的未标记样本，将满足条件的未标记样本分配伪标签，并加入模型评估当中，之后再迭代训练。</p><h3 id="Noisy-Student"><a href="#Noisy-Student" class="headerlink" title="Noisy Student"></a><a href="https://arxiv.org/abs/1911.04252">Noisy Student</a></h3><p>论文的关键 idea 是训练两个模型，“teacher”和“student”，强调的是在student模型中加入噪声，teacher 模型和 student 模型可以用不同的模型训练，也可以使用相同的模型。</p><p>在有标签数据中训练“teacher”模型，并利用该模型对未标记数据进行推断伪标签，这些伪标签可以是<u><strong>软标签</strong></u>，也可以取其最大概率的类别将其转换为<u><strong>硬标签</strong></u>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161507462.png" alt="image-20230511161507462"></p><p>然后将标记数据与为标记数据（带有伪标签）置入“student”进行训练，在训练之前<u><strong>数据增强</strong></u>使用 <strong>RandAugment</strong>，待“student”模型训练好后，使用最新的模型作为新的“teacher”，进行下一次迭代，此过程会重复几次（通常为 3 次）。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511211613877.png" alt="image-20230511211613877"></p><p>总结该篇论文的流程思路：</p><ol><li>首先将在 ImageNet 上训练好的模型作为 Teacher Network 来训练 Student Network</li><li>再使用训练好的 T 网络（无噪音）来对另一个数据集 [JFT dataset] 生成尽可能准确的伪标签</li><li>之后使用生成伪标签的数据集 [JFT dataset] 和 ImageNet 一起训练 Student Network</li><li>Student Network中增加了模型噪音<ul><li>Dropout</li><li>随机深度  Stochastic Depth</li><li>数据噪音：对图片进行数据增广（RandAugment）</li></ul></li></ol><p>对 Student 模型添加噪音的作用：</p><ol><li><strong>数据噪音</strong>：提高泛化能力</li><li><strong>模型噪音</strong>：提高模型鲁棒性和泛化能力</li></ol><p>具体参数设置：</p><ol><li><strong>Stochastic Depth</strong>：幸存概率因子为 0.8</li><li><strong>Dropout</strong>：分类层（final layer）引入 0.5 的丢弃率</li><li><strong>RandAugment</strong>：应用两个随机计算，其震级设置为 27</li></ol><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512204307583.png" alt="image-20230512204307583"></p><p>其他 Tricks：</p><ol><li><strong>数据过滤</strong>：将教师模型中置信度不高的图片过滤，因为这通常代表着域外图像（out-of-domain data）</li><li><strong>数据平衡</strong>：平衡不同类别的图片数量，当一个类别所对应的图片数量不是很充足时，会采取<u>随机复制</u>的方法来扩充样本量</li><li><strong>软标签（Soft Pseudo-Label）</strong>：在消融实验中表示，软标签对域外图像有更强的指导作用</li></ol><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p>1.噪音是否对模型有影响？（The Importance of Noise in Self-training）</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512205434466.png" alt="image-20230512205434466" style="zoom:80%;"><p>从结果可以看出，<u>噪音</u>、<u>随机深度</u>、<u>数据扩充</u>起着重要的作用使学生模型胜过教师模型，对此有人提出是不是对未标记数据加入正则项以防止过拟合来代替噪音，作者在实验中说明这是不对的。因为在去噪的情况下，未标记图像的训练损失并没有下降多少以此说明模型并没有对未标记数据过拟合。</p><p>2.对于迭代训练的消融实验（A Study of Iterative Training）</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512205650571.png" alt="image-20230512205650571" style="zoom:80%;"><p>作者先在<u><strong>标记数据</strong></u>上训练了 EfficientNet-B7 作为 Teacher，然后再训练 EfficientNet-L2 作为 Student，然后让 Student 作为 Teacher 依次迭代三轮，作者表明，迭代训练提高了准确度，并且，给出再最后通过调整未标记图像和标记图像的比为 <strong>1 : 28</strong> 时达到最优 Top-1 Acc.。</p><p>3.能力强的教师模型会不会对学生模型造成的影响</p><p>4.无标签的数据量大小</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512210409208.png" alt="image-20230512210409208" style="zoom:80%;"><p>作者按照比例分别从整个数据集中<u><strong>均匀采样（uniformly sampling）</strong></u>，会发现在数据量减少至 1/16 中，模型的性能大都相似，在数据量达到 1/32 或更小后，模型性能有了显著的下降（可能 .3 个点就可以算是显著的下降了吧…），所以，使用大量未标记的数据会产生更好的性能，但作者指出：<u>对于大模型来说，数据量越多越好，而小模型由于容量限制则很容易饱和</u>。</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512210908139.png" alt="image-20230512210908139" style="zoom:80%;"><p>5.硬标签和软标签对域外图像的影响</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512210946955.png" alt="image-20230512210946955" style="zoom:80%;"><p>作者将预测置信度高（high-confidence）的图像视作域内图像（in-domain images），反之，将预测置信度低（low-confidence）的图像视作域外图像（out-of-domain images），作者表明：对于域内图像，软伪标签（ soft pseudo labels）和硬伪标签（hard pseudo labels）都对模型有一定的帮助；而对于域外图像，软伪标签使得模型对域外图像的判断有着一定的帮助，而硬标签则会对模型的精度有一定的损害。</p><p>剩下的<u><strong>消融实验</strong></u>就不写了，有点过于冗长了。</p><h2 id="Consistency-Regularization"><a href="#Consistency-Regularization" class="headerlink" title="Consistency Regularization"></a>Consistency Regularization</h2><p>说到一致（Consistency），其实很多代价都有这个内涵，如 MSE 代价，最小化预测与标签的差值，也就是希望预测与标签能够一致。其他的代价，如 KL 散度、交叉熵代价也类似。</p><p>所以一致性，是一种非常内在而本质的目标，可以让深度网络进行有效学习。 </p><p>在半监督领域中，未标记数据没有标签，所以需要让模型有个参照，从而通过这个参照从未标记数据中学习。</p><p><strong>Consistency Regularization</strong> 的主要思想是：相同的一张图片，通过模型所预测出来的结果应该是一样的。对于未标记样本，虽然没有对应的标签来使得模型判断预测结果是否准确，但是，却可以在原有的样本中添加一定的噪声，让模型通过比对预测结果来进行学习。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515151137355.png" alt="image-20230515151137355"></p><h3 id="π-model-and-Temporal-Ensembling"><a href="#π-model-and-Temporal-Ensembling" class="headerlink" title="π-model and Temporal Ensembling"></a>π-model and Temporal Ensembling</h3><p>论文地址：<a href="https://arxiv.org/abs/1610.02242">Temporal Ensembling for Semi-Supervised Learning</a> </p><h4 id="π-model"><a href="#π-model" class="headerlink" title="π-model"></a>π-model</h4><p>如下图所示，一个有标签样本（可视作无标签样本），经过随机图像增强输入网络，同时网络也会进行 Dropout 也可以视作噪声，输入两次得到两个结果 Z 和 Z’，将 Z 与图片真正标签 y 进行比对，使用 <code>交叉熵（cross-entropy）</code>计算损失 L1；将 Z 与 Z’ 使用 <code>平方差（squared difference）</code>计算损失 L2，然后将两个过程所得到的损失相加，但需要注意的是，两个损失分别占有一定权重，且占有的权重值会随着训练时间改变，在图 2 的所标记的损失函数公式可以看出。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515154058143.png" alt="image-20230515154058143"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515160459380.png" alt="image-20230515160459380"></p><p>其中 C 表示标记样本的的全部类别数量，w(t) 是随时间变化的加权函数，</p><p>并且，作者指出，第一部分的损失只针对有标签数据进行计算，而第二部分的损失则针对所有数据进行计算，即在算损失的时候，有标签数据两个损失项都用，无标签数据只用第二个损失项：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515161927609.png" alt="image-20230515161927609"></p><p>作者指出，使用 w(t) 时，由于前期模型差不多被有监督损失部分（标记数据样本）所支配，所以，在增长的前期，使权重增长的尽可能慢一点。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515163443077.png" alt="image-20230515163443077"></p><p>总结流程就是，数据集：有标签/无标签图片，分别两次输入模型，其中，L1只在有标签数据参与训练的过程中用于计算损失，L2 则是都有参与计算损失（有标签和无标签）。模型的噪音来自于：输入图像的随机增强，以及模型训练过程中的 Dropout。计算损失公式：L1 使用<u><strong>交叉熵</strong></u>来计算损失，L2 使用<u><strong>平方差公式</strong></u>来计算损失。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516131038965.png" alt="image-20230516131038965"></p><h4 id="Temporal-Ensembling"><a href="#Temporal-Ensembling" class="headerlink" title="Temporal Ensembling"></a>Temporal Ensembling</h4><p>由于以这种方式获得的培训目标是基于对网络的单一评估，因此可以预期它们会很嘈杂。暂时性结合通过将多个先前网络评估的预测汇总为整体预测来减轻这种情况</p><p>在 π-model 中，模型的训练结果都只是对网络进行单一评估，预测结果没有很强的关联性，这样子就造成了模型变得很“嘈杂”，并且，直觉认为，很难不相信过去预测结果与现在的预测结果不存在联系，这就是 π-model 的缺点。</p><p>在 Temporal Ensembling 中就很好的解决了这一个问题，在计算第二部分的损失中，生成的 Z’ 要参与下一个 epoch 的损失计算，（注意是每一个 epoch，而不是每一个 batch，这种改变其实是非常缓慢）。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515154108557.png" alt="image-20230515154108557"></p><p>每训练完一个 epoch 后，就会将 Z 进行更新：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515165329913.png" alt="image-20230515165329913"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515154426776.png" alt="image-20230515154426776"></p><p>总结流程，不同于 π-model，Temporal Ensembling 只进行一次模型预测，π-model 中的第二次训练预测，前一个 epoch 保存下来的 Z 与当前 epoch 的 z 进行指数滑动平均（Exponential Moving Average，EMA）运算得到，其中 z’ 是当前 epoch 的模型预测，参与 L2 的损失计算。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516132020990.png" alt="image-20230516132020990"></p><h3 id="Mean-Teacher"><a href="#Mean-Teacher" class="headerlink" title="Mean Teacher"></a><a href="https://arxiv.org/abs/1703.01780">Mean Teacher</a></h3><p>论文的关键思想是使用两个模型一个叫做<code>Student</code>，另一个叫做<code>Teacher</code>，其中Student 模型是一个带有 <u><strong>Dropout</strong></u> 的标准网络模型，与上文中所提到的 <u><strong>Noise Student</strong></u> 不同的是，该篇论文中的 Teacher 模型的架构与 Student 模型一致，且更新参数时是由 Student 模型的参数作<u><strong>指数滑动平均</strong></u>计算得到。计算损失的方式与 <u><strong>Temporal Ensembling</strong></u>相似/一致，有标签数据使用两个损失项，而无标签数据使用 Student 和 Teacher 模型得到的结果计算一致性损失，最后模型的总损失是由两个部分的损失得到。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516162131724.png" alt="image-20230516162131724"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516165459330.png" alt="image-20230516165459330"></p><p>使用指数滑动平均来更新参数后，模型可以在每一个 step 而不是在每一个 epoch来聚合信息，这大大提高了推断效率。并且，模型的每一层输出都得到了提升，而不是仅仅是在最后一层输出结果上得到改善，所以运用指数滑动平均，使得目标模型有了更好的<code>中间表示（intermediate representations）</code>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516170114727.png" alt="image-20230516170114727"></p><p>Teacher 模型参数 θ 的更新公式如下所示，在论文中将模型参数 θ 视作<u><strong>常量</strong></u>，不参与模型训练更新，而是在每一个 step 中按照公式进行迭代更新。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516183523851.png" alt="image-20230516183523851"></p><p>使用三种类型的噪声：</p><ul><li><p>对输入图像输入图像作随机平移和水平翻转</p><p><code>Translation Randomly {∆x, ∆y} ∼ [−2, 2]</code></p><p><code>Horizontal flip Randomly p = 0.5</code></p></li><li><p>Gaussian noise on the input layer</p><p><code>Gaussian noise σ = 0.15</code></p></li><li><p>Dropout</p><p><code>Dropout p = 0.5</code></p></li></ul><p>并且，计算一致性损失（ consistency cost）与上述所提到的模型不同，使用的是<code>均方误差(mean squared error)</code>，</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516185627093.png" alt="image-20230516185627093"></p><p>所使用的初始网络框架为：</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516195214426.png" alt="image-20230516195214426" style="zoom: 50%;"><h3 id="Virtual-Adversarial-Training"><a href="#Virtual-Adversarial-Training" class="headerlink" title="Virtual Adversarial Training"></a><a href="https://arxiv.org/abs/1704.03976">Virtual Adversarial Training</a></h3><p>由于有相关公式推导，且篇幅可能会很长（这篇论文理论部分好多…），故单独放在另一篇<a href="https://ytz7.github.io/posts/cc87ae2e.html">博客</a>中。</p><h3 id="Unsupervised-Data-Augmentation"><a href="#Unsupervised-Data-Augmentation" class="headerlink" title="Unsupervised Data Augmentation"></a><a href="https://arxiv.org/abs/1904.12848">Unsupervised Data Augmentation</a></h3><p>该篇论文的关键 idea 是使用 <code>AutoAugment</code> 创建一个未标记的图像的增强版本，使用最小化无标签数据增广数据和无标签数据的 KL 散度，如下图所示，具体思想和半监督学习中其他论文所使用的方法很相似，新鲜点就是论文的增强方式，对待不同任务使用不同的增强方式，且效果都还不错。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518191734446.png" alt="image-20230518191734446"></p><p>具体细节可查阅<a href="https://ytz7.github.io/posts/77eb0ac8.html">博客</a>。</p><blockquote><p>520 这天还在工位，真实咸鱼一条了 (#｀-_ゝ-) ，但今天见证历史了额，居然不看题解 LeetCode AC 了一道困难题，哈哈哈😊，然后就奖励了自己看一下午的视频（堕落了一下午），shit… 看论文看论文~~</p></blockquote><h2 id="混合方法（Hybrid-Methods）"><a href="#混合方法（Hybrid-Methods）" class="headerlink" title="混合方法（Hybrid Methods）"></a>混合方法（Hybrid Methods）</h2><p>看名字就知道，混合混合，就是把前人所提出的一些方法就行合并，以及添加一些其他组件来提高性能。</p><h3 id="MixMatch"><a href="#MixMatch" class="headerlink" title="MixMatch"></a><a href="https://arxiv.org/abs/1905.02249">MixMatch</a></h3><p>主要工作是在生成数据上做了很大的功夫，但感觉还是很工程…生成数据的算法如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520200836920.png" alt="image-20230520200836920"></p><p>对于有标签的数据，对其作一次增强，对于无标签的数据，对其作 K 个不同的增强，生成 K 个对应增强后的数据，并且对 K 个生成后的无标签数据做预测，预测后将其预测分布取平均后的分布作一次<u><strong>锐化</strong></u>，，作为 K 个无标签数据的对应分布，或者可以称之为最终的<u><strong>伪标签</strong></u>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520210753737.png" alt="image-20230520210753737"></p><p>得到有标签数据增强后的结果 <code>X^(x, p)</code>，和无标签数据生成后带有伪标签的数据<code>U^(u, q)</code>，过程如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520201404911.png" alt="image-20230520201404911"></p><p>然后将增强后的数据 <code>X</code> 和  <code>U</code> 混合后做一次 <u><strong>Shuffle</strong></u>，得到样本集 <code>W</code>，其中前 N 项作为 <code>W_L</code>，剩余的 M 项为 <code>W_U</code>：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520202313703.png" alt="image-20230520202313703"></p><p>锐化公式如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520204542849.png" alt="image-20230520204542849"></p><p>然后将 <u><strong>MixUp</strong></u> 算法运用在上述集合之中，作以下操作：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520202419730.png" alt="image-20230520202419730"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520202432527.png" alt="image-20230520202432527"></p><p>其中  <u><strong><a href="https://arxiv.org/abs/1710.09412">MixUp</a></strong></u> 算法为图像增强算法，将两个图像按下面的公式生成新的图像：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520203113608.png" alt="image-20230520203113608"></p><p>为了让 MixUp 方法与损失项更兼容，作者对其做了一点小小的修改：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520212837377.png" alt="image-20230520212837377"></p><p>损失函数如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520203414452.png" alt="image-20230520203414452"></p><p>输入：<u><strong>相同 batch 大小</strong></u>有标签数据 <code>X</code> 和无标签数据 <code>U</code></p><p>其中 <code>T</code>，<code>K</code>，<code>α</code> 为模型的超参数，<code>α</code> 在 <u><strong>MixUp</strong></u> 中需要使用，<code>T</code> 在<u><strong>锐化</strong></u>中使用到， <code>K</code> 为 K 个不同的<u><strong>数据增强</strong></u>方法：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520204213042.png"></p><p>输出：增强后的结果 <code>X'</code> 和  <code>U'</code></p><p>然后计算有标签数据的损失 <code>L_X</code>，其中 <code>H</code> 为<u><strong>交叉熵</strong></u>，无标签数据的损失 <code>L_U</code>，使用 <u><strong>L2 损失</strong></u>，其中，无标签数据的损失项中 <code>L</code> 是总的标签数量（也就是输入的标签的维度）：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520204253594.png" alt="image-20230520204253594"></p><p>最后，将有标签数据的损失和无标签数据的损失合并，得到最终的损失，其中无监督的损失会对其施加一个权重 <code>λ_U</code>。</p><p>消融实验的结果：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520220721107.png" alt="image-20230520220721107"></p><blockquote><p>OK，按照这个<a href="https://amitness.com/2020/07/semi-supervised-learning/">博主</a>关于半监督学习的综述，终于到达最后一篇经典论文了，不过有件事情一直想吐槽，就是 tensorflow 1. 怎么那么难用啊，本来说想复现一下 MixMatch 的实验，今天捣鼓了一整个下午，还是没有成功，气死我了，后面说是和 Cuda 的版本有冲突，tensorflow 1.15 的版本只能用 Cuda 10. ，服务器上又是 11. 的版本，奈何公共服务器都是大家一起用的，我怕搞得服务器崩了…索性算了，看下一篇去了，以后找论文源码的时候，应该要先找 Pytorch 版本的，不然搞这 tensorflow 又要浪费不少时间。</p><p>算了今天先不写了，会去跑步加和爸妈打打电话吧…</p><p>写于 2023/5/21.</p><p>就写了个开头 😂😅</p></blockquote><h3 id="FixMatch"><a href="#FixMatch" class="headerlink" title="FixMatch"></a><a href="https://arxiv.org/abs/2001.07685">FixMatch</a></h3><p>论文的方法是将先前所提到的<u><strong>伪标签（pseudo-labeling）</strong></u>和<u><strong>一致性正则化（consistency regularization）</strong></u>结合在一起，但又大大简化了整体方法，如下图所示，使用<u><strong>交叉熵</strong></u>在有标签数据上训练一个模型，损失记为 <code>L1</code>，对每一张未标记数据，做一次弱<u><strong>弱增强</strong></u>和<u><strong>强增强</strong></u>，对<u><strong>弱增强</strong></u>后的样本做预测，如果预测出的最大概率大于设置的阈值，那么将这个最大概率对于的类别作为<u><strong>弱增强后的样本的伪标签</strong></u>，即最后的输出分布为 <u><strong>One-Hot 编码</strong></u>，然后将弱增强和强增强的无标签数据对应的标签做一次<u><strong>交叉熵</strong></u>，得到损失 <code>L2</code>，最后将两个损失加权得到最终的损失 <code>L</code>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522130017362.png" alt="image-20230522130017362"></p><p>算法步骤：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522163328704.png" alt="image-20230522163328704"></p><p>具体细节可查阅<a href="https://ytz7.github.io/posts/3680c2d7.html">博客</a>。</p>]]></content>
      
      
      <categories>
          
          <category> 半监督学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Semi-Surprised-Learning </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 个人随笔 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title></title>
      <link href="/manifest.json"/>
      <url>/manifest.json</url>
      
        <content type="html"><![CDATA[{"name":"七号zz の Blog","short_name":"七号zz","theme_color":"#eedeab","background_color":"#eedeab","display":"standalone","scope":"/","start_url":"/","icons":[{"src":"/img/siteicon/16.png","sizes":"16x16","type":"image/png"},{"src":"/img/siteicon/32.png","sizes":"32x32","type":"image/png"},{"src":"/img/siteicon/48.png","sizes":"48x48","type":"image/png"},{"src":"/img/siteicon/64.png","sizes":"64x64","type":"image/png"},{"src":"/img/siteicon/128.png","sizes":"128x128","type":"image/png"},{"src":"/img/siteicon/144.png","sizes":"144x144","type":"image/png"},{"src":"/img/siteicon/512.png","sizes":"512x512","type":"image/png"}],"splash_pages":null}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>about</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<h1 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h1><h2 id="一个废材研究生"><a href="#一个废材研究生" class="headerlink" title="一个废材研究生"></a>一个废材研究生</h2><h2 id="ISFJ-x2F-守护者"><a href="#ISFJ-x2F-守护者" class="headerlink" title="ISFJ/守护者"></a>ISFJ/守护者</h2><p>让我在抑郁的边缘徘徊</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/ali_font.js"/>
      <url>/js/ali_font.js</url>
      
        <content type="html"><![CDATA[!(function (c) {  var l,    h,    a,    t,    i,    v =      '<svg><symbol id="icon-dragon_chen" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#D6B196"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-498.122105 265.620211L431.157895 754.526316V485.052632h-66.074948c-14.470737 110.645895-44.355368 197.066105-102.696421 260.742736l-39.747368-36.432842C306.526316 617.876211 323.368421 462.901895 323.368421 242.526316V215.578947h377.263158v53.894737H377.182316c-0.404211 58.260211-2.209684 112.128-6.359579 161.684211H700.631579v53.894737h-122.152421a481.172211 481.172211 0 0 0 76.826947 119.70021l66.479158-39.855158 27.728842 46.214737-54.460631 32.687158c29.507368 24.953263 63.757474 45.675789 102.80421 58.098526l-16.303158 51.361684c-134.224842-42.711579-222.773895-167.073684-261.551158-268.207157H485.052632v221.857684l68.985263-41.391158 27.728842 46.214737-109.783579 65.886316zM646.736842 377.263158h-215.578947v-53.894737h215.578947v53.894737z" fill="#231F20"></path></symbol><symbol id="icon-dog_xu" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#D6B196"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-375.592421 150.393263c33.684211 44.544 75.210105 74.698105 124.739369 90.812632l11.425684 3.718737 10.401684-6.009264C781.204211 727.740632 808.421053 622.565053 808.421053 592.842105h-53.894737c0 22.069895-19.132632 80.869053-33.711158 103.504842-34.816-14.605474-64.538947-39.262316-89.249684-74.13221 48.316632-55.269053 92.079158-117.328842 120.535579-179.900632l-49.044211-22.285473c-23.767579 52.250947-59.742316 104.717474-100.055579 152.656842-24.010105-50.930526-41.148632-115.927579-51.658105-195.395369H700.631579v-53.894737h-155.189895A1848.050526 1848.050526 0 0 1 538.947368 161.684211h-53.894736c0 58.206316 2.155789 112.074105 6.494315 161.68421H323.368421v26.947368c0 216.549053-13.177263 263.545263-100.702316 359.046737l39.747369 36.432842c63.326316-69.093053 92.806737-118.272 105.714526-206.848H485.052632v-53.894736h-111.319579a1742.147368 1742.147368 0 0 0 3.449263-107.789474h120.158316c12.611368 98.250105 35.031579 177.475368 67.395368 238.187789-61.978947 65.536-128.053895 117.975579-173.298526 142.282106l25.519158 47.481263c47.589053-25.573053 114.095158-77.446737 177.55621-142.821053z m125.170526-411.971368l-80.842105-80.842106-38.103579 38.103579 80.842105 80.842106 38.103579-38.103579z" fill="#231F20"></path></symbol><symbol id="icon-dog" viewBox="0 0 1024 1024"><path d="M894.814316 904.434526l83.240421-183.134315-13.824-13.204211c-0.485053-0.458105-45.648842-47.589053-47.939369-185.263158-0.134737-7.922526-0.134737-33.953684-0.134736-55.996631-30.693053 15.306105-70.090105 19.887158-106.09179 19.887157-92.752842 0-163.624421-23.983158-210.647579-71.275789a192.512 192.512 0 0 1-27.944421-36.513684H377.263158v377.263158c342.662737 0 403.105684 51.092211 494.592 128.377263 7.922526 6.682947 15.521684 13.312 22.959158 19.86021z" fill="#85C3DE"></path><path d="M326.063158 282.947368c0 34.250105-13.231158 44.463158-29.642105 44.463158s-29.642105-10.213053-29.642106-44.463158c0-34.223158 13.231158-44.463158 29.642106-44.463157s29.642105 10.24 29.642105 44.463157zM269.473684 430.295579v311.646316L190.275368 916.210526h59.203369L323.368421 753.637053V377.263158h-26.947368c-119.403789 0-172.732632-53.382737-185.505685-107.789474h35.624421c51.092211 0 68.581053-15.764211 120.535579-62.544842 12.773053-11.506526 28.079158-25.276632 47.023158-41.741474l18.351158-15.952842-69.658947-99.139368-44.085895 30.989474 41.768421 59.472842c-11.183158 9.862737-20.884211 18.593684-29.480421 26.327579C180.736 212.156632 176.235789 215.578947 146.539789 215.578947H53.894737v26.947369c0 88.710737 66.910316 178.149053 215.578947 187.769263z m216.710737-161.414737c2.290526 71.733895 28.698947 136.326737 75.048421 182.918737C618.711579 509.628632 702.437053 538.947368 810.091789 538.947368c18.593684 0 36.190316-1.158737 52.628211-3.449263 3.745684 111.265684 33.630316 170.334316 51.496421 196.015158l-38.507789 84.722526C782.174316 742.049684 688.774737 700.631579 377.263158 700.631579v53.894737c34.277053 0 65.697684 0.512 94.639158 1.509052L374.595368 970.105263h59.203369l96.013474-211.240421c66.182737 4.338526 117.005474 11.829895 157.911578 22.016L626.229895 916.210526h59.176421l54.16421-119.134315c47.616 18.405053 79.737263 42.091789 113.125053 69.739789L805.753263 970.105263h59.203369l113.071157-248.778105-13.824-13.204211c-0.485053-0.458105-45.648842-47.589053-47.939368-185.263158C985.168842 498.553263 1024 447.811368 1024 377.263158c0-95.205053-66.506105-161.684211-161.684211-161.684211v53.894737c65.482105 0 107.789474 42.307368 107.789474 107.789474 0 89.088-87.013053 107.789474-160.013474 107.789474-92.752842 0-163.624421-23.983158-210.647578-71.27579-30.315789-30.504421-45.891368-65.832421-53.35579-98.735158 11.210105 6.952421 22.932211 13.338947 35.274105 19.186527l23.04-48.720843c-92.106105-43.654737-148.992-128.646737-219.243789-243.981473l-46.026105 28.05221c49.448421 81.246316 92.968421 148.506947 147.051789 199.302737z" fill="#231F20"></path></symbol><symbol id="icon-goat" viewBox="0 0 1024 1024"><path d="M548.378947 646.736842a952.32 952.32 0 0 1 140.90779-161.68421H107.789474c0 107.600842 0 107.600842-63.649685 169.283368l-13.069473 12.665263L66.721684 754.526316h417.172211c20.345263-41.472 43.654737-77.446737 64.485052-107.789474z" fill="#F7C768"></path><path d="M608.256 144.734316C555.762526 115.577263 506.098526 107.789474 485.052632 107.789474V53.894737c32.579368 0 91.270737 11.452632 149.369263 43.735579 75.290947 41.822316 130.694737 94.531368 171.385263 150.878316C755.873684 288.013474 697.101474 323.368421 646.736842 323.368421h-107.789474v-53.894737h107.789474c20.506947 0 48.424421-11.210105 80.437895-31.285895a471.04 471.04 0 0 0-118.918737-93.453473zM832.673684 342.231579c-16.384 0-29.642105 10.24-29.642105 44.463158 0 34.250105 13.231158 44.463158 29.642105 44.463158s29.642105-10.213053 29.642105-44.463158c0-34.223158-13.231158-44.463158-29.642105-44.463158zM1024 619.789474C1024 347.109053 901.066105 122.448842 686.753684 3.395368l-26.165895 47.104C914.324211 191.461053 964.688842 440.400842 969.647158 592.842105h-84.506947c-17.92-35.624421-45.352421-69.12-87.013053-101.995789l-16.788211-13.285053-16.734315 13.392842c-66.128842 52.897684-134.629053 127.083789-187.311158 209.677474H102.965895l-8.272842-20.318316C159.043368 617.013895 161.684211 603.109053 161.684211 485.052632v-53.894737h485.052631v-53.894737H161.684211c0-80.384 14.309053-110.026105 66.586947-137.916632l-25.384421-47.535158C123.365053 234.226526 107.789474 291.920842 107.789474 377.263158v107.789474c0 107.600842 0 107.600842-63.649685 169.283368l-13.069473 12.665263L110.618947 862.315789h58.206316l-43.897263-107.789473h103.477895l43.897263 107.789473h58.206316l-43.897263-107.789473h259.47621C508.981895 824.939789 485.052632 899.152842 485.052632 970.105263h53.894736c0-68.688842 27.270737-144.060632 68.958316-215.578947H687.157895c7.410526 0 13.473684 6.063158 13.473684 13.473684V862.315789h53.894737v-94.315789c0-37.160421-30.208-67.368421-67.368421-67.368421h-44.65179c40.771368-58.017684 89.438316-111.427368 138.913684-153.626947C841.512421 600.037053 862.315789 655.225263 862.315789 754.526316h53.894737c0-38.912-2.748632-74.482526-11.102315-107.789474H1024v-26.947368z" fill="#231F20"></path></symbol><symbol id="icon-goat_wei" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#D6B196"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-431.157895 50.202947c52.304842 70.925474 136.973474 152.144842 232.528843 190.383158l19.994947-50.041263c-109.271579-43.708632-202.805895-152.629895-238.780632-217.49221H808.421053v-53.894737H538.947368v-53.894737h215.578948v-53.894737h-215.578948V161.684211h-53.894736v161.68421h-215.578948v53.894737h215.578948v53.894737H215.578947v53.894737h255.757474c-35.974737 64.862316-129.536 173.783579-238.807579 217.49221l20.021895 50.041263c95.528421-38.238316 180.197053-119.484632 232.501895-190.383158V808.421053h53.894736v-246.218106z" fill="#231F20"></path></symbol><symbol id="icon-dragon" viewBox="0 0 1024 1024"><path d="M366.376421 344.441263l152.980211-152.98021c43.142737-43.142737 141.204211-9.216 270.201263 115.738947-15.225263 9.835789-25.114947 15.818105-44.13979 32.256s-38.076632 35.489684-59.418947 56.832c-4.203789 4.203789-51.173053 53.221053-78.740211 82.027789-10.805895-12.126316-22.743579-24.171789-34.654315-36.082526L493.136842 362.792421l-54.218105 54.218105-72.542316-72.569263zM862.315789 512c0 46.834526-45.352421 80.842105-107.789473 80.842105-108.948211 0-189.359158-28.806737-267.129263-56.697263C414.100211 509.871158 344.872421 485.052632 258.182737 485.052632 80.788211 485.052632 0 588.126316 0 683.897263h53.894737C73.216 659.779368 135.302737 646.736842 177.340632 646.736842c77.338947 0 223.124211 23.282526 291.893894 47.912421C547.462737 722.701474 615.989895 754.526316 734.315789 754.526316 862.315789 754.526316 916.210526 670.315789 916.210526 512h-53.894737z" fill="#FF8787"></path><path d="M552.421053 1024c-69.766737 0-113.825684-13.958737-156.402527-27.459368-54.487579-17.273263-110.807579-35.004632-232.421052-26.516211l-3.826527-53.733053c131.718737-9.458526 195.934316 10.967579 252.52379 28.887579 42.226526 13.365895 78.686316 24.926316 140.126316 24.926316 92.752842 0 148.210526-57.936842 148.210526-113.960421 0-16.949895-5.524211-101.618526-114.634105-101.618526-64.970105 0-112.747789 23.336421-163.328 48.02021C365.325474 830.571789 300.301474 862.315789 204.288 862.315789 85.908211 862.315789 0 787.294316 0 683.897263 0 588.126316 80.788211 485.052632 258.182737 485.052632c86.689684 0 155.917474 24.818526 229.214316 51.09221 45.810526 16.410947 92.564211 33.172211 145.488842 44.166737 9.000421-7.033263 13.850947-16.276211 13.850947-26.758737 0-37.187368-37.672421-74.859789-74.13221-111.265684l-3.287579-3.287579 38.103579-38.103579 3.260631 3.287579C652.853895 446.275368 700.631579 494.026105 700.631579 553.552842c0 12.719158-2.802526 24.926316-7.976421 36.109474A594.997895 594.997895 0 0 0 754.526316 592.842105c62.437053 0 107.789474-34.007579 107.789473-80.842105 0-58.853053-52.870737-110.268632-108.840421-164.702316l-8.057263-7.841684c-19.024842 16.437895-38.076632 35.489684-59.418947 56.832l-38.103579-38.103579c74.805895-74.832842 134.898526-134.898526 268.314947-141.931789V55.619368c-63.407158 7.787789-120.993684 39.424-121.667368 39.801264l-15.818105 8.811789-14.120421-11.344842C731.701895 66.452211 709.712842 53.894737 673.684211 53.894737c-41.418105 0-74.347789 25.869474-109.190737 53.301895-26.624 20.911158-54.137263 42.549895-86.851369 53.194105L469.342316 161.684211h-69.093053l-105.525895 105.525894-38.103579-38.130526L324.015158 161.684211H161.684211V107.789474h303.104c22.231579-8.272842 43.708632-25.168842 66.398315-42.981053C569.829053 34.438737 613.618526 0 673.684211 0c48.909474 0 81.408 17.946947 110.888421 40.097684C813.702737 26.300632 877.729684 0 943.157895 0h26.947368v323.368421h-53.894737v-53.167158c-54.164211 3.098947-92.914526 15.845053-127.002947 36.675369l1.832421 1.778526C852.587789 368.505263 916.210526 430.376421 916.210526 512c0 60.928-43.708632 109.945263-107.789473 127.622737V700.631579h53.894736v-53.894737h53.894737v53.894737h53.894737v53.894737h-53.894737v53.894737h-53.894737v-53.894737h-53.894736c-29.722947 0-53.894737-24.171789-53.894737-53.894737v-53.894737c-118.325895 0-207.063579-31.797895-285.318737-59.877053C400.437895 562.229895 335.494737 538.947368 258.182737 538.947368 117.059368 538.947368 53.894737 611.732211 53.894737 683.897263 53.894737 757.221053 115.738947 808.421053 204.288 808.421053c11.910737 0 23.228632-0.538947 34.034526-1.536C248.454737 796.321684 269.473684 770.640842 269.473684 739.166316c0-33.118316-43.088842-70.979368-58.152421-81.596632l30.935579-44.139789c8.299789 5.793684 81.111579 58.664421 81.111579 125.736421 0 19.429053-4.527158 37.052632-10.994526 52.304842 30.773895-10.051368 58.314105-23.498105 86.662737-37.349053C452.877474 727.848421 508.577684 700.631579 585.997474 700.631579 702.410105 700.631579 754.526316 778.725053 754.526316 856.144842 754.526316 938.657684 678.912 1024 552.421053 1024z m-21.180632-623.104L493.136842 362.792421l137.889684-137.889684 38.103579 38.103579-137.889684 137.889684z m-126.760421-18.351158l-38.103579-38.103579 152.980211-152.98021 38.103579 38.103579-152.980211 152.98021z m282.004211-218.624c15.494737-9.754947 43.331368-31.447579 43.331368-31.447579-25.734737-27.809684-49.556211-33.333895-67.368421-29.07621-19.240421 4.608-37.753263 24.602947-37.753263 24.602947s42.253474 22.447158 61.790316 35.920842z" fill="#231F20"></path></symbol><symbol id="icon-horse" viewBox="0 0 1024 1024"><path d="M776.003368 646.736842c16.599579-99.947789 43.439158-181.086316 83.213474-256.538947l6.817684-12.934737H269.473684c-36.756211 0-53.894737 54.945684-53.894737 92.05221 0 46.753684 6.656 77.527579 70.278737 176.074106l84.533895 128.269473L498.876632 646.736842h277.126736z" fill="#FFAF6E"></path><path d="M1024 0v404.210526c0 33.333895 0 134.736842-92.079158 134.736842h-13.824l-78.362947-109.056c-22.743579 49.906526-40.340211 103.046737-53.490527 162.950737h115.092211C937.310316 592.842105 970.105263 625.637053 970.105263 661.638737c0 60.631579-69.389474 154.300632-77.312 164.75621l-43.008-32.471579C875.466105 759.861895 916.210526 693.813895 916.210526 661.638737c0-5.982316-8.919579-14.901895-14.901894-14.901895h-125.332211C761.128421 736.121263 754.526316 840.569263 754.526316 970.105263h-53.894737c0-283.971368 31.097263-453.605053 110.888421-605.049263l20.318316-38.534737 112.801684 156.995369c14.443789-4.419368 25.465263-20.938105 25.465263-79.306106V0h53.894737z m-161.684211 161.684211h53.894737V0h-53.894737v80.842105c-17.381053-14.955789-38.184421-26.947368-80.842105-26.947368h-134.736842v53.894737h134.736842c37.672421 0 80.842105 40.906105 80.842105 53.894737z m-107.789473 0h-215.578948v53.894736h161.684211l53.894737-53.894736zM300.894316 766.544842L400.680421 916.210526h64.754526l-95.043368-142.551579L498.876632 646.736842h167.855157a1212.631579 1212.631579 0 0 1 9.431579-53.894737h-199.383579l-175.885473 173.702737z m109.97221-184.400842l-37.861052-38.319158-132.419369 130.802526C173.729684 571.095579 161.684211 529.812211 161.684211 469.315368 161.684211 398.578526 199.464421 323.368421 269.473684 323.368421h323.368421l53.894737-53.894737H269.473684c-6.709895 0-13.258105 0.565895-19.698526 1.482105C234.927158 249.451789 204.638316 215.578947 160.633263 215.578947 65.967158 215.578947 0 349.291789 0 469.315368c0 70.170947 16.141474 136.650105 49.232842 202.671158L6.197895 723.833263l41.472 34.41179 66.128842-79.737264-8.704-16.033684C83.105684 622.133895 53.894737 558.214737 53.894737 469.315368 53.894737 368.451368 106.765474 269.473684 160.633263 269.473684c13.231158 0 25.815579 9.889684 35.43579 20.533895C142.874947 321.967158 107.789474 388.500211 107.789474 469.315368c0 78.201263 19.698526 130.937263 93.642105 243.981474l-55.296 54.622316L280.899368 970.105263h64.754527l-130.048-195.072 195.260631-192.889263z" fill="#231F20"></path></symbol><symbol id="icon-monkey_shen" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#BBC4C9"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-431.157895 134.736842h161.684211v53.894737h53.894737V269.473684h-215.578948V161.684211h-53.894736v107.789473h-215.578948v431.157895h53.894737v-53.894737h161.684211v215.578947h53.894736v-215.578947z m0-161.68421h161.684211v107.789473h-161.684211v-107.789473z m-215.578947 0h161.684211v107.789473h-161.684211v-107.789473z m215.578947-161.684211h161.684211v107.789474h-161.684211v-107.789474z m-215.578947 0h161.684211v107.789474h-161.684211v-107.789474z" fill="#231F20"></path></symbol><symbol id="icon-ox_chou" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#D6B196"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-161.68421 188.631579h-159.555369c13.985684-172.813474 43.115789-357.429895 70.817684-385.158737L700.631579 269.473684H323.368421v53.894737h107.169684c-1.940211 45.756632-8.192 103.962947-15.76421 161.684211H323.368421v53.894736h83.968c-9.862737 68.446316-20.264421 130.128842-25.734737 161.684211H215.578947v53.894737h592.842106v-53.894737z m-346.543158-161.684211h149.800421a3313.717895 3313.717895 0 0 0-16.842105 161.684211h-158.477474c6.036211-35.247158 16.114526-95.636211 25.519158-161.684211z m22.608842-215.578947h171.735579c-15.198316 41.121684-27.405474 100.594526-36.890948 161.684211h-150.123789c7.383579-57.505684 13.419789-115.361684 15.279158-161.684211z" fill="#231F20"></path></symbol><symbol id="icon-monkey" viewBox="0 0 1024 1024"><path d="M757.733053 485.052632H565.894737a80.842105 80.842105 0 0 0-80.842105 80.842105v215.578947c0 40.96 43.546947 99.678316 77.446736 139.210105C596.426105 960.215579 603.055158 970.105263 603.055158 970.105263H754.526316s15.144421-18.674526 45.891368-58.071579S862.315789 809.984 862.315789 717.608421c0-89.573053-47.993263-166.346105-104.582736-232.555789z" fill="#C3D686"></path><path d="M538.947368 1024h-53.894736c0-32.794947 25.869474-87.417263 77.446736-103.316211C528.599579 881.152 485.052632 822.433684 485.052632 781.473684c0-44.570947 36.271158-80.842105 80.842105-80.842105h80.842105v53.894737h-80.842105a26.947368 26.947368 0 0 0-26.947369 26.947368c0 19.725474 36.675368 77.473684 92.133053 134.736842h88.602947c20.210526-14.147368 88.737684-71.464421 88.737685-198.602105 0-108.382316-93.237895-202.967579-168.151579-278.986105-49.502316-50.202947-88.576-89.842526-98.735158-128.61979-11.749053-44.732632-21.584842-112.586105-26.327579-148.318315H377.263158c-45.136842 0-89.519158 8.434526-121.802105 53.894736H431.157895v53.894737c-97.28 0-107.789474 113.071158-107.789474 161.684211v53.894737h53.894737v161.68421h-53.894737v-107.789474h-26.947368c-170.253474 0-188.631579-94.234947-188.631579-134.736842 0-31.043368 35.220211-72.326737 55.727158-93.722947 2.694737-14.686316 5.847579-28.348632 9.431579-41.013895H161.684211V215.578947h31.528421C239.642947 120.993684 317.224421 107.789474 377.263158 107.789474h185.640421l2.802526 23.794526c0.134737 1.050947 12.719158 106.657684 27.944421 164.756211 6.494316 24.872421 44.624842 63.514947 84.965053 104.448C760.481684 483.813053 862.315789 587.129263 862.315789 717.608421c0 92.375579-31.124211 155.028211-61.898105 194.425263C904.919579 892.146526 970.105263 803.004632 970.105263 673.684211c0-91.405474-42.819368-154.381474-84.237474-215.255579C847.791158 402.458947 808.421053 344.576 808.421053 269.473684c0-119.349895 87.093895-161.684211 161.68421-161.68421v53.894737c-32.417684 0-107.789474 10.509474-107.789474 107.789473 0 58.502737 31.555368 104.933053 68.096 158.639158C974.282105 492.597895 1024 565.679158 1024 673.684211c0 177.286737-108.301474 296.421053-269.473684 296.421052h-161.684211c-37.672421 0-53.894737 40.906105-53.894737 53.894737zM229.214316 269.473684a384.808421 384.808421 0 0 0-14.012632 58.341053l-1.401263 8.488421-6.090105 6.117053c-22.878316 22.932211-44.813474 52.601263-46.026105 62.275368 0 56.805053 53.76 75.264 107.789473 79.386947V431.157895c0-58.691368 13.473684-119.619368 46.511158-161.684211h-86.770526zM323.368421 1024h-53.894737c0-32.794947 25.869474-87.417263 77.446737-103.316211C313.020632 881.152 269.473684 822.433684 269.473684 781.473684c0-44.570947 36.271158-80.842105 80.842105-80.842105h45.16379A188.847158 188.847158 0 0 1 565.894737 592.842105h134.736842v53.894737h-134.736842c-74.293895 0-134.736842 60.442947-134.736842 134.736842v26.516211l-53.894737 0.377263V781.473684c0-9.162105 0.646737-18.135579 1.913263-26.947368H350.315789c-14.848 0-26.947368 12.072421-26.947368 26.947368 0 19.725474 36.675368 77.473684 92.133053 134.736842H431.157895v53.894737h-53.894737c-37.672421 0-53.894737 40.906105-53.894737 53.894737z" fill="#231F20"></path></symbol><symbol id="icon-horse_wu" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#FF8787"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-431.157895 26.947368h269.473685v-53.894736H538.947368v-161.684211h161.684211v-53.894737H411.001263c12.045474-33.28 20.156632-69.793684 20.156632-107.789473h-53.894737c0 121.963789-105.364211 233.391158-106.415158 234.496l38.858105 37.349052c2.883368-3.018105 43.816421-46.133895 77.392842-110.160842H485.052632v161.684211H215.578947v53.894736h269.473685v323.368421h53.894736V538.947368z" fill="#231F20"></path></symbol><symbol id="icon-ox" viewBox="0 0 1025 1024"><path d="M540.294737 754.526316h215.578947c20.210526 0 35.112421 1.374316 53.894737 4.581052 91.863579 15.656421 145.354105 67.691789 161.684211 86.069895V916.210526h53.894736V635.580632l-7.895579-7.895579c-9.269895-9.269895-36.513684-49.232842-44.032-196.527158H540.294737a161.684211 161.684211 0 0 0-161.684211 161.68421v131.098948c43.304421 20.210526 97.28 30.585263 161.684211 30.585263z" fill="#FFAF6E"></path><path d="M1025.347368 635.580632V916.210526h-53.894736v-71.033263c-16.330105-18.405053-69.820632-70.413474-161.684211-86.069895V916.210526h-53.894737v-161.68421h-107.789473v215.578947h-53.894737V700.631579h161.68421c100.998737 0 172.570947 38.669474 215.578948 71.868632v-115.738948c-33.684211-43.627789-51.712-137.458526-53.706106-279.498105H701.978947c-76.934737 0-127.218526-26.219789-175.804631-51.550316a1556.048842 1556.048842 0 0 0-26.839579-13.743158c-26.839579 26.004211-66.209684 44.921263-115.738948 55.511579 24.441263 22.986105 60.874105 52.116211 106.469053 72.838737l-22.312421 49.044211c-76.584421-34.816-129.589895-88.926316-150.824421-113.125053-10.644211 0.619789-21.477053 1.024-32.687158 1.024a473.734737 473.734737 0 0 1-123.365053-15.952842l-93.022315 186.314105 68.581052 53.86779C167.882105 579.557053 237.891368 538.947368 324.715789 538.947368v53.894737c-95.986526 0-170.361263 62.490947-171.088842 63.137684l-16.78821 14.282106-136.838737-107.358316 109.729684-219.809684C46.430316 314.448842 1.347368 267.371789 1.347368 199.868632 1.347368 89.815579 121.586526 53.894737 163.031579 53.894737v53.894737c-14.120421 0-107.789474 17.165474-107.789474 92.079158C55.242105 290.465684 192.188632 323.368421 284.240842 323.368421c67.907368 0 122.421895-12.988632 157.696-35.624421-42.711579-14.336-95.097263-23.120842-169.337263-18.324211l-3.503158-53.786947c95.878737-6.117053 160.148211 8.515368 211.429053 28.833684C484.244211 235.439158 486.4 225.818947 486.4 215.578947c0-48.855579-57.829053-76.288-58.394947-76.557473l22.393263-49.017263C454.063158 91.648 540.294737 131.826526 540.294737 215.578947c0 18.566737-3.422316 35.84-9.997474 51.631158 7.060211 3.584 13.985684 7.168 20.776421 10.698106C597.854316 302.322526 638.248421 323.368421 701.978947 323.368421h269.473685v26.947368c0 214.689684 35.220211 266.590316 45.999157 277.369264l7.895579 7.895579z m-729.384421 25.141894l-98.789052 118.541474 86.797473 137.835789 45.594948-28.725894-65.913263-104.690527 37.052631-44.43621C358.642526 785.192421 439.080421 808.421053 540.294737 808.421053v-53.894737c-99.893895 0-175.077053-24.549053-223.474526-72.946527l-20.857264-20.857263z" fill="#231F20"></path></symbol><symbol id="icon-rabbit_mao" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#7DD47F"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-377.263158-188.631579h107.789474v323.368421c-20.48 0-39.936-11.264-40.016842-11.317895l-27.728842 46.214737c3.206737 1.940211 32.660211 18.997895 67.745684 18.997895 30.746947 0 53.894737-23.147789 53.894737-53.894737V269.473684h-215.578948v538.947369h53.894737V323.368421z m-107.789473 242.526316v-242.526316h-53.894737v196.904421l-107.789474 40.421053v-243.927579l169.094737-48.316632-14.821053-51.819789L269.473684 276.102737v304.801684l-36.405895 13.662316 18.917053 50.472421 178.741895-67.018105c-5.039158 69.928421-55.269053 106.981053-165.133474 122.933894l7.733895 53.328842C325.712842 746.657684 485.052632 723.536842 485.052632 565.894737z" fill="#231F20"></path></symbol><symbol id="icon-rabbit" viewBox="0 0 1024 1024"><path d="M680.96 488.744421a1666.667789 1666.667789 0 0 0-54.433684-23.95621c-16.006737 12.234105-33.899789 20.264421-60.631579 20.264421h-80.842105c-36.810105 0-83.644632 30.396632-104.394106 67.772631-42.819368 77.123368-53.409684 117.813895-11.021473 201.701053C397.096421 808.879158 431.157895 876.409263 431.157895 970.105263h338.539789l68.338527-138.859789c20.129684-40.96 24.252632-73.701053 24.252631-110.349474 0.026947-57.397895-25.061053-159.717053-181.328842-232.151579z" fill="#FFBDD8"></path><path d="M862.315789 720.896c0 36.621474-4.122947 69.389474-24.252631 110.349474L769.697684 970.105263H485.052632v-53.894737h48.370526C507.877053 880.074105 485.052632 833.509053 485.052632 781.473684c0-59.418947 24.171789-113.313684 63.218526-152.360421l38.103579 38.103579A161.091368 161.091368 0 0 0 538.947368 781.473684c0 54.784 35.381895 104.043789 63.514948 134.736842h133.712842l53.490526-108.759579c15.710316-31.851789 18.755368-55.834947 18.755369-86.554947 0-80.976842-63.434105-150.096842-178.607158-195.503158-17.542737 8.138105-38.292211 13.554526-63.919158 13.554526h-80.842105c-13.958737 0-43.924211 15.979789-57.290106 40.016843l-47.104-26.165895C401.408 515.449263 448.242526 485.052632 485.052632 485.052632h80.842105c37.268211 0 57.478737-15.440842 79.090526-36.45979C625.367579 336.195368 549.753263 269.473684 485.052632 269.473684h-107.789474a21.288421 21.288421 0 0 0-5.955369 2.021053A683.762526 683.762526 0 0 0 302.187789 194.021053c-35.84-34.223158-61.763368-58.933895-94.908631-79.440842A42.442105 42.442105 0 0 0 185.478737 107.789474a22.824421 22.824421 0 0 0-17.381053 7.194947c-10.913684 11.425684-6.063158 28.240842 1.428211 39.181474 21.989053 32.121263 47.912421 56.858947 83.752421 91.109052 20.614737 19.671579 49.259789 43.169684 77.392842 63.08379C281.007158 367.400421 215.578947 484.432842 215.578947 592.842105c0 74.482526 24.791579 124.065684 51.065264 176.586106C294.534737 825.209263 323.368421 882.903579 323.368421 970.105263h-53.894737c0-74.482526-24.791579-124.065684-51.065263-176.586105C190.517895 737.738105 161.684211 680.043789 161.684211 592.842105c0-90.866526 42.226526-197.685895 93.453473-274.485894a803.759158 803.759158 0 0 1-39.046737-34.115369C177.852632 247.754105 150.231579 221.399579 125.035789 184.616421c-24.441263-35.759158-22.797474-78.686316 4.069053-106.819368 26.300632-27.567158 70.898526-31.043368 106.522947-9.000421 37.941895 23.444211 65.562947 49.798737 103.774316 86.258526 9.970526 9.512421 33.037474 32.309895 56.93979 60.550737h68.634947c-27.621053-37.780211-60.416-72.730947-88.522105-99.543579-28.833684-27.540211-54.730105-52.116211-84.533895-74.024421L326.305684 0.296421c31.232 23.228632 57.802105 48.532211 87.309474 76.719158 53.840842 51.388632 94.450526 100.594526 121.74821 146.83621 82.836211 26.650947 150.042947 116.870737 165.025685 230.750316l1.724631 13.177263-9.404631 9.404632c-3.772632 3.772632-7.706947 7.653053-11.802948 11.587368C837.227789 561.178947 862.315789 663.498105 862.315789 720.896zM309.463579 754.526316c3.934316 8.057263 7.895579 16.087579 11.991579 24.144842C348.887579 832.970105 377.263158 889.128421 377.263158 970.105263h53.894737c0-93.696-34.061474-161.226105-61.520842-215.578947h-60.173474z m597.90821 53.894737c-3.422316 9.404632-7.814737 19.806316-13.770105 31.959579L829.790316 970.105263h60.065684l52.143158-105.957052c10.778947-21.935158 17.515789-40.016842 21.90821-55.727158h-56.535579zM514.694737 390.736842c0-34.223158-13.231158-44.463158-29.642105-44.463158s-29.642105 10.24-29.642106 44.463158c0 34.250105 13.231158 44.463158 29.642106 44.463158s29.642105-10.213053 29.642105-44.463158z" fill="#231F20"></path></symbol><symbol id="icon-rat_zi" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#85C3DE"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-431.157895 188.631579v-215.578947h269.473685v-53.894737H538.947368v-39.585684c26.543158-18.081684 94.585263-65.050947 177.852632-127.488L700.631579 215.578947H323.368421v53.894737h295.316211a4221.008842 4221.008842 0 0 1-121.640421 85.369263l-11.991579 8.003369V431.157895H242.526316v53.894737h242.526316v215.578947c0 48.343579-13.850947 53.894737-134.736843 53.894737v53.894737c105.391158 0 188.631579 0 188.631579-107.789474z" fill="#231F20"></path></symbol><symbol id="icon-rat" viewBox="0 0 1024 1024"><path d="M727.659789 431.157895c-132.581053 0-220.348632 47.454316-285.803789 154.354526-19.779368 32.309895-15.845053 76.503579-9.404632 96.579368 3.260632 10.159158 7.760842 18.647579 12.422737 25.546106C464.761263 737.010526 499.927579 754.526316 538.947368 754.526316h66.829474c1.158737 17.893053-1.967158 34.762105-15.144421 53.975579-12.692211 18.539789-37.807158 40.151579-56.32 54.810947 25.249684-0.673684 52.709053-0.997053 83.240421-0.997053C877.487158 862.315789 970.105263 711.922526 970.105263 571.176421 936.421053 512 882.364632 431.157895 727.659789 431.157895z" fill="#85C3DE"></path><path d="M210.432 1012.897684l-43.573895-31.690105c106.954105-147.051789 185.317053-171.196632 423.828211-172.705684 21.396211-31.258947 16.249263-56.266105 9.377684-89.70779-3.557053-17.138526-7.221895-34.842947-7.221895-54.433684 0-68.958316 25.330526-104.636632 63.407158-136.973474l34.896842 41.040842c-29.453474 25.061053-44.409263 46.780632-44.409263 95.932632 0 14.093474 2.937263 28.402526 6.063158 43.546947 5.901474 28.510316 12.8 62.032842-1.131789 99.462737 166.373053-10.24 264.542316-96.902737 264.542315-236.193684C916.210526 418.330947 827.580632 323.368421 684.921263 323.368421c-83.644632 0-153.303579 29.696-174.187789 39.612632a224.875789 224.875789 0 0 1-20.533895 31.339789l-41.741474-34.115368 20.884211 17.057684-20.911158-16.976842C448.781474 359.828211 485.052632 314.287158 485.052632 262.736842c0-34.816-8.946526-60.766316-26.570106-77.069474-17.515789-16.249263-44.786526-24.602947-81.219368-24.953263V323.368421h-53.894737V109.783579l24.872421-1.913263c64.700632-4.931368 114.095158 7.895579 146.863158 38.238316C524.207158 173.056 538.947368 212.291368 538.947368 262.736842c0 11.102316-1.131789 21.908211-3.072 32.202105 37.268211-12.584421 89.842526-25.465263 149.045895-25.465263C858.165895 269.473684 970.105263 387.907368 970.105263 571.176421 970.105263 711.922526 877.487158 862.315789 617.552842 862.315789c-258.667789 0-311.942737 19.698526-407.120842 150.581895z m19.105684-256.835368c-12.045474 0-24.387368-0.565895-37.025684-1.64379l-22.096842-1.859368-2.425263-22.016C167.747368 728.144842 161.684211 672.444632 161.684211 631.026526c0-103.585684 21.450105-178.903579 53.894736-259.045052V107.789474h53.894737v274.782315l-2.021052 4.904422C235.439158 465.758316 215.578947 533.800421 215.578947 631.026526c0 22.878316 2.101895 51.442526 3.826527 70.979369 99.678316 2.802526 172.813474-35.408842 222.450526-116.493474l48.020211 24.090947c-11.237053 28.133053-11.371789 51.577263-0.377264 67.853474 9.701053 14.282105 28.645053 23.174737 49.448421 23.174737v53.894737c-39.019789 0-74.186105-17.515789-94.073263-46.888421a100.244211 100.244211 0 0 1-12.422737-25.546106c-53.221053 49.178947-121.128421 73.943579-202.913684 73.970527zM379.957895 525.473684c0-34.223158-13.231158-44.463158-29.642106-44.463158s-29.642105 10.24-29.642105 44.463158c0 34.250105 13.231158 44.463158 29.642105 44.463158s29.642105-10.213053 29.642106-44.463158z" fill="#231F20"></path></symbol><symbol id="icon-rooster_you" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#BBC4C9"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-215.578947-188.631579h-161.684211v-26.947368h161.684211V242.526316H269.473684v53.894737h161.684211v26.947368h-161.684211v485.052632h53.894737v-53.894737h377.263158v53.894737h53.894737V323.368421zM323.368421 646.736842h377.263158v53.894737H323.368421v-53.894737z m0-269.473684h107.789474c0 103.316211-72.784842 107.654737-81.084632 107.789474L350.315789 538.947368c46.592 0 134.736842-33.792 134.736843-161.68421h53.894736v107.789474c0 29.722947 24.171789 53.894737 53.894737 53.894736h107.789474v53.894737H323.368421v-215.578947z m377.263158 0v107.789474h-107.789474v-107.789474h107.789474z m-215.578947-80.842105h53.894736v26.947368h-53.894736v-26.947368z" fill="#231F20"></path></symbol><symbol id="icon-rooster" viewBox="0 0 1024 1024"><path d="M891.688421 506.421895C877.244632 455.033263 862.315789 401.893053 862.315789 323.368421V116.224l-323.368421 195.745684V323.368421c0 78.524632 14.928842 131.664842 29.372632 183.053474 12.611368 44.894316 24.522105 87.282526 24.522105 140.314947 0 101.618526-77.931789 176.693895-168.286316 203.991579l5.416422 11.587368h215.578947c24.333474 0 43.385263-0.242526 58.556631-2.128842C811.52 846.821053 916.210526 764.550737 916.210526 646.736842c0-53.032421-11.910737-95.420632-24.522105-140.314947z" fill="#FF8787"></path><path d="M673.684211 354.357895c-16.384 0-29.642105-10.213053-29.642106-44.463158 0-34.223158 13.231158-44.463158 29.642106-44.463158s29.642105 10.24 29.642105 44.463158c0 34.250105-13.258105 44.463158-29.642105 44.463158zM540.106105 970.105263l-50.58021-107.789474h156.05221l50.607158 107.789474h59.553684l-51.60421-109.918316C811.52 846.821053 916.210526 764.550737 916.210526 646.736842c0-53.032421-11.910737-95.420632-24.522105-140.314947C877.244632 455.033263 862.315789 401.893053 862.315789 323.368421V107.789474c0-59.445895-48.343579-107.789474-107.789473-107.789474a107.924211 107.924211 0 0 0-107.789474 106.172632 100.890947 100.890947 0 0 0-24.117895-3.314527 88.710737 88.710737 0 0 0-88.602947 88.602948c0 20.668632 5.227789 39.720421 10.671158 53.921684l-99.489684 59.688421 93.749894 14.470737V377.263158c0 14.416842-5.901474 21.692632-33.360842 49.152l-11.129263 11.129263C398.228211 326.521263 324.985263 269.473684 215.740632 269.473684 96.768 269.473684 0 366.241684 0 485.214316V646.736842h53.894737v-161.522526A162.007579 162.007579 0 0 1 215.740632 323.368421c82.081684 0 140.422737 36.244211 240.64 152.252632l-38.615579 38.615579C367.804632 461.285053 323.098947 431.157895 259.584 431.157895A151.983158 151.983158 0 0 0 107.789474 582.952421V754.526316h53.894737v-171.573895A98.007579 98.007579 0 0 1 259.584 485.052632c46.322526 0 79.629474 20.911158 137.027368 86.016l18.970948 21.530947 128.080842-128.080842C572.200421 435.981474 592.842105 415.366737 592.842105 377.263158v-97.926737l23.309474-14.120421-13.662316-23.04c-0.161684-0.242526-14.578526-24.899368-14.578526-50.688 0-19.132632 15.575579-34.708211 34.70821-34.708211 5.093053 0 26.785684 3.179789 39.558737 18.647579l26.327579 46.026106 39.774316-24.090948-20.372211-49.367579C704.754526 140.449684 700.631579 117.517474 700.631579 107.789474c0-29.722947 24.171789-53.894737 53.894737-53.894737s53.894737 24.171789 53.894737 53.894737v215.578947c0 85.935158 16.680421 145.300211 31.366736 197.632C851.887158 564.008421 862.315789 601.141895 862.315789 646.736842c0 95.285895-99.408842 161.684211-188.631578 161.684211h-209.461895l-68.419369-145.704421C375.242105 618.954105 338.108632 592.842105 296.448 592.842105A80.976842 80.976842 0 0 0 215.578947 673.711158V862.315789h53.894737v-188.604631c0-14.874947 12.099368-26.974316 26.974316-26.974316 20.533895 0 38.965895 14.147368 50.553263 38.858105L480.579368 970.105263h59.526737z" fill="#231F20"></path></symbol><symbol id="icon-snake_si" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#FF8787"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-242.041263 180.762947l-52.116211-13.797052C657.219368 749.864421 651.425684 754.526316 619.789474 754.526316h-242.526316V485.052632h269.473684v53.894736h53.894737V215.578947H323.368421v538.947369c0 29.722947 24.171789 53.894737 53.894737 53.894737h242.526316c77.689263 0 91.189895-51.065263 108.274526-115.658106zM377.263158 269.473684h269.473684v161.684211H377.263158v-161.684211z" fill="#231F20"></path></symbol><symbol id="icon-tiger_yin" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#7DD47F"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-257.42821 299.250526l-107.789474-53.894737-24.117895 48.208843 107.789474 53.894736 24.117895-48.208842z m-269.473685-5.658947l-24.117894-48.208842-107.789474 53.894737 24.117895 48.208842 107.789473-53.894737zM700.631579 431.157895h-161.684211v-53.894737h107.789474v-53.894737H377.263158v53.894737h107.789474v53.894737h-161.684211v323.368421h53.894737v-53.894737h269.473684v53.894737h53.894737V431.157895z m-161.684211 161.68421h107.789474v53.894737h-107.789474v-53.894737z m-161.68421 0h107.789474v53.894737h-107.789474v-53.894737z m161.68421-107.789473h107.789474v53.894736h-107.789474v-53.894736z m-161.68421 0h107.789474v53.894736h-107.789474v-53.894736zM754.526316 215.578947h-223.097263l-20.803369-62.410105-51.119158 17.057684L474.624 215.578947H269.473684v107.789474h53.894737v-53.894737h377.263158v53.894737h53.894737V215.578947z" fill="#231F20"></path></symbol><symbol id="icon-snake" viewBox="0 0 1024 1024"><path d="M107.789474 790.474105c0-72.434526 67.880421-91.513263 121.451789-91.513263 74.401684 0 153.815579 34.438737 237.891369 70.925474 50.580211 21.935158 104.609684 45.325474 162.250105 63.083789-52.412632 44.786526-118.784 74.347789-195.152842 83.078737-143.171368 16.357053-326.440421 7.006316-326.440421-125.574737zM377.263158 215.578947c-15.575579 0-30.288842 3.449263-43.654737 9.377685A250.691368 250.691368 0 0 0 323.368421 296.421053c0 115.550316 76.422737 169.391158 137.83579 212.614736 8.138105 5.712842 16.141474 11.371789 23.848421 17.057685V323.368421a107.789474 107.789474 0 0 0-107.789474-107.789474z" fill="#C3D686"></path><path d="M671.528421 788.857263c44.328421 11.964632 89.626947 19.563789 136.892632 19.56379 89.168842 0 161.684211-60.442947 161.68421-134.736842s-72.515368-134.736842-161.68421-134.736843c-19.078737 0-37.025684 1.509053-54.218106 4.015158-0.754526-101.402947-38.211368-172.355368-79.413894-219.648L673.684211 323.368421a1749.962105 1749.962105 0 0 1-79.036632-1.751579c45.702737 35.866947 108.705684 107.870316 105.984 232.367158 0 0.431158-0.080842 0.808421-0.10779 1.239579-34.923789 10.994526-66.155789 26.731789-95.097263 45.190737a163.085474 163.085474 0 0 0-15.845052-42.388211c-21.557895-39.639579-60.065684-66.775579-97.360842-93.022316C433.098105 423.343158 377.263158 384 377.263158 296.421053c0-130.290526 108.274526-188.631579 215.578947-188.631579 64.134737 0 132.715789 12.045474 214.366316 37.807158C802.330947 180.250947 780.099368 209.381053 700.631579 214.635789V161.684211h-53.894737v53.679157c-63.272421-1.024-104.528842-5.200842-104.986947-5.254736l-5.578106 53.598315C538.408421 263.949474 592.357053 269.473684 673.684211 269.473684c125.170526 0 188.631579-48.128 188.631578-143.063579V106.981053l-18.432-6.144C747.789474 68.823579 668.025263 53.894737 592.842105 53.894737c-158.666105 0-269.473684 99.732211-269.473684 242.526316 0 115.550316 76.422737 169.391158 137.83579 212.614736 33.684211 23.713684 65.509053 46.106947 81.003789 74.698106 9.539368 17.542737 13.285053 33.414737 12.341895 47.750737 21.153684 9.108211 42.118737 17.839158 62.949052 25.977263C671.151158 620.193684 729.977263 592.842105 808.421053 592.842105c59.445895 0 107.789474 36.271158 107.789473 80.842106s-48.343579 80.842105-107.789473 80.842105c-105.472 0-203.237053-42.388211-297.768421-83.429053-94.800842-41.094737-184.346947-79.952842-281.411369-79.952842C122.718316 591.171368 53.894737 644.715789 53.894737 727.578947c0 79.063579 67.098947 136.434526 159.555368 136.434527 142.174316 0 230.426947-66.883368 306.79579-129.886316 31.420632 13.419789 62.787368 26.058105 94.450526 37.133474-47.077053 49.637053-110.969263 82.566737-186.610526 91.270736l5.066105 53.625264c93.453474-7.006316 143.144421 9.350737 195.718737 26.543157 46.457263 15.225263 94.127158 30.854737 169.822316 30.854737 19.994947 0 41.957053-1.077895 66.344421-3.557052l-5.416421-53.625263c-105.283368 10.778947-158.100211-6.548211-213.935158-24.872422-22.150737-7.275789-44.624842-14.632421-70.305684-20.345263a334.848 334.848 0 0 0 96.14821-82.297263z m-458.078316 21.261474C162.573474 810.118737 107.789474 784.276211 107.789474 727.578947c0-60.847158 62.733474-82.539789 121.451789-82.539789 77.850947 0 154.731789 30.288842 235.250526 64.943158-66.263579 52.924632-139.722105 100.136421-251.041684 100.136421z" fill="#231F20"></path></symbol><symbol id="icon-tiger" viewBox="0 0 1024 1024"><path d="M431.157895 162.250105V134.736842c0-41.552842-39.289263-80.842105-80.842106-80.842105-28.833684 0-57.128421 4.661895-58.314105 4.850526L269.473684 62.490947v83.887158C144.788211 223.824842 89.222737 346.839579 66.991158 431.157895h266.051368c240.747789 0 415.851789 107.789474 415.85179 269.473684-14.848-25.114947-43.924211-53.894737-88.68379-53.894737-67.988211 0-121.263158 71.033263-121.263158 161.684211 0 66.802526 30.477474 119.888842 60.712421 156.16 12.638316 15.171368 36.055579 37.726316 59.014737 58.88 5.066105 0.107789 9.781895 0.538947 15.009685 0.538947 219.297684 0 350.315789-191.811368 350.315789-377.263158C1024 327.545263 679.855158 172.813474 431.157895 162.250105z" fill="#F7C768"></path><path d="M673.684211 1024c-114.768842 0-188.820211-33.333895-254.167579-62.787368-53.625263-24.144842-99.974737-45.002105-161.28-45.002106-40.448 0-83.590737 23.255579-103.639579 45.16379l-39.747369-36.432842C142.497684 894.787368 199.168 862.315789 258.236632 862.315789c68.392421 0 119.861895 21.288421 172.921263 45.056V673.684211c0-35.166316-17.542737-64.107789-30.639158-80.815158-15.198316 9.835789-32.067368 18.890105-50.741895 26.947368l-21.342316-49.475368C469.800421 509.413053 485.052632 377.317053 485.052632 323.368421V221.642105A597.827368 597.827368 0 0 0 404.210526 215.578947h-26.947368V134.736842c0-12.099368-14.848-26.947368-26.947369-26.947368-9.377684 0-18.836211 0.592842-26.947368 1.347368V269.473684h-53.894737V211.671579c-136.030316 102.912-158.450526 266.886737-161.306947 295.882105 9.135158 9.108211 38.992842 25.061053 71.976421 38.669474l38.103579-59.365053 12.449684-1.589894C321.212632 473.653895 377.263158 392.192 377.263158 323.368421h53.894737c0 88.333474-68.796632 192.242526-180.870737 213.342316l-48.397474 75.398737-20.291368-7.437474C53.894737 557.756632 53.894737 523.317895 53.894737 512c0-50.041263 37.025684-254.733474 215.578947-365.621895V62.490947l22.528-3.745684C293.187368 58.556632 321.482105 53.894737 350.315789 53.894737c41.552842 0 80.842105 39.289263 80.842106 80.842105v27.513263c248.697263 10.563368 592.842105 165.295158 592.842105 484.486737 0 185.451789-131.018105 377.263158-350.315789 377.263158z m-13.473685-323.368421c-36.513684 0-67.368421 49.367579-67.368421 107.789474 0 85.746526 68.096 145.084632 89.465263 161.549473 91.540211-2.533053 164.378947-45.487158 213.827369-107.654737H700.631579v-53.894736h230.238316c8.919579-17.273263 16.357053-35.354947 22.285473-53.894737h-239.885473l-6.467369-17.650527C706.290526 735.582316 692.439579 700.631579 660.210526 700.631579zM485.052632 931.112421c33.926737 14.066526 70.521263 26.597053 114.607157 33.468632C569.424842 928.309895 538.947368 875.223579 538.947368 808.421053c0-90.650947 53.274947-161.684211 121.263158-161.684211 44.759579 0 73.835789 28.779789 88.68379 53.894737h217.007158c2.775579-17.866105 4.203789-35.920842 4.203789-53.894737 0-38.938947-5.658947-74.752-15.925895-107.627789l-126.706526 126.679579-38.103579-38.103579L932.001684 485.052632a367.939368 367.939368 0 0 0-57.775158-81.596632l-154.543158 154.543158-38.103579-38.103579 153.573053-153.573053a537.869474 537.869474 0 0 0-82.593684-56.751158l-140.665263 140.638316-38.103579-38.103579 128.134737-128.134737A794.731789 794.731789 0 0 0 538.947368 231.046737V323.368421c0 50.149053-11.102316 156.698947-95.932631 236.328421 18.378105 23.417263 42.037895 63.407158 42.037895 113.987369v257.42821zM215.578947 431.157895v-53.894737c39.774316 0 53.894737-29.022316 53.894737-53.894737h53.894737c0 53.571368-37.025684 107.789474-107.789474 107.789474z" fill="#231F20"></path></symbol><symbol id="icon-boar" viewBox="0 0 1024 1024"><path d="M732.079158 377.263158c-107.789474 0-186.421895 31.393684-281.869474 126.841263L180.331789 773.982316C257.724632 807.909053 348.725895 808.421053 485.052632 808.421053h96.013473c55.834947-34.411789 133.551158-53.894737 227.354948-53.894737h121.344L970.105263 680.555789V572.631579c0-94.315789-130.236632-195.368421-238.026105-195.368421z" fill="#FFBDD8"></path><path d="M808.421053 700.631579v53.894737c-196.446316 0-323.368421 84.641684-323.368421 215.578947h-53.894737c0-163.705263 148.075789-269.473684 377.263158-269.473684z m-323.368421 107.789474v-53.894737c-158.342737 0-245.598316 0-319.649685-49.367579L158.612211 700.631579H80.842105c-21.692632 0-26.624-14.821053-26.947368-26.947368v-82.620632c84.156632-11.183158 161.684211-74.913684 161.68421-186.853053V215.578947H161.684211v161.684211H134.736842c-66.964211 0-134.736842 37.025684-134.736842 107.789474h53.894737c0-42.630737 52.870737-53.894737 80.842105-53.894737h24.629895C147.132632 504.912842 85.153684 538.947368 26.947368 538.947368H0v134.736843c0 32.498526 21.530947 80.842105 80.842105 80.842105h61.682527c32.687158 20.506947 67.125895 33.145263 105.957052 41.013895A232.879158 232.879158 0 0 0 215.578947 916.210526h53.894737c0-41.930105 14.012632-80.303158 39.424-112.505263C358.885053 808.151579 415.959579 808.421053 485.052632 808.421053z m-72.946527-342.420211L323.368421 554.738526V431.157895h-53.894737v253.682526l180.736-180.736-38.103579-38.103579zM323.368421 161.684211h-53.894737v190.032842a769.536 769.536 0 0 1 53.894737-49.098106V161.684211z m323.368421-53.894737c-72.623158 0-146.809263 23.336421-215.578947 58.637473V107.789474h-53.894737v154.138947C458.832842 205.392842 555.331368 161.684211 646.736842 161.684211c148.587789 0 269.473684 120.885895 269.473684 269.473684v235.654737L809.579789 862.315789h61.359158L970.105263 680.555789V431.157895c0-178.310737-145.057684-323.368421-323.368421-323.368421z" fill="#231F20"></path></symbol><symbol id="icon-boar_hai" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#85C3DE"></path><path d="M309.975579 804.756211l-27.136-46.592c103.073684-60.011789 183.026526-132.473263 241.475368-219.24379H350.315789l-13.473684-50.283789c58.88-33.980632 99.435789-117.571368 118.703158-165.295158H242.526316v-53.894737h538.947368v53.894737h-268.18021c-12.395789 34.088421-42.469053 106.603789-90.435369 161.68421h134.009263a680.555789 680.555789 0 0 0 46.349474-107.708631l51.092211 17.057684c-58.421895 175.265684-171.034947 309.490526-344.333474 410.381474z m192.350316-2.937264L467.806316 760.454737c88.414316-73.728 154.516211-158.773895 202.105263-259.907369l48.801684 22.959158a797.372632 797.372632 0 0 1-82.351158 137.781895c32.741053 15.009684 83.456 44.867368 137.647158 101.591579l-38.938947 37.268211c-57.236211-59.877053-109.325474-85.557895-133.766737-95.178106a850.997895 850.997895 0 0 1-98.977684 96.848842z m48.613052-536.872421l-80.842105-53.894737 29.884632-44.840421 80.842105 53.894737-29.884632 44.840421zM512 53.894737C259.395368 53.894737 53.894737 259.395368 53.894737 512s205.500632 458.105263 458.105263 458.105263c9.081263 0 17.973895-0.835368 26.947368-1.374316v-53.894736c-8.946526 0.619789-17.866105 1.374316-26.947368 1.374315-222.881684 0-404.210526-181.328842-404.210526-404.210526S289.118316 107.789474 512 107.789474s404.210526 181.328842 404.210526 404.210526c0 195.206737-139.075368 358.507789-323.368421 396.045474v54.460631c214.096842-38.346105 377.263158-225.549474 377.263158-450.533052C970.105263 259.395368 764.604632 53.894737 512 53.894737z" fill="#231F20"></path></symbol><symbol id="icon-bilibili1" viewBox="0 0 1129 1024"><path d="M234.909 9.656a80.468 80.468 0 0 1 68.398 0 167.374 167.374 0 0 1 41.843 30.578l160.937 140.82h115.07l160.936-140.82a168.983 168.983 0 0 1 41.843-30.578A80.468 80.468 0 0 1 930.96 76.445a80.468 80.468 0 0 1-17.703 53.914 449.818 449.818 0 0 1-35.406 32.187 232.553 232.553 0 0 1-22.531 18.508h100.585a170.593 170.593 0 0 1 118.289 53.109 171.397 171.397 0 0 1 53.914 118.288v462.693a325.897 325.897 0 0 1-4.024 70.007 178.64 178.64 0 0 1-80.468 112.656 173.007 173.007 0 0 1-92.539 25.75H212.377a341.186 341.186 0 0 1-72.421-4.024A177.835 177.835 0 0 1 28.91 939.065a172.202 172.202 0 0 1-27.36-92.539V388.662a360.498 360.498 0 0 1 0-66.789A177.03 177.03 0 0 1 162.487 178.64h105.414c-16.899-12.07-31.383-26.555-46.672-39.43a80.468 80.468 0 0 1-25.75-65.984 80.468 80.468 0 0 1 39.43-63.57M216.4 321.873a80.468 80.468 0 0 0-63.57 57.937 108.632 108.632 0 0 0 0 30.578v380.615a80.468 80.468 0 0 0 55.523 80.469 106.218 106.218 0 0 0 34.601 5.632h654.208a80.468 80.468 0 0 0 76.444-47.476 112.656 112.656 0 0 0 8.047-53.109v-354.06a135.187 135.187 0 0 0 0-38.625 80.468 80.468 0 0 0-52.304-54.719 129.554 129.554 0 0 0-49.89-7.242H254.22a268.764 268.764 0 0 0-37.82 0z m0 0" fill="#20B0E3"></path><path d="M348.369 447.404a80.468 80.468 0 0 1 55.523 18.507 80.468 80.468 0 0 1 28.164 59.547v80.468a80.468 80.468 0 0 1-16.094 51.5 80.468 80.468 0 0 1-131.968-9.656 104.609 104.609 0 0 1-10.46-54.719v-80.468a80.468 80.468 0 0 1 70.007-67.593z m416.02 0a80.468 80.468 0 0 1 86.102 75.64v80.468a94.148 94.148 0 0 1-12.07 53.11 80.468 80.468 0 0 1-132.773 0 95.757 95.757 0 0 1-12.875-57.133V519.02a80.468 80.468 0 0 1 70.007-70.812z m0 0" fill="#20B0E3"></path></symbol><symbol id="icon-yinle" viewBox="0 0 1024 1024"><path d="M512.2976 0a531.2 531.2 0 0 0-512 548.48V960h128V548.48a398.72 398.72 0 0 1 384-411.52 398.72 398.72 0 0 1 384 411.52V960h128V548.48A531.2 531.2 0 0 0 512.2976 0z" fill="#5c8add"></path><path d="M64.2976 576l256 0 0 448-256 0 0-448Z" fill="#5c8add"></path><path d="M704.2976 576l256 0 0 448-256 0 0-448Z" fill="#5c8add"></path></symbol><symbol id="icon-icon-test-copy" viewBox="0 0 1024 1024"><path d="M512 512m-229.517241 0a229.517241 229.517241 0 1 0 459.034482 0 229.517241 229.517241 0 1 0-459.034482 0Z" fill="#5c8add"></path><path d="M512 1024A512 512 0 1 1 1024 512 512 512 0 0 1 512 1024z m0-141.241379A370.758621 370.758621 0 1 0 141.241379 512 370.758621 370.758621 0 0 0 512 882.758621z" fill="#5c8add"></path></symbol><symbol id="icon-V" viewBox="0 0 1024 1024"><path d="M1012.47774251 492.58192592L544.94137566 87.22962963a49.96686561 49.96686561 0 0 0-65.88275132 0L11.63784127 492.6975097c-21.03624691 18.26223633-23.3479224 49.93219048-5.08568606 70.96843739 18.03106878 21.03624691 49.93219048 23.3479224 70.96843738 5.08568607L512 191.83294532l434.71057495 376.91868784c9.47786949 8.20644797 21.26741446 12.25188008 32.82579189 12.13629629 14.10122046 0 27.97127337-5.77918871 38.02706173-17.33756613 18.14665256-20.92066314 15.95056084-52.70620106-5.08568606-70.9684374z" fill="#5c8add"></path><path d="M109.30613051 567.59579541V896.89396825c0 42.53482892 34.90629982 77.44112875 77.44112875 77.44112875h220.76500882V666.30433862c0-25.54401411 20.92066314-46.46467725 46.46467724-46.46467724h116.16169313c25.54401411 0 46.46467725 20.92066314 46.46467725 46.46467724V974.335097h220.76500882c42.53482892 0 77.44112875-34.90629982 77.44112874-77.44112875l0.11558377-329.29817284L512 218.18604586 109.30613051 567.59579541zM848.00203175 197.49655027h-63.91782716c-12.82979894 0-23.23233862 10.40253968-23.23233863 23.23233862v24.27259259l110.49808818 95.70336508V220.72888889h-0.11558377c0-12.82979894-10.40253968-23.23233862-23.23233862-23.23233862zM905.44716754 83.18419754s-34.90629982 56.86721693-89.11508994 100.32671603c152.68616579 13.98563668 127.83565432-133.26809171 127.83565432-133.2680917-134.07717813-10.28695591-132.92134039 102.29164021-131.072 127.83565432 20.92066314-20.92066314 49.70102293-62.64640564 92.35143562-94.89427865zM798.53217637 174.61096297c-19.64924162-16.52847972-40.56990476-43.45949912-51.203612-53.97762258 0 0 32.94137566 20.57391182 56.40488184 49.3542716 2.42725926-18.37782011 6.47269135-93.3916896-93.16052205-85.3008254 0 0-13.98563668 104.71889947 87.95925221 89.92417638z" fill="#5c8add"></path></symbol><symbol id="icon-zhifeiji" viewBox="0 0 1167 1024"><path d="M41.201759 463.52493L1110.665064 30.117647c10.32605-4.159104 21.942857 0.860504 26.101961 11.043137 1.434174 3.728852 1.864426 7.744538 1.003921 11.616807L949.033691 978.823529c-2.151261 10.89972-12.764146 17.927171-23.663865 15.632493-2.72493-0.573669-5.306443-1.721008-7.601121-3.298599L634.80624 789.79944l-163.065546 133.951821c-16.492997 13.62465-40.87395 11.186555-54.498599-5.306443-3.011765-3.728852-5.306443-7.887955-6.884034-12.477311l-102.973669-313.080112-265.178712-91.787115c-10.469468-3.585434-16.062745-15.058824-12.333893-25.528291 1.864426-5.44986 6.023529-9.895798 11.329972-12.047059z" fill="#FCFDFC"></path><path d="M929.385512 1023.569748c-3.155182 0-6.453782-0.286835-9.752381-1.003922-6.740616-1.434174-12.907563-4.015686-18.50084-8.031372L635.953579 825.940616l-146.142297 120.040336c-13.911485 11.473389-31.408403 16.779832-49.335574 15.058824-17.927171-1.721008-34.133333-10.32605-45.463305-24.237535-5.306443-6.453782-9.322129-13.768067-11.903642-21.79944l-98.527731-299.598879-251.697479-87.19776c-12.333894-4.302521-22.229692-13.05098-27.966386-24.811204s-6.453782-24.954622-2.151261-37.288515c4.589356-13.337815 14.771989-23.9507 27.82297-29.257143L1099.908761 3.585434c24.954622-10.039216 53.351261 2.007843 63.533894 26.819048 3.585434 8.891877 4.445938 18.644258 2.581513 28.109804L977.143495 984.560224c-4.732773 23.090196-25.098039 39.009524-47.757983 39.009524z m-294.579272-233.770308l282.962465 201.357983c2.294678 1.577591 4.87619 2.72493 7.601121 3.298599 10.89972 2.151261 21.512605-4.87619 23.663865-15.632493L1137.914364 52.777591c0.860504-3.872269 0.430252-7.887955-1.003922-11.616807-4.159104-10.32605-15.919328-15.202241-26.101961-11.043137L41.201759 463.52493c-5.306443 2.151261-9.465546 6.597199-11.47339 12.047059-1.721008 5.019608-1.434174 10.469468 0.860505 15.345658 2.294678 4.87619 6.453782 8.461625 11.473389 10.182633l265.178711 91.787115L410.214644 905.967507c1.434174 4.589356 3.872269 8.748459 6.884033 12.477311 6.597199 8.031373 15.919328 12.907563 26.101961 13.911485 10.32605 1.003922 20.365266-2.007843 28.396639-8.605042l163.208963-133.951821z" fill="#4A4A4A"></path><path d="M307.097557 592.743978l105.698599 316.091876c6.310364 18.787675 26.532213 28.970308 45.319888 22.659944 4.159104-1.434174 7.887955-3.442017 11.186555-6.166946l164.786555-133.951821-165.360224-118.892997c297.017367-287.982073 447.462185-433.980952 451.191036-437.853222 0.573669-0.573669 2.581513-3.442017 0.430252-7.027451-1.290756-1.577591-3.298599-3.298599-7.027451-2.15126-202.218487 120.327171-404.293557 242.805602-606.22521 367.291877z" fill="#CAE0EE"></path><path d="M446.786072 934.794398c-5.736695 0-11.329972-1.290756-16.636414-3.872269-8.891877-4.445938-15.632493-12.047059-18.787675-21.512605L305.376549 592.313725l1.003921-0.573669C507.308201 467.684034 711.391114 344.058263 912.60568 224.161345l0.286835-0.143418c3.585434-1.147339 6.310364-0.286835 8.605042 2.581513l0.143417 0.143417c2.438095 4.015686 0.573669 7.457703-0.573669 8.74846-3.872269 4.015686-155.177591 150.87507-450.043698 436.705882l165.503642 119.036414-166.220728 135.09916c-3.442017 2.868347-7.457703 5.019608-11.760225 6.453782-3.728852 1.290756-7.744538 2.007843-11.760224 2.007843z m-137.967507-341.333334l105.268348 314.944538c2.868347 8.748459 9.035294 15.77591 17.210084 19.935014 8.17479 4.159104 17.496919 4.732773 26.245378 1.864426 3.872269-1.290756 7.60112-3.298599 10.756302-5.880112l163.352381-132.804482L466.434252 672.627451l1.290756-1.147339C763.308201 384.932213 915.043775 237.642577 918.772627 233.626891c0 0 2.007843-2.294678 0.286835-5.306443-1.003922-1.290756-2.438095-2.438095-5.306443-1.577591-200.784314 119.610084-404.293557 242.94902-604.934454 366.718207z" fill="#CAE0EE"></path><path d="M460.840974 924.898599l7.457703-253.561904 165.933894 119.896918-168.658824 135.959664c-1.290756 1.003922-3.011765 0.860504-4.015686-0.430252-0.430252-0.430252-0.717087-1.147339-0.717087-1.864426z" fill="#94C3E2"></path><path d="M463.709322 929.344538c-1.290756 0-2.438095-0.573669-3.2986-1.577591-0.573669-0.860504-1.003922-1.864426-1.003921-2.868348l7.60112-256.286834 169.519328 122.621848-1.434174 1.147339-168.658823 135.959664c-0.860504 0.717087-1.721008 1.003922-2.72493 1.003922z m6.023529-255.282913l-7.457703 250.836974c0 0.286835 0.143417 0.717087 0.286835 1.003922 0.430252 0.573669 1.434174 0.717087 2.007843 0.286835l167.22465-134.812325-162.061625-117.315406z" fill="#94C3E2"></path></symbol><symbol id="icon-lianjie" viewBox="0 0 1079 1024"><path d="M695.355535 432.666896c-0.553495-1.10699-0.885592-2.186305-1.383737-3.265619-0.193723-0.193723-0.193723-0.359772-0.359771-0.719543-12.508983-26.318678-39.436506-43.366319-69.325226-41.013966-39.076734 3.265619-68.439634 39.021384-65.312388 79.841627 0.857917 10.516401 3.653066 20.147211 7.998 28.83708 19.78744 46.659613 11.097571 103.448181-25.377737 141.750022l-191.094085 199.950001a118.088119 118.088119 0 0 1-171.998513 0c-47.434506-49.537786-47.434506-130.098956 0-179.636742l71.234782-74.389703-0.52582-0.553494a75.911814 75.911814 0 0 0 24.326097-61.880721c-3.127246-40.820243-37.3609-71.51153-76.437634-68.24591a69.463599 69.463599 0 0 0-46.908685 23.966325l-0.166049-0.193723-72.618519 75.856464c-103.226783 107.793115-103.226783 282.36538 0 390.158495 103.171433 107.793115 270.299193 107.793115 373.498301 0l191.619904-200.1714c80.256748-83.992838 97.636485-208.307773 52.83108-310.289193z" fill="#5c8add"></path><path d="M1002.047012 80.865592c-103.226783-107.82079-270.382217-107.82079-373.581325 0l-191.619905 200.199075c-80.284423 83.854464-97.66416 208.197074-52.997128 310.233843 0.52582 1.079315 0.857917 2.15863 1.383737 3.26562 0.166048 0.166048 0.166048 0.359772 0.332097 0.719543 12.536658 26.291004 39.46418 43.366319 69.3529 41.013966 39.076734-3.265619 68.439634-39.021384 65.312388-79.869302a78.679288 78.679288 0 0 0-7.998-28.864755c-19.78744-46.631938-11.097571-103.448181 25.377737-141.750022l191.287808-199.839302a118.088119 118.088119 0 0 1 172.026188 0c47.434506 49.537786 47.434506 130.126631 0 179.692091l-71.234782 74.417378 0.52582 0.553495a75.939489 75.939489 0 0 0-24.353772 61.88072c3.15492 40.847917 37.3609 71.51153 76.465309 68.245911a69.463599 69.463599 0 0 0 46.908685-23.938651l0.166049 0.166048 72.646194-75.856464c103.03306-107.82079 103.03306-282.642127 0-390.269194z" fill="#5c8add"></path></symbol><symbol id="icon-liaotian" viewBox="0 0 1171 1024"><path d="M1068.71699 0.243751H102.193768C46.228437 0.243751 0.500666 45.045267 0.500666 99.74309v696.251622c0 54.697824 45.727771 99.450589 101.693102 99.450589h329.113198l120.851966 114.465677a48.652788 48.652788 0 0 0 66.641644 0l120.851966-114.465677h329.064448c55.965331 0 101.741852-44.752765 101.741852-99.450589V99.74309C1170.458842 45.045267 1124.682321 0.243751 1068.71699 0.243751z m-439.776354 596.849784h-370.989696c-27.933915 0-50.846551-22.425133-50.846551-49.774045 0-27.348912 22.912636-49.725294 50.846551-49.725294h370.989696c27.933915 0 50.846551 22.376382 50.846551 49.725294 0 27.348912-22.912636 49.774045-50.846551 49.774045z m287.18795-211.381252H254.782171a50.456549 50.456549 0 0 1-50.846551-49.725294c0-27.397662 22.912636-49.774045 50.846551-49.774045h661.346415c27.933915 0 50.846551 22.376382 50.846551 49.774045 0 27.348912-22.912636 49.725294-50.846551 49.725294z" fill="#5C8ADD"></path></symbol><symbol id="icon-xinfeng" viewBox="0 0 1400 1024"><path d="M1301.63733163 214.78520234a207.81921797 207.81921797 0 0 1 7.02423018 52.42036465v489.73590176a205.10753818 205.10753818 0 0 1-205.05853125 205.05853125H283.05853124A205.15654424 205.15654424 0 0 1 77.99999999 756.79444971V267.20556699a201.36672685 201.36672685 0 0 1 7.02423106-52.42036465L586.24393329 562.1905874c69.44187217 51.96297217 146.36536612 49.13694404 214.1736961 0zM1103.60303056 62.0000167H283.05853124A204.50312753 204.50312753 0 0 0 106.37462518 163.41030547l489.71956641 335.75823018c62.43397646 50.77048623 127.85733457 50.31309463 194.62019765 0L1280.28693749 163.41030547A204.68281729 204.68281729 0 0 0 1103.60303056 62.0000167z m0 0" fill="#5c8add"></path></symbol><symbol id="icon-QQ1" viewBox="0 0 1024 1024"><path d="M0 512a512 512 0 1 0 1024 0A512 512 0 1 0 0 512z" fill="#18ACFC"></path><path d="M500.113 228.39c118.396-1.518 178.924 61.004 201 156 3.497 15.048 0.15 34.807 0 50 27.143 5.682 33.087 60.106 10 75v1h1c8.26 14.33 19.04 28.125 26 44 7.332 16.723 9.306 35.16 14 55 4.024 17.01-2.287 51.505-10 57-0.771 0.683-2.231 1.312-3 2-14.601-3.016-30.377-16.865-38-27-3.065-4.074-5.275-9.672-10-12-0.395 21.568-12.503 41.15-22 55-3.514 5.123-14.073 13.217-14 18 3.691 2.836 8.305 2.956 13 5 10.513 4.577 25.449 13.168 32 22 2.334 3.146 5.548 7.555 7 11 16.193 38.414-36.527 48.314-63 54-27.185 5.839-77.818-10.224-92-19-8.749-5.414-16.863-18.573-29-19-3.666 2.389-14.438 1.132-20 1-16.829 32.804-101.913 47.868-148 31-14.061-5.146-43.398-17.695-38-40 4.437-18.327 19.947-29.224 35-37 5.759-2.975 18.915-4.419 22-10-13.141-8.988-24.521-28.659-31-44-3.412-8.077-4.193-25.775-9-32-7.789 12.245-32.097 36.91-52 33-3.071-4.553-7.213-9.097-9-15-4.792-15.835-1.81-40.379 2-54 8.117-29.02 16.965-50.623 32-72 4.672-6.643 11.425-12.135 16-19-8.945-9.733-6.951-37.536-1-49 4.002-7.709 9.701-7.413 10-20-1.92-3.022-0.071-8.604-1-13-4.383-20.75 3.273-47.552 9-63 19.8-53.421 53.712-90.466 105-112 11.986-5.033 25.833-7.783 39-11 5.322-1.3 11.969 0.518 16-2z" fill="#FFFFFF"></path></symbol><symbol id="icon-rss" viewBox="0 0 1024 1024"><path d="M749.61196492 908.06119793C749.61196492 560.41848146 463.58151854 274.36328126 115.93880207 274.36328126V115.93880207c434.50388795 0 792.12239584 357.61850789 792.12239586 792.12239586zM224.55858562 690.72261555a108.91682943 108.91682943 0 0 1 108.69404499 108.74355267C333.25263061 859.29616292 284.24005737 908.06119793 224.31104736 908.06119793 164.48105265 908.06119793 115.96355592 859.41993206 115.96355592 799.46616822s48.69077351-108.71879883 108.61978351-108.74355267zM641.01693522 908.06119793h-153.96879069c0-203.60020956-167.50913289-371.13409627-371.10934246-371.13409629v-153.96879068c288.03550619 0 525.07813313 237.11688843 525.07813315 525.10288697z" fill="#FFA500"></path></symbol><symbol id="icon-youxiang" viewBox="0 0 1024 1024"><path d="M583.60666667 972h-68.08c-8.43333333 0-15.33333333-6.9-15.33333334-15.33333333V609.52c0-8.43333333 6.9-15.33333333 15.33333334-15.33333333h68.08c8.43333333 0 15.33333333 6.9 15.33333333 15.33333333V956.66666667c0 8.43333333-6.9 15.33333333-15.33333333 15.33333333z" fill="#629FF9"></path><path d="M294.42 167c-113.62 0-205.77333333 92-205.77333333 205.31333333v336.72h411.39333333V372.31333333c0.15333333-113.31333333-92-205.31333333-205.62-205.31333333z" fill="#2166CC"></path><path d="M519.97333333 627H216.98666667c-25.45333333 0-46-20.54666667-46-46V393.78c0-25.45333333 20.54666667-46 46-46h302.98666666c25.45333333 0 46 20.54666667 46 46V581c0 25.45333333-20.54666667 46-46 46z" fill="#D2E4FF"></path><path d="M565.97333333 397a49.22 49.22 0 0 0-49.37333333-49.22H220.36c-27.29333333 0-49.37333333 22.08-49.37333333 49.22v10.27333333l179.4 94.60666667c11.34666667 5.98 24.84 5.98 36.18666666 0l179.4-94.60666667v-10.27333333z" fill="#FFFFFF"></path><path d="M730.5 167h-427.8v0.46c109.78666667 4.29333333 197.49333333 94.3 197.49333333 205.00666667v336.72h411.39333334c27.29333333 0 49.37333333-22.08 49.37333333-49.22V397c0-126.96-103.19333333-230-230.46-230z" fill="#4E8DF6"></path><path d="M845.80666667 52H681.12666667c-9.04666667 0-16.40666667 7.36-16.40666667 16.40666667v336.72a24.67133333 24.67133333 0 1 0 49.37333333 0V134.18666667h131.71333334c9.04666667 0 16.40666667-7.36 16.40666666-16.40666667V68.40666667c0-9.04666667-7.36-16.40666667-16.40666666-16.40666667z" fill="#2166CC"></path><path d="M896.25333333 659.81333333h-35.11333333c-8.43333333 0-15.33333333-6.9-15.33333333-15.33333333v-35.11333333c0-8.43333333 6.9-15.33333333 15.33333333-15.33333334h35.11333333c8.43333333 0 15.33333333 6.9 15.33333334 15.33333334v35.11333333c0 8.58666667-6.9 15.33333333-15.33333334 15.33333333z" fill="#FFFFFF"></path><path d="M88.8 709.18666667l-24.22666667 131.40666666c-9.66 54.43333333 26.83333333 98.59333333 81.26666667 98.59333334h213.9c54.58666667 0 106.56666667-44.16 116.22666667-98.59333334l23.15333333-131.40666666H88.8z" fill="#2974CE"></path></symbol><symbol id="icon-gitHub" viewBox="0 0 1049 1024"><path d="M523.6581816 52C262.83923907 52 52 262.8401375 52 523.6581816c0 208.49703047 135.09433812 384.97758117 322.50789391 447.44906532 23.42658172 4.68531653 32.01647887-10.15136894 32.01647796-22.64584583 0-10.93210574-0.78163433-48.41463703-0.78163433-87.45953855-131.18885996 28.11189824-158.5200223-56.22379738-158.52002231-56.22379739-21.08437312-54.66232469-52.3201152-68.71827336-52.3201152-68.71827335-42.94858371-28.89353348 3.12384382-28.89353348 3.12384384-28.89353348 47.63479867 3.12384382 72.62285398 48.41643391 72.62285398 48.4164339 42.16784782 71.84121875 110.10538527 51.53758242 137.43654672 39.04400399 3.90457972-30.45500618 16.3990566-51.5393793 29.67427028-63.25222094-104.64023039-10.93300418-214.74561566-51.53848086-214.74561657-232.70524742 0-51.53848086 18.74126609-93.70632867 48.4164339-126.50444187-4.68621496-11.71284164-21.08527156-60.12837711 4.6844181-124.94207075 0 0 39.82563922-12.49447688 129.62738726 48.41463704 37.48253129-10.15136894 78.08980484-15.61742227 117.91454562-15.61742137s80.43201433 5.46605242 117.91454473 15.61742137c89.80264648-60.90911391 129.62828571-48.41463703 129.62828571-48.41463704 25.76879122 64.81369363 9.37063305 113.22922911 4.68531651 124.94207075 30.45410773 32.79721477 48.41463703 74.96506258 48.41463703 126.50444187 0 181.16676656-110.10538527 220.99150644-215.52545401 232.70524742 17.1797934 14.83668547 32.01647887 42.94858371 32.01647886 87.45953946 0 63.25222094-0.78163433 114.009965-0.78163523 129.62738636 0 12.49447688 8.59079468 27.33116234 32.01737731 22.64584583 187.41265734-62.4705866 322.50699547-238.95203574 322.50699546-447.44996375C995.31636231 262.8401375 783.69369203 52 523.6581816 52z" fill="#663399"></path><path d="M230.82365863 729.03136735c-0.7807359 2.34310703-4.68531653 3.12384382-7.80916035 1.56237113s-5.46605242-4.68531653-3.90368129-7.02842356c0.7807359-2.34220859 4.68531653-3.12384382 7.80826192-1.56147269s4.68531653 4.68531653 3.90457972 7.02752512z m18.7412661 21.08437312c-2.34220859 2.34220859-7.02752512 0.78163433-9.37063305-2.34310703-3.12294539-3.12294539-3.90457972-7.80826192-1.5614727-10.15136894 2.34220859-2.34220859 6.24678922-0.7807359 9.37063305 2.34310702 3.12384382 3.90457972 3.90457972 8.58899782 1.5614727 10.15136895zM268.30618992 777.44690281c-3.12294539 2.34220859-7.80826192 0-10.15136895-3.90457972-3.12384382-3.90457972-3.12384382-9.37063305 0-10.93210574 3.12384382-2.34310703 7.80916035 0 10.15226739 3.90457972 3.12294539 3.90368129 3.12294539 8.58899782 0 10.93210574z m25.76968965 26.55042555c-2.34220859 3.12294539-7.80916035 2.34220859-12.49447688-1.56237113-3.90457972-3.90368129-5.46605242-9.37063305-2.34220859-11.71284164 2.34220859-3.12384382 7.80826192-2.34310703 12.49447687 1.56147269 3.90368129 3.12384382 4.68531653 8.58989625 2.3422086 11.71374008z m35.1403227 14.83668637c-0.78163433 3.90457972-6.24768766 5.46605242-11.71374008 3.90457972-5.46605242-1.5614727-8.58899782-6.24768766-7.80916036-9.37063305 0.78163433-3.90457972 6.24768766-5.46605242 11.71374009-3.90457972 5.46605242 1.5614727 8.58899782 5.46605242 7.80916035 9.37063305z m38.26416562 3.12384382c0 3.90457972-4.68621496 7.02752512-10.15226738 7.02752512-5.46605242 0-10.15226738-3.12294539-10.15226739-7.02752512s4.68621496-7.02842356 10.15226739-7.02842445c5.46605242 0 10.15226738 3.12384382 10.15226738 7.02842445z m35.92016106-6.24768766c0.78163433 3.90457972-3.12384382 7.80916035-8.58899872 8.58989625-5.46695086 0.78163433-10.15226738-1.5614727-10.93390172-5.46605241-0.77983747-3.90457972 3.12384382-7.80916035 8.5907947-8.58899872 5.46605242-0.78163433 10.15136894 1.56057426 10.93210574 5.46515488z m0 0" fill="#663399"></path></symbol><symbol id="icon-bilibili" viewBox="0 0 1024 1024"><path d="M832.61667555 181.33447111h-164.32545185l74.45617778-74.45617778c12.84020148-12.84020148 12.84020148-30.8140563 0-43.65425778-12.84020148-12.84020148-30.8140563-12.84020148-43.65425778 0L573.2882963 189.04101925H450.04420741L324.2272237 63.23617185c-10.26730667-12.84020148-25.68040297-15.40096-41.08136295-7.70654815-2.57289482 0-2.57289482 2.57289482-5.13365334 5.13365333-12.84020148 12.84020148-12.84020148 30.8140563 0 43.65425779l77.02907259 77.02907259h-164.32545185c-89.86927408 0-164.32545185 74.45617778-164.32545185 164.32545184v408.24073483c0 87.29637925 74.45617778 161.75255703 164.32545185 161.75255703h25.68040296c0 30.8140563 25.68040297 53.92156445 53.92156444 53.92156444s53.92156445-25.68040297 53.92156445-53.92156444H704.23893333c2.57289482 30.8140563 28.24116148 53.92156445 59.05521778 51.34866964 28.24116148-2.57289482 48.78791111-23.10750815 51.34866964-51.34866964h20.53461333c89.86927408 0 164.32545185-74.45617778 164.32545184-164.32545186V343.09916445c-2.56075852-89.86927408-77.02907259-161.76469333-166.88621037-161.76469334z m-5.13365333 634.19429926H200.99527111c-33.37481482 0-59.05521778-28.24116148-61.61597629-61.61597629l-2.57289482-415.94728297c0-33.37481482 28.24116148-61.6159763 61.6159763-61.61597629h626.48775111c33.37481482 0 59.05521778 28.24116148 61.61597629 61.61597629l2.57289482 415.94728297c-2.57289482 35.93557333-28.24116148 61.6159763-61.6159763 61.61597629z" fill="#ff7299"></path><path d="M403.82919111 417.55534222l15.40096 77.0290726-205.40681481 38.50846815-15.40096-77.0290726 205.40681481-38.50846815z m197.70026667 77.0290726l15.40096-77.0290726 205.40681481 38.50846815-15.40096 77.0290726-205.40681481-38.50846815z m41.08136297 161.75255703c0 2.57289482 0 7.70654815-2.57289483 10.26730667-12.84020148 28.24116148-41.08136297 46.2150163-74.45617777 48.78791111-20.53461333 0-41.08136297-10.26730667-53.92156445-25.68040296-15.40096 15.40096-33.37481482 25.68040297-53.92156445 25.68040296-30.8140563-2.57289482-59.05521778-20.53461333-74.45617777-48.78791111 0-2.57289482-2.57289482-5.13365333-2.57289481-10.26730667 0-10.26730667 7.70654815-17.97385482 17.97385481-20.53461333h2.57289482c7.70654815 0 12.84020148 2.57289482 15.40096 10.26730666 0 0 20.53461333 28.24116148 38.50846815 28.24116149 35.94770963 0 35.94770963-30.8140563 56.48232296-53.92156445 23.10750815 25.68040297 23.10750815 53.92156445 56.48232296 53.92156445 23.10750815 0 38.50846815-28.24116148 38.50846815-28.24116149 2.57289482-5.13365333 10.26730667-10.26730667 15.40096-10.26730666 10.26730667-2.57289482 17.97385482 5.13365333 20.53461333 15.40096v5.13365333h0.0364089z" fill="#ff7299"></path></symbol></svg>',    o = (o = document.getElementsByTagName("script"))[o.length - 1].getAttribute("data-injectcss"),    p = function (c, l) {      l.parentNode.insertBefore(c, l);    };  if (o &amp;&amp; !c.__iconfont__svg__cssinject__) {    c.__iconfont__svg__cssinject__ = !0;    try {      document.write(        "<style>.svgfont {display: inline-block;width: 1em;height: 1em;fill: currentColor;vertical-align: -0.1em;font-size:16px;}</style>"      );    } catch (c) {      console &amp;&amp; console.log(c);    }  }  function d() {    i || ((i = !0), a());  }  function m() {    try {      t.documentElement.doScroll("left");    } catch (c) {      return void setTimeout(m, 50);    }    d();  }  (l = function () {    var c,      l = document.createElement("div");    (l.innerHTML = v),      (v = null),      (l = l.getElementsByTagName("svg")[0]) &amp;&amp;        (l.setAttribute("aria-hidden", "true"),        (l.style.position = "absolute"),        (l.style.width = 0),        (l.style.height = 0),        (l.style.overflow = "hidden"),        (l = l),        (c = document.body).firstChild ? p(l, c.firstChild) : c.appendChild(l));  }),    document.addEventListener      ? ~["complete", "loaded", "interactive"].indexOf(document.readyState)        ? setTimeout(l, 0)        : ((h = function () {            document.removeEventListener("DOMContentLoaded", h, !1), l();          }),          document.addEventListener("DOMContentLoaded", h, !1))      : document.attachEvent &amp;&amp;        ((a = l),        (t = c.document),        (i = !1),        m(),        (t.onreadystatechange = function () {          "complete" == t.readyState &amp;&amp; ((t.onreadystatechange = null), d());        }));})(window);]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>友情链接</title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>标签</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
