<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/posts/16107.html"/>
      <url>/posts/16107.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Unsupervised-Data-Augmentation</title>
      <link href="/posts/77eb0ac8.html"/>
      <url>/posts/77eb0ac8.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>Unsupervised Data Augmentation for Consistency Training</p></blockquote><p>会议：NeurIPS 2019</p><p>论文地址：<a href="https://arxiv.org/abs/1904.12848v2"> Unsupervised Data Augmentation for Consistency Training</a></p><p>Pytorch 复现：<a href="https://github.com/sndnyang/vat_pytorch">sndnyang/vat_pytorch</a></p><p>参考文章：</p><ul><li><a href="https://amitness.com/2020/07/semi-supervised-learning/">Semi-Supervised Learning in Computer Vision</a></li><li><a href="https://blog.csdn.net/by6671715/article/details/122706003">码侯烧酒的博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/369023559">Unsupervised Data Augmentation for Consistency Training </a></li></ul><p>参考视频：</p><ul><li><a href="https://www.bilibili.com/video/BV1Ab4y1S7i5/?spm_id_from=333.999.0.0&amp;vd_source=7801e0201bc8ddcb3d951c7388a08c3e">数最后一名</a></li></ul><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>最近的半监督学习方式都是基于一致性学习（consistency training）的，尽管成果不错，但是它们添加噪声的方式都比较初级，大多数都是基于 Gaussian noise， dropout noise 或者是 adversarial noise。因此，作者想引入一些更高级的数据增强方式，尤其是那些在监督学习中已经证明有效的方法，看看它们在半监督学习中是否也是有效的。</p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>少样本标注情况下，充分利用无标注数据，使其尽可能达到有充分标注的效果。</p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>根据一致性训练的原则，认为对一个样本进行较好的数据增强后，预测标签不应该发生变化，因此一般来说会有两个部分的 loss：（1）标注样本的损失，一般采用<code>交叉熵</code>；（2）未标注样本的损失，<code>KL 散度</code>等。</p><p>流程图如下所示，对有标签样本计算损失 L1，对无标签样本执行一次预测得到一个结果，再对改变本做一次特殊的数据增强后在进行预测得到一个结果，将两个结果计算一致性损失 L2，将 L1 与 L2 计算加权和。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519192319207.png" alt="image-20230519192319207"></p><p>数学形式说明该流程，输入 <code>x</code>，计算输入分布 <code>p_θ(y|x)</code> 和带噪声的分布 <code>p_θ(y|x, ɛ)</code>，然后最小化两个分布之间的距离：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519193105506.png" alt="image-20230519193105506"></p><p>上述图片对应的损失函数如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519193157993.png" alt="image-20230519193157993"></p><p>其中，<code>λ</code> 为平衡有监督学习和一致性训练之间的损失，<code>CE</code> 为交叉熵，<code>f*(.)</code> 为索要训练的最终目标模型。</p><p><u><strong>但有一点很奇怪</strong></u>，作者在之前的版本中计算一致性损失所采用的计量方式为 KL 散度，但是在最新版本又换回了 Cross-Entropy，不懂这是为什么…可能看效果而异吧。</p><p>不过有的人说：</p><blockquote><p>MSE 和 KL 散度各有优劣，二者的选取与数据集的实际分布特征关系很大，在实践中不妨进行对比测试</p></blockquote><p>在论文中，作者正对计算机视觉和自然语言任务提出了不同的对应增强方法，其中对于图片的增强方式，作者使用了一个增强方法 <u><strong>RandAugment</strong></u>，（只了解了大概，具体论文没看，好像是从强化学习中的 AutoAugment 衍生而来的）。不同任务的效果如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519193829228.png" alt="image-20230519193829228"></p><h2 id="训练技巧（Tricks）"><a href="#训练技巧（Tricks）" class="headerlink" title="训练技巧（Tricks）"></a>训练技巧（Tricks）</h2><h3 id="数据层面"><a href="#数据层面" class="headerlink" title="数据层面"></a>数据层面</h3><ol><li><p>基于置信度的掩码（Confidence-based masking）</p><p>对预测效果不好的样本（指针对一致性预测的原始样本），即置信度（最大的样本的概率）小于一定阈值数据，不计入一致性损失。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519194350155.png" alt="image-20230519194350155"></p></li><li><p>输出分布锐化（Sharpening Predictions）</p><p>降低预测结果的熵对训练有好处，所以作者对无标签的预测见过做<u><strong>锐化</strong></u>，采用的方法为 <u><strong>low Softmax temperature τ</strong></u>，与方法一相结合，在 batch 大小为 B 的情况下，损失为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519194803301.png" alt="image-20230519194803301"></p></li><li><p>域外相关数据的选择（Domain-relevance Data Filtering）</p><p>无标签数据量大，其中的数据分布往往是不均衡且有大量任务无关数据，为了解决这个问题，论文提出一种通用的无标签数据分布均衡化策略。首先利用有标签数据训练一个初始化模型，然后去预测所有无标签数据，根据置信度均衡选择各类别数据。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519194958974.png" alt="image-20230519194958974"></p></li></ol><h3 id="训练层面"><a href="#训练层面" class="headerlink" title="训练层面"></a>训练层面</h3><p>Training Signal Annealing（训练信号退火）</p><p>主要是针对标签数据与未标签数据不平衡时的场景，由于有大量的未标签数据需要UDA 处理，所以需要一个较大模型，但是由于较大模型很容易在少量标签数据下过拟合，所以，提出了本方法用于解决该问题。</p><p>基本原理就是在训练过程中，随着未标签数据的增加，逐步去除带标签数据，从而避免模型过拟合到带标签的训练数据。具体而言，就是在训练的 <code>t</code> 时刻，设置一个阈值 <code>ηt</code>，当 <code>1/K ≤ ηt ≤ 1</code>，其中，<code>K</code> 是类别数，当某个标签数据计算的 <code>p_θ(y∗| x)</code>大于阈值<code>ηt</code>，将该标签数据移除出计算损失的过程，而只计算 miniBatch 里面的其余数据。</p><p>具体策略有 3 种：</p><ul><li>log-schedule</li><li>linear-schedule</li><li>exp-schedule</li></ul><p>对于 labeled 数据量少，容易过拟合情况，选择最后一种；对于 labeled 数据量较多，过拟合不严重，可视情况选择前两种。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519195814680.png" alt="image-20230519195814680"></p><h2 id="理论方面的探讨（Theoretical-Analysis）"><a href="#理论方面的探讨（Theoretical-Analysis）" class="headerlink" title="理论方面的探讨（Theoretical Analysis）"></a>理论方面的探讨（Theoretical Analysis）</h2><blockquote><p>理论渣渣只能意会不可用文字整理出来，具体看论文吧…</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>给人的感觉这一篇好“工程”😂，在数据方面做了很多文章，但半监督领域又好像都是在对数据这一方面做很多“动作”，不管了，接着看，接着读。</p>]]></content>
      
      
      <categories>
          
          <category> 半监督学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Semi-Surprised-Learning </tag>
            
            <tag> Data Augmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Virtual Adversarial Training</title>
      <link href="/posts/cc87ae2e.html"/>
      <url>/posts/cc87ae2e.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>A Regularization Method for Supervised and Semi-Supervised Learning</p></blockquote><p>期刊：IEEE Transactions on Pattern Analysis and Machine Intelligence (2018)</p><p>论文地址：<a href="https://arxiv.org/abs/1704.03976"> Virtual Adversarial Training</a></p><p>Pytorch 复现：<a href="https://github.com/sndnyang/vat_pytorch">sndnyang/vat_pytorch</a></p><p>参考文章：</p><ul><li><a href="https://amitness.com/2020/07/semi-supervised-learning/">Semi-Supervised Learning in Computer Vision</a></li><li><a href="https://blog.csdn.net/by6671715/article/details/122723657"> 码侯烧酒的博客</a></li><li><a href="https://facico.blog.csdn.net/article/details/114634056?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-114634056-blog-119420855.235%5Ev36%5Epc_relevant_default_base&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-114634056-blog-119420855.235%5Ev36%5Epc_relevant_default_base&amp;utm_relevant_index=1">Facico的博客</a></li></ul><p>参考视频：</p><ul><li><a href="https://www.bilibili.com/video/BV1j64y1v7p4?p=1&amp;vd_source=7801e0201bc8ddcb3d951c7388a08c3e">数最后一名</a></li></ul><h2 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h2><p>论文的关键想法同样保持着<u><strong>一致性正则化</strong></u>的思想，只不过将图像的增强方式以生成<u><strong>对抗样本（Adversarial Example）</strong></u>来代替之，有标签的损失采用<u><strong>交叉熵（Cross-Entropy）</strong></u>计算，无标签的损失使用 <u><strong>KL 散度（KL Divergence）</strong></u>来计算，最后施以对应的权重得到最终的损失，大致流程图如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517202628300.png" alt="image-20230517202628300"></p><h2 id="一致性正则化"><a href="#一致性正则化" class="headerlink" title="一致性正则化"></a>一致性正则化</h2><p>具体来说，基于平滑假设和聚类假设，具有不同标签的数据点在低密度区域分离，并且相似的数据点具有相似的输出。如果对一个未标记的数据应用实际的扰动，其预测结果不应该发生显著变化，也就是输出具有一致性。其数学表达如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517203120942.png" alt="image-20230517203120942"></p><p>其中，<code>D</code> 为度量函数，一般采用 <code>KL 散度</code> 或者 <code>JS 散度</code>，当然也可以使用交叉熵或者平方误差等，<code>Augment(·)</code> 是指数据增强函数，当采用数据增强是，视为对样本/模型添加一些噪声扰动，<code>θ</code> 为模型参数。</p><p>常见的数据增强有以下：</p><ul><li>常规的数据增强：平移旋转，随机 dropout等</li><li>时序移动平均：Temporal Ensembling，Mean-Teacher 中使用的方法</li><li>对抗样本扰动：VAT</li><li>高级数据增强：<a href="https://arxiv.org/abs/1904.12848v2">UDA</a></li></ul><h2 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h2><p>对抗训练（Adversarial Training）是增强神经网络鲁棒性的重要方式，在对抗训练的过程中，样本会被混合一些微小的扰动（哪怕改变很小，但很有可能会造成错误分类），使神经网络适应这种改变，从而对对抗样本具有鲁棒性。</p><p>生成对抗样本有以下几种常见方式：</p><ul><li><p>基于梯度法: <a href="https://arxiv.org/abs/1412.6572">Explaining and Harnessing Adversarial Examples</a></p></li><li><p>基于超平面分类: <a href="https://arxiv.org/abs/1511.04599">DeepFool: a simple and accurate method to fool deep neural networks</a></p></li><li><p>对抗攻击（Adversarial Attack）：在模型原始输入上添加对抗扰动</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/2020101817001880.png" alt="img"></p></li></ul><p>Goodfellow 对对抗训练损失函数定义如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517204302784.png" alt="image-20230517204302784"></p><p>其中，<code>D</code> 是衡量两个分布相似度的函数，<code>q(y|xl)</code> 是样本的真实分布，<code>p(y|xl, θ)</code> 是由参数和 <code>xl</code> 生成的预测分布，通过增加扰动 <code>radv</code> 来使得两个分布尽量相似。</p><p>对抗为什么可行？</p><ul><li>因为很多网络被设计得十分“线性”，像 LSTM 这样的，对x的每个维度都做微小扰动，当x的维度变大的时候，会对网络造成较大的影响</li><li>网络的线性，使得高阶导近似0，Taylor 展开后占主导的是线性的部分，所以用来干扰的主要就是对抗样本中线性的部分</li></ul><p>通常，我们无法获得精确对抗性扰动的<u><strong>闭式解</strong></u>，不过可以通过上式中的度量 D 来线性近似 r，使用 <code>L2</code> 正则是，对抗扰动可以通过下面的式子近似：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517205056447.png" alt="image-20230517205056447"></p><p>使用 <code>L∞</code> 正则时，可通过下面的式子近似：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517205330497.png" alt="image-20230517205330497"></p><p>其中 <code>g</code> 为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517205353604.png" alt="image-20230517205353604"></p><p>可通过<u><strong>反向传播</strong></u>进行计算，对抗方法得到的扰动方向，比随机找一个扰动更好。</p><p>对抗训练存在的缺点：仅仅只能适配于有监督学习，当样本没有标签就不能进行，故作者提出了一种可以运用于<u><strong>半监督学习</strong></u>的对抗训练方法。</p><h2 id="Virtual-Adversarial-Training-VAT"><a href="#Virtual-Adversarial-Training-VAT" class="headerlink" title="Virtual Adversarial Training, VAT"></a>Virtual Adversarial Training, VAT</h2><p>Adversarial Direction，对抗性方向，其概念是指能够最大程度减少准确分类的概率的方向，如下图所示，寻找扰动项 <code>radv</code>，其投影点与准确分类的方向为对抗性方向。</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/1684329811138.png" alt="1684329811138" style="zoom: 50%;"><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517212606646.png" alt="image-20230517212606646"></p><p>local distributional smoothness（LDS），定义为衡量当前基于每个输入数据的模型的平滑度的负函数。</p><h3 id="推导流程"><a href="#推导流程" class="headerlink" title="推导流程"></a>推导流程</h3><blockquote><p>由于不知道为什么公式渲染不出来，我就不手打公式了，等有时间再去把这个问题弄了，我就直接套用现成已有的公式，或是直接手写出来，Sorry…</p></blockquote><p>文中的符号定义如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518145424158.png" alt="image-20230518145424158"></p><p>在上面，我们提到，<u><strong>对抗训练</strong></u>只适用于带标签的有监督问题，在半监督学习中并不是很适用，所以，作者就提出了<u><strong>虚拟对抗训练</strong></u>，<code>VAT</code> 的损失函数如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518145654097.png" alt="image-20230518145654097"></p><p>其中，<code>x⁎</code>可以表示为有标签或者无标签数据，正如上文中所提到的，在实际中，并没有关于 <code>q(y, xul)</code> 的直接信息，因此可以使用<code>p(y|x, θ)</code>来替换之，如果带标签的样本比较多时，<code>p</code>就会逼近<code>q</code>，利用<code>p</code>生成的虚拟标签代替不知道的标签，并根据虚拟标签计算对抗方向，此时的虚拟标签是用上一步训练之后得到的模型进行估计，故损失函数更新如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518150409362.png" alt="image-20230518150409362"></p><p>将损失函数求平均，得到正则项（<strong>regularization term</strong>）：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518150651440.png" alt="image-20230518150651440"></p><p>完整的目标函数为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518150735667.png" alt="image-20230518150735667"></p><p>其中第一项为<u><strong>带标签数据的负对数似然函数</strong></u>（针对有标签数据），VAT 的一个优点是：只有两个超参数：（1）对抗方向的限制参数（norm constraint）<code>ɛ &gt; 0</code>；（2）控制目标函数的两个目标项的相对平衡的正则化系数（regularization coefficient）<code>α &gt; 0</code>，事实上，作者指出，两个超参数的作用大抵相同，于是 VAT 中只微调了超参数 <code>ɛ</code>，而将 <code>α</code> 固定为 1。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518151554425.png" alt="image-20230518151554425"></p><p>对于上述目标函数，观察到，当 r = 0 时，D 永远为 0，所以需要对 D 进行二阶泰勒展开，这里的 D 为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518151854028.png" alt="image-20230518151854028"></p><p>二阶展开后得到：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518151914984.png" alt="image-20230518151914984"></p><p>其中 H 为海森矩阵（Hessian Matrix），具体如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518152200457.png" alt="image-20230518152200457"></p><p>故 <code>rvadv</code> 可以近似为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518152332268.png" alt="image-20230518152332268"></p><p>其中：u 是 H 的第一特征主向量，上划线表示同方向单位向量（算了我写出来.）</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/1684394786562.png" alt="1684394786562"></p><p>在计算 H 的特征值/特征向量时，时间复杂度需要 <code>O(I³)</code>，论文中提到使用幂迭代法（ power iteration method）和有限差分法（finite difference method）来近似求解，假设 <code>d</code> 为与特征向量不垂直的一个随机单位向量，迭代计算如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153132095.png" alt="image-20230518153132095"></p><p>注意到 <code>Hd</code> 可以被有限差分来近似计算：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153216356.png" alt="image-20230518153216356"></p><p>由于 D 的一介导还是很可能为 0，所以 r 用如下方式迭代更新：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153419621.png" alt="image-20230518153419621"></p><p><u><strong>迭代的次数越多效果越好</strong></u>，甚至很多数据集上迭代次数为 1 都取得了较好的效果，迭代次数如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153804154.png" alt="image-20230518153804154"></p><p>伪代码如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153918543.png" alt="image-20230518153918543"></p><p>效果如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518154205948.png" alt="image-20230518154205948"></p><p>迭代次数较少的情况下，有大量的<u><strong>未标记点（灰色点）</strong></u>会有较高的<u><strong>LDS（深蓝色）</strong></u>，即不平滑，因为模型一开始对相同类别的 point 预测了不同的 label，VAT 会让 LDS 较高的数据点更大的压力，使得数据点间边界更平滑。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>文章的亮点就是对于二阶导以及之后的各种近似优化，理论方面很充足，实验也很充足，理论方面对于我这个渣硕来说真的挺艰难，我在去年 12 月份就准备看这一篇，结果因为理论被劝退，终于时隔数月，我算是啃下来了…不过也还是云里雾里的，还是得补充一下数学理论知识啊，果然我导说的没错，理论知识都不懂，给我发论文看我都看不懂，那还搞啥呢，哈哈🤦‍♂️，就这样子，如果有机会再复盘这篇论文，有问题在更新。</p><p>最后附上我得手写版公式（惨不忍睹的被公式薄纱了）：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518155255116.png" alt="image-20230518155255116"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518155319698.png" alt="image-20230518155319698"></p>]]></content>
      
      
      <categories>
          
          <category> 半监督学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Semi-Surprised-Learning </tag>
            
            <tag> Adversarial Example </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于深度学习中遇到的知识盲区</title>
      <link href="/posts/eaa3f406.html"/>
      <url>/posts/eaa3f406.html</url>
      
        <content type="html"><![CDATA[<h1 id="关于深度学习的一些问题与知识盲区"><a href="#关于深度学习的一些问题与知识盲区" class="headerlink" title="关于深度学习的一些问题与知识盲区"></a>关于深度学习的一些问题与知识盲区</h1><p>此篇用于记录在阅读论文以及博客时遇到的一些问题和知识盲区，方便日后进行复盘。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="为什么需要激活函数？"><a href="#为什么需要激活函数？" class="headerlink" title="为什么需要激活函数？"></a><strong>为什么需要激活函数？</strong></h3><p>通常激活函数都是非线性的，它能够帮助我们引入非线性因素，使得神经网络能够更好地解决更加复杂地问题，在简单的二分类问题中，如果不使用激活函数，使用简单的**<u>逻辑回归</u>**，那么该模型只能作简单的线性分类，而不能作复杂的非线性划分，如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511195806300.png" alt="image-20230511195806300"></p><p>值得一提的是，如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，效果与单个神经元无异。另外，如果是拟合问题而不是分类问题，输出层的激活函数可以使用线性函数。</p><h3 id="Sigmoid-函数饱和带来的问题"><a href="#Sigmoid-函数饱和带来的问题" class="headerlink" title="Sigmoid 函数饱和带来的问题"></a>Sigmoid 函数饱和带来的问题</h3><p>Sigmoid 函数的取值范围在 (0,1) 之间，单调连续，求导容易，一般用于二分类神经网络的输出层。</p><p>Sigmoid 函数饱和区范围广，容易造成梯度消失。饱和区如下图所示，图中红色椭圆标注的饱和区曲线平缓，梯度的值很小，近似为零，且 Sigmoid 函数的饱和区范围很广，除了 [-5,5]，其余区域都可以认为是饱和区，这种情况很容易造成梯度消失，梯度消失会增大神经网络训练难度，影响神经网络模型的性能。</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511200333796.png" alt="image-20230511200333796" style="zoom:50%;"><h2 id="降噪自编码器-Denoising-Auto-Encoder"><a href="#降噪自编码器-Denoising-Auto-Encoder" class="headerlink" title="降噪自编码器 Denoising Auto-Encoder"></a>降噪自编码器 Denoising Auto-Encoder</h2><p>在神经网络模型训练阶段开始前，通过 Auto-encoder 对模型进行预训练可确定编码器 W 的初始参数值。然而，受模型复杂度、训练集数据量以及数据噪音等问题的影响，通过 Auto-encoder 得到的初始模型往往存在过拟合的风险。</p><p>简单理解，在人类的感知过程中，某些模态的信息对结果的判断影响并不大。举个例子，一块圆形的饼干和一块方形的饼干，在认知中同属于饼干这一类，因此形状对我们判断是否是饼干没有太大作用，也就是<strong>噪声</strong>。如果不能将形状数据去除掉，可能会产生“圆饼干是饼干，方饼干就不是饼干”的问题（过拟合）。</p><p>当采用无监督的方法分层预训练深度网络的权值时，为了学习到较鲁棒的特征，可以**<u>在网络的可视层（即数据的输入层）引入随机噪声</u>**，这种方法称为降噪自编码器（<a href="https://dl.acm.org/doi/abs/10.1145/1390156.1390294">Denoising Auto-Encoder， DAE</a>）。</p><blockquote><p>降噪自编码器：一个模型，能够从有噪音的原始数据作为输入，而能够恢复出真正的原始数据。这样的模型，更具有鲁棒性。</p></blockquote><p>以下是以经典的 MNIST 手写数字识别为例，对于输入的数据引入了变换角度、随机噪点、添加背景图像等噪音。模型通过训练后可以对有噪音图像更加鲁棒，而这也更符合实际使用的需求。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-2b0c0b5855f0c98b44fbeaf75d6a72dd_r.jpg" alt="img"></p><p>对于有噪音的输入数据，区别于一般自编码机，降噪自编码机要做的就是数据的降噪。关于降噪的过程如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-9ab89078a4630eb52347244a61410c2f_r.jpg" alt="img"></p><p>对于输入层 $x$，以一定概率将其节点置 0，得到 $\hat{x}$,用 $\hat{x}$ 去计算 $y$,计算 $z$ ，并将 $z$ 与原始 $x$ 做误差迭代，对结果误差较小的节点可以认为是噪声。每层处理重复上述工作。</p><p>自编码器的本质是学习一个相等函数，即网络的输入和重构后的输出相等，这种相等函数的表示有个缺点就是当测试样本和训练样本不符合同一分布，效果不好，而降噪自编码器在这方面的处理有所进步。</p><h2 id="随机深度-Stochastic-Depth"><a href="#随机深度-Stochastic-Depth" class="headerlink" title="随机深度 Stochastic Depth"></a>随机深度 Stochastic Depth</h2><p>针对于残差模块的优化，由于很深的 ResNet 通常需要很长时间的训练(也就是训练很慢)，作者引入了一种类似于 dropout 的方法，在训练过程中随机丢弃子图层（randomly drop a subset of layers），而在推断时正常使用完整的图。</p><p>ResNet 网络是由一个接一个的残差模块(ResBlock)串联起来的，可以视为ResBlock的集合。在训练时，对每个 ResBlock 随机 drop（按伯努利分布），drop 就是将上一个 ResBlock 直接输出到下一个 ResBlock，被 drop 的 ResBlock 什么都不做也不更新。另外，网络的输入被视为第一层，是不会 drop 的。</p><p>与 Dropout 的不同之处在于，该方法 drop 整个 ResBlock，而 Dropout 在训练期间只 drop 一部分神经元节点。这种方法大大降低了训练时间，甚至在训练完成后删除部分layer，还能不影响精度。</p><h2 id="最小化信息熵-Entropy-Minimization"><a href="#最小化信息熵-Entropy-Minimization" class="headerlink" title="最小化信息熵 Entropy Minimization"></a>最小化信息熵 Entropy Minimization</h2><p>参考文章：<a href="https://kexue.fm/archives/5448">最小熵原理（一）</a></p><p>在半监督学习中，有标签（分类完全准确）的数据样本通常相对较少，通过训练模型对未标记数据样本进行预测，选择出高置信度的样本，作为标记样本同有标签样本作为下一次训练的数据样本。</p><p><u><strong>Entropy Minimization</strong></u> 是一种在半监督学习中使用的技术，它的目的是最小化信息熵，从而使模型在分类时的不确定性最小。在半监督学习中，我们希望模型尽可能地利用未标记数据来学习，但是这些数据并不带有正确的标签，因此我们需要利用某些技术来帮助我们学习这些数据。</p><p><u><strong>加快模型学习进度的唯一方法就是降低学习目标的冗余信息量</strong></u>，所提到的“去冗余”，可以理解为“省去没必要的学习成本”。</p><p>也就是通常所使用到的技巧：过滤掉低置信度的未标记样本，保留高置信度样本。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512164646319.png" alt="image-20230512164646319"></p><h2 id="Top-1-and-Top-5-Accuracy"><a href="#Top-1-and-Top-5-Accuracy" class="headerlink" title="Top-1 and Top-5 Accuracy"></a>Top-1 and Top-5 Accuracy</h2><p>Top-1：</p><p>在多分类问题中，一般认为在经过全连接层后得到的概率分布中概率最大的类别为我们模型的预测类别，就判断为正确。</p><p>Top-5：</p><p>在多分类问题中，一般认为在经过全连接层后得到的概率分布中概率最大的全五个类别中有我们模型的预测类别，就判断为正确。</p><h2 id="Dark-Knowledge"><a href="#Dark-Knowledge" class="headerlink" title="Dark Knowledge"></a>Dark Knowledge</h2><p>可以看作是经过 Softmax 函数后得到的各类类别的可能性，其包含着类别之间的相关性，比如，猫和狗的相似性，要远远大于猫和船的相似性，而这种相似性，会在概率值中有所体现，而这部分信息一致没有被很好的利用，所以称之为 Dark Knowledge。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>test-blog</title>
      <link href="/posts/467ec823.html"/>
      <url>/posts/467ec823.html</url>
      
        <content type="html"><![CDATA[<h1 id="这是一篇测试博客"><a href="#这是一篇测试博客" class="headerlink" title="这是一篇测试博客"></a>这是一篇测试博客</h1><p>修改一下在上传试试…<br>在测试测试</p>]]></content>
      
      
      
        <tags>
            
            <tag> test </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>半监督学习学习过程</title>
      <link href="/posts/52ee1a8c.html"/>
      <url>/posts/52ee1a8c.html</url>
      
        <content type="html"><![CDATA[<h1 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h1><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>学习了快一个学期的 Java 程序开发，让我感觉开发的过程真的好空洞…但是我并不知道下一步到底要做什么，又处于想找实习又怕找不到的情况下，刷了几天 LeetCode，然后又变成了“小🐏人”，紧接着被隔离😅，真的挺痛苦，所幸在第三天就“出狱”了，在蹲“监狱”期间，受到一个“狱友”的启发，“还是要做好职业规划的，若是没有做好职业规划，像一只无头苍蝇一样，很可能在毕业之际还是找不到工作。”在思考了一天后，我决定重新开始在“半监督学习”这个领域开始零基础学习了，为什么会有这个想法？是因为我觉得我起初考研的目的是为了能够在人工智能这个领域继续深入了解，目的很单纯，那为什么我不坚持一下呢？于是我决定重新开始这个计划，管他呢，起码是自己喜欢做的事情，哪怕未来找不到工作什么的，起码现在是由试错的资本的。</p><p>以下将会记录我在学习时候遇到的问题/解决方式/新思路/感想。</p><h2 id="Self-Training"><a href="#Self-Training" class="headerlink" title="Self-Training"></a>Self-Training</h2><p>自训练方法，模型基于已标记好的训练集进行训练，得到一个基础模型，利用该基础模型对未标记的数据集进行预测一个<strong>伪标签</strong>，然后将两个数据集整合训练，得到一个新的模型，从而迭代更新模型参数，生成一个最优模型。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161304502.png" alt="image-20230511161304502"></p><h3 id="Pseudo-Label"><a href="#Pseudo-Label" class="headerlink" title="Pseudo-Label"></a><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.664.3543&amp;rep=rep1&amp;type=pdf">Pseudo-Label</a></h3><p>伪标签技术适用于小样本学习，实际上在样本极其珍贵的金融、医疗图像、安全等领域，伪标签学习有时候很有效。</p><p>伪标签的定义来自于半监督学习，半监督学习的核心思想是<u><strong>通过借助无标签的数据来提升有监督过程中的模型性能</strong></u>。</p><p>粗略来讲，伪标签技术就是<u><strong>利用在已标注数据所训练的模型在未标注的数据上进行预测，根据预测结果对样本进行筛选，再次输入模型中进行训练的一个过程</strong></u>。</p><p>如下图所示，利用有标签的数据集训练出一个模型，运用训练出的模型给予无标签数据一个<u><strong>伪标签</strong></u>。如何定义所属类别？利用训练好的模型对无标签数据进行预测，以概率最高的类别作为无标签数据的伪标签。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161425308.png" alt="image-20230511161425308"></p><blockquote><p>entropy regularization：用于防止模型过拟合，通过在损失函数中加入熵（entropy）项来实现</p></blockquote><p>利用 <em>entropy regularization</em> 思想，将无监督数据转为目标数据的正则项，即将拥有伪标签的无标签数据视为有标签的数据，利用交叉熵（与最初训练模型一致）来评估误差大小。</p><p>模型整体的目标函数如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-103a30551a86aa99222f8f35129088d7_r.jpg" alt="img"></p><p>其中左边一项为交叉熵，用来评估有标签数据的误差，右边一项即为 <em>entropy regularization</em> 项，用来从无标签的数据中获取训练信号。</p><p>为了平衡有标签数据和无标签数据的信号强度，如上所示，算法在目标函数中引入了时变参数 α(t)，其数学形式如下，其中 T1 和 T2 都为超参数：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-4d5a8a1d8aa4b9037859ab19dd346c0b_r.jpg" alt="img"></p><p>因此，随着训练时间的增加，α(t) 将会从零开始线性增长至某个饱和值，对应无标签数据的信号也将逐渐释放出来。背后的核心想法也很直观，早期模型预测效果不佳，因此 <em>entropy regularization</em> 产生信号的误差也较大，因而 α(t) 应该从零开始，由小逐渐增大。</p><p><strong>存在不足：</strong>只在训练时间这个维度上，采用了退火思想，即采用时变系数α(t)。而在伪标签这个维度，对于模型给予的预测标签一视同仁，这种方法在实际中存在明显问题。若模型在对伪标签的数据预测后， 10 个类别预测概率值都接近于 0.1，以最大概率这一原则选择对应的标签，那么这个标签对模型的训练会造成一定的<u><strong>副作用</strong></u>。</p><p><strong>设想如何突破这一不足？</strong></p><p>也许可以设定一个阈值，抛弃那些预测最大概率值小于该阈值的未标记样本，将满足条件的未标记样本分配伪标签，并加入模型评估当中，之后再迭代训练。</p><h3 id="Noisy-Student"><a href="#Noisy-Student" class="headerlink" title="Noisy Student"></a><a href="https://arxiv.org/abs/1911.04252">Noisy Student</a></h3><p>论文的关键 idea 是训练两个模型，“teacher”和“student”，强调的是在student模型中加入噪声，teacher 模型和 student 模型可以用不同的模型训练，也可以使用相同的模型。</p><p>在有标签数据中训练“teacher”模型，并利用该模型对未标记数据进行推断伪标签，这些伪标签可以是<u><strong>软标签</strong></u>，也可以取其最大概率的类别将其转换为<u><strong>硬标签</strong></u>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161507462.png" alt="image-20230511161507462"></p><p>然后将标记数据与为标记数据（带有伪标签）置入“student”进行训练，在训练之前<u><strong>数据增强</strong></u>使用 <strong>RandAugment</strong>，待“student”模型训练好后，使用最新的模型作为新的“teacher”，进行下一次迭代，此过程会重复几次（通常为 3 次）。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511211613877.png" alt="image-20230511211613877"></p><p>总结该篇论文的流程思路：</p><ol><li>首先将在 ImageNet 上训练好的模型作为 Teacher Network 来训练 Student Network</li><li>再使用训练好的 T 网络（无噪音）来对另一个数据集 [JFT dataset] 生成尽可能准确的伪标签</li><li>之后使用生成伪标签的数据集 [JFT dataset] 和 ImageNet 一起训练 Student Network</li><li>Student Network中增加了模型噪音<ul><li>Dropout</li><li>随机深度  Stochastic Depth</li><li>数据噪音：对图片进行数据增广（RandAugment）</li></ul></li></ol><p>对 Student 模型添加噪音的作用：</p><ol><li><strong>数据噪音</strong>：提高泛化能力</li><li><strong>模型噪音</strong>：提高模型鲁棒性和泛化能力</li></ol><p>具体参数设置：</p><ol><li><strong>Stochastic Depth</strong>：幸存概率因子为 0.8</li><li><strong>Dropout</strong>：分类层（final layer）引入 0.5 的丢弃率</li><li><strong>RandAugment</strong>：应用两个随机计算，其震级设置为 27</li></ol><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512204307583.png" alt="image-20230512204307583"></p><p>其他 Tricks：</p><ol><li><strong>数据过滤</strong>：将教师模型中置信度不高的图片过滤，因为这通常代表着域外图像（out-of-domain data）</li><li><strong>数据平衡</strong>：平衡不同类别的图片数量，当一个类别所对应的图片数量不是很充足时，会采取<u>随机复制</u>的方法来扩充样本量</li><li><strong>软标签（Soft Pseudo-Label）</strong>：在消融实验中表示，软标签对域外图像有更强的指导作用</li></ol><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p>1.噪音是否对模型有影响？（The Importance of Noise in Self-training）</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512205434466.png" alt="image-20230512205434466" style="zoom:80%;"><p>从结果可以看出，<u>噪音</u>、<u>随机深度</u>、<u>数据扩充</u>起着重要的作用使学生模型胜过教师模型，对此有人提出是不是对未标记数据加入正则项以防止过拟合来代替噪音，作者在实验中说明这是不对的。因为在去噪的情况下，未标记图像的训练损失并没有下降多少以此说明模型并没有对未标记数据过拟合。</p><p>2.对于迭代训练的消融实验（A Study of Iterative Training）</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512205650571.png" alt="image-20230512205650571" style="zoom:80%;"><p>作者先在<u><strong>标记数据</strong></u>上训练了 EfficientNet-B7 作为 Teacher，然后再训练 EfficientNet-L2 作为 Student，然后让 Student 作为 Teacher 依次迭代三轮，作者表明，迭代训练提高了准确度，并且，给出再最后通过调整未标记图像和标记图像的比为 <strong>1 : 28</strong> 时达到最优 Top-1 Acc.。</p><p>3.能力强的教师模型会不会对学生模型造成的影响</p><p>4.无标签的数据量大小</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512210409208.png" alt="image-20230512210409208" style="zoom:80%;"><p>作者按照比例分别从整个数据集中<u><strong>均匀采样（uniformly sampling）</strong></u>，会发现在数据量减少至 1/16 中，模型的性能大都相似，在数据量达到 1/32 或更小后，模型性能有了显著的下降（可能 .3 个点就可以算是显著的下降了吧…），所以，使用大量未标记的数据会产生更好的性能，但作者指出：<u>对于大模型来说，数据量越多越好，而小模型由于容量限制则很容易饱和</u>。</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512210908139.png" alt="image-20230512210908139" style="zoom:80%;"><p>5.硬标签和软标签对域外图像的影响</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512210946955.png" alt="image-20230512210946955" style="zoom:80%;"><p>作者将预测置信度高（high-confidence）的图像视作域内图像（in-domain images），反之，将预测置信度低（low-confidence）的图像视作域外图像（out-of-domain images），作者表明：对于域内图像，软伪标签（ soft pseudo labels）和硬伪标签（hard pseudo labels）都对模型有一定的帮助；而对于域外图像，软伪标签使得模型对域外图像的判断有着一定的帮助，而硬标签则会对模型的精度有一定的损害。</p><p>剩下的<u><strong>消融实验</strong></u>就不写了，有点过于冗长了。</p><h2 id="Consistency-Regularization"><a href="#Consistency-Regularization" class="headerlink" title="Consistency Regularization"></a>Consistency Regularization</h2><p>说到一致（Consistency），其实很多代价都有这个内涵，如 MSE 代价，最小化预测与标签的差值，也就是希望预测与标签能够一致。其他的代价，如 KL 散度、交叉熵代价也类似。</p><p>所以一致性，是一种非常内在而本质的目标，可以让深度网络进行有效学习。 </p><p>在半监督领域中，未标记数据没有标签，所以需要让模型有个参照，从而通过这个参照从未标记数据中学习。</p><p><strong>Consistency Regularization</strong> 的主要思想是：相同的一张图片，通过模型所预测出来的结果应该是一样的。对于未标记样本，虽然没有对应的标签来使得模型判断预测结果是否准确，但是，却可以在原有的样本中添加一定的噪声，让模型通过比对预测结果来进行学习。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515151137355.png" alt="image-20230515151137355"></p><h3 id="π-model-and-Temporal-Ensembling"><a href="#π-model-and-Temporal-Ensembling" class="headerlink" title="π-model and Temporal Ensembling"></a>π-model and Temporal Ensembling</h3><p>论文地址：<a href="https://arxiv.org/abs/1610.02242">Temporal Ensembling for Semi-Supervised Learning</a> </p><h4 id="π-model"><a href="#π-model" class="headerlink" title="π-model"></a>π-model</h4><p>如下图所示，一个有标签样本（可视作无标签样本），经过随机图像增强输入网络，同时网络也会进行 Dropout 也可以视作噪声，输入两次得到两个结果 Z 和 Z’，将 Z 与图片真正标签 y 进行比对，使用 <code>交叉熵（cross-entropy）</code>计算损失 L1；将 Z 与 Z’ 使用 <code>平方差（squared difference）</code>计算损失 L2，然后将两个过程所得到的损失相加，但需要注意的是，两个损失分别占有一定权重，且占有的权重值会随着训练时间改变，在图 2 的所标记的损失函数公式可以看出。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515154058143.png" alt="image-20230515154058143"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515160459380.png" alt="image-20230515160459380"></p><p>其中 C 表示标记样本的的全部类别数量，w(t) 是随时间变化的加权函数，</p><p>并且，作者指出，第一部分的损失只针对有标签数据进行计算，而第二部分的损失则针对所有数据进行计算，即在算损失的时候，有标签数据两个损失项都用，无标签数据只用第二个损失项：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515161927609.png" alt="image-20230515161927609"></p><p>作者指出，使用 w(t) 时，由于前期模型差不多被有监督损失部分（标记数据样本）所支配，所以，在增长的前期，使权重增长的尽可能慢一点。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515163443077.png" alt="image-20230515163443077"></p><p>总结流程就是，数据集：有标签/无标签图片，分别两次输入模型，其中，L1只在有标签数据参与训练的过程中用于计算损失，L2 则是都有参与计算损失（有标签和无标签）。模型的噪音来自于：输入图像的随机增强，以及模型训练过程中的 Dropout。计算损失公式：L1 使用<u><strong>交叉熵</strong></u>来计算损失，L2 使用<u><strong>平方差公式</strong></u>来计算损失。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516131038965.png" alt="image-20230516131038965"></p><h4 id="Temporal-Ensembling"><a href="#Temporal-Ensembling" class="headerlink" title="Temporal Ensembling"></a>Temporal Ensembling</h4><p>由于以这种方式获得的培训目标是基于对网络的单一评估，因此可以预期它们会很嘈杂。暂时性结合通过将多个先前网络评估的预测汇总为整体预测来减轻这种情况</p><p>在 π-model 中，模型的训练结果都只是对网络进行单一评估，预测结果没有很强的关联性，这样子就造成了模型变得很“嘈杂”，并且，直觉认为，很难不相信过去预测结果与现在的预测结果不存在联系，这就是 π-model 的缺点。</p><p>在 Temporal Ensembling 中就很好的解决了这一个问题，在计算第二部分的损失中，生成的 Z’ 要参与下一个 epoch 的损失计算，（注意是每一个 epoch，而不是每一个 batch，这种改变其实是非常缓慢）。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515154108557.png" alt="image-20230515154108557"></p><p>每训练完一个 epoch 后，就会将 Z 进行更新：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515165329913.png" alt="image-20230515165329913"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515154426776.png" alt="image-20230515154426776"></p><p>总结流程，不同于 π-model，Temporal Ensembling 只进行一次模型预测，π-model 中的第二次训练预测，前一个 epoch 保存下来的 Z 与当前 epoch 的 z 进行指数滑动平均（Exponential Moving Average，EMA）运算得到，其中 z’ 是当前 epoch 的模型预测，参与 L2 的损失计算。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516132020990.png" alt="image-20230516132020990"></p><h3 id="Mean-Teacher"><a href="#Mean-Teacher" class="headerlink" title="Mean Teacher"></a><a href="https://arxiv.org/abs/1703.01780">Mean Teacher</a></h3><p>论文的关键思想是使用两个模型一个叫做<code>Student</code>，另一个叫做<code>Teacher</code>，其中Student 模型是一个带有 <u><strong>Dropout</strong></u> 的标准网络模型，与上文中所提到的 <u><strong>Noise Student</strong></u> 不同的是，该篇论文中的 Teacher 模型的架构与 Student 模型一致，且更新参数时是由 Student 模型的参数作<u><strong>指数滑动平均</strong></u>计算得到。计算损失的方式与 <u><strong>Temporal Ensembling</strong></u>相似/一致，有标签数据使用两个损失项，而无标签数据使用 Student 和 Teacher 模型得到的结果计算一致性损失，最后模型的总损失是由两个部分的损失得到。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516162131724.png" alt="image-20230516162131724"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516165459330.png" alt="image-20230516165459330"></p><p>使用指数滑动平均来更新参数后，模型可以在每一个 step 而不是在每一个 epoch来聚合信息，这大大提高了推断效率。并且，模型的每一层输出都得到了提升，而不是仅仅是在最后一层输出结果上得到改善，所以运用指数滑动平均，使得目标模型有了更好的<code>中间表示（intermediate representations）</code>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516170114727.png" alt="image-20230516170114727"></p><p>Teacher 模型参数 θ 的更新公式如下所示，在论文中将模型参数 θ 视作<u><strong>常量</strong></u>，不参与模型训练更新，而是在每一个 step 中按照公式进行迭代更新。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516183523851.png" alt="image-20230516183523851"></p><p>使用三种类型的噪声：</p><ul><li><p>对输入图像输入图像作随机平移和水平翻转</p><p><code>Translation Randomly {∆x, ∆y} ∼ [−2, 2]</code></p><p><code>Horizontal flip Randomly p = 0.5</code></p></li><li><p>Gaussian noise on the input layer</p><p><code>Gaussian noise σ = 0.15</code></p></li><li><p>Dropout</p><p><code>Dropout p = 0.5</code></p></li></ul><p>并且，计算一致性损失（ consistency cost）与上述所提到的模型不同，使用的是<code>均方误差(mean squared error)</code>，</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516185627093.png" alt="image-20230516185627093"></p><p>所使用的初始网络框架为：</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516195214426.png" alt="image-20230516195214426" style="zoom: 50%;"><h3 id="Virtual-Adversarial-Training"><a href="#Virtual-Adversarial-Training" class="headerlink" title="Virtual Adversarial Training"></a><a href="https://arxiv.org/abs/1704.03976">Virtual Adversarial Training</a></h3><p>由于有相关公式推导，且篇幅可能会很长（这篇论文理论部分好多…），故单独放在另一篇<a href="https://ytz7.github.io/posts/cc87ae2e.html">博客</a>中。</p><h3 id="Unsupervised-Data-Augmentation"><a href="#Unsupervised-Data-Augmentation" class="headerlink" title="Unsupervised Data Augmentation"></a><a href="https://arxiv.org/abs/1904.12848">Unsupervised Data Augmentation</a></h3><p>该篇论文的关键 idea 是使用 <code>AutoAugment</code> 创建一个未标记的图像的增强版本，使用最小化无标签数据增广数据和无标签数据的 KL 散度，如下图所示，具体思想和半监督学习中其他论文所使用的方法很相似，新鲜点就是论文的增强方式，对待不同任务使用不同的增强方式，且效果都还不错。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518191734446.png" alt="image-20230518191734446"></p><p>具体细节可查阅<a href="https://ytz7.github.io/posts/77eb0ac8.html">博客</a>。</p><blockquote><p>520 这天还在工位，真实咸鱼一条了 (#｀-_ゝ-) ，但今天见证历史了额，居然不看题解 LeetCode AC 了一道困难题，哈哈哈😊，然后就奖励了自己看一下午的视频（堕落了一下午），shit… 看论文看论文~~</p></blockquote><h2 id="混合方法（Hybrid-Methods）"><a href="#混合方法（Hybrid-Methods）" class="headerlink" title="混合方法（Hybrid Methods）"></a>混合方法（Hybrid Methods）</h2><p>看名字就知道，混合混合，就是把前人所提出的一些方法就行合并，以及添加一些其他组件来提高性能。</p><h3 id="MixMatch"><a href="#MixMatch" class="headerlink" title="MixMatch"></a><a href="https://arxiv.org/abs/1905.02249">MixMatch</a></h3><p>主要工作是在生成数据上做了很大的功夫，但感觉还是很工程…生成数据的算法如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520200836920.png" alt="image-20230520200836920"></p><p>对于有标签的数据，对其作一次增强，对于无标签的数据，对其作 K 个不同的增强，生成 K 个对应增强后的数据，并且对 K 个生成后的无标签数据做预测，预测后将其预测分布取平均后的分布作一次<u><strong>锐化</strong></u>，，作为 K 个无标签数据的对应分布，或者可以称之为最终的<u><strong>伪标签</strong></u>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520210753737.png" alt="image-20230520210753737"></p><p>得到有标签数据增强后的结果 <code>X^(x, p)</code>，和无标签数据生成后带有伪标签的数据<code>U^(u, q)</code>，过程如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520201404911.png" alt="image-20230520201404911"></p><p>然后将增强后的数据 <code>X</code> 和  <code>U</code> 混合后做一次 <u><strong>Shuffle</strong></u>，得到样本集 <code>W</code>，其中前 N 项作为 <code>W_L</code>，剩余的 M 项为 <code>W_U</code>：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520202313703.png" alt="image-20230520202313703"></p><p>锐化公式如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520204542849.png" alt="image-20230520204542849"></p><p>然后将 <u><strong>MixUp</strong></u> 算法运用在上述集合之中，作以下操作：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520202419730.png" alt="image-20230520202419730"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520202432527.png" alt="image-20230520202432527"></p><p>其中  <u><strong><a href="https://arxiv.org/abs/1710.09412">MixUp</a></strong></u> 算法为图像增强算法，将两个图像按下面的公式生成新的图像：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520203113608.png" alt="image-20230520203113608"></p><p>为了让 MixUp 方法与损失项更兼容，作者对其做了一点小小的修改：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520212837377.png" alt="image-20230520212837377"></p><p>损失函数如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520203414452.png" alt="image-20230520203414452"></p><p>输入：<u><strong>相同 batch 大小</strong></u>有标签数据 <code>X</code> 和无标签数据 <code>U</code></p><p>其中 <code>T</code>，<code>K</code>，<code>α</code> 为模型的超参数，<code>α</code> 在 <u><strong>MixUp</strong></u> 中需要使用，<code>T</code> 在<u><strong>锐化</strong></u>中使用到， <code>K</code> 为 K 个不同的<u><strong>数据增强</strong></u>方法：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520204213042.png"></p><p>输出：增强后的结果 <code>X'</code> 和  <code>U'</code></p><p>然后计算有标签数据的损失 <code>L_X</code>，其中 <code>H</code> 为<u><strong>交叉熵</strong></u>，无标签数据的损失 <code>L_U</code>，使用 <u><strong>L2 损失</strong></u>，其中，无标签数据的损失项中 <code>L</code> 是总的标签数量（也就是输入的标签的维度）：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520204253594.png" alt="image-20230520204253594"></p><p>最后，将有标签数据的损失和无标签数据的损失合并，得到最终的损失，其中无监督的损失会对其施加一个权重 <code>λ_U</code>。</p><p>消融实验的结果：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520220721107.png" alt="image-20230520220721107"></p>]]></content>
      
      
      <categories>
          
          <category> 半监督学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Semi-Surprised-Learning </tag>
            
            <tag> 个人随笔 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title></title>
      <link href="/manifest.json"/>
      <url>/manifest.json</url>
      
        <content type="html"><![CDATA[{"name":"七号zz の Blog","short_name":"七号zz","theme_color":"#eedeab","background_color":"#eedeab","display":"standalone","scope":"/","start_url":"/","icons":[{"src":"/img/siteicon/16.png","sizes":"16x16","type":"image/png"},{"src":"/img/siteicon/32.png","sizes":"32x32","type":"image/png"},{"src":"/img/siteicon/48.png","sizes":"48x48","type":"image/png"},{"src":"/img/siteicon/64.png","sizes":"64x64","type":"image/png"},{"src":"/img/siteicon/128.png","sizes":"128x128","type":"image/png"},{"src":"/img/siteicon/144.png","sizes":"144x144","type":"image/png"},{"src":"/img/siteicon/512.png","sizes":"512x512","type":"image/png"}],"splash_pages":null}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>about</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<h1 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h1><h2 id="一个废材研究生"><a href="#一个废材研究生" class="headerlink" title="一个废材研究生"></a>一个废材研究生</h2><h2 id="ISFJ-x2F-守护者"><a href="#ISFJ-x2F-守护者" class="headerlink" title="ISFJ/守护者"></a>ISFJ/守护者</h2><p>让我在抑郁的边缘徘徊</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/ali_font.js"/>
      <url>/js/ali_font.js</url>
      
        <content type="html"><![CDATA[!(function (c) {  var l,    h,    a,    t,    i,    v =      '<svg><symbol id="icon-dragon_chen" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#D6B196"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-498.122105 265.620211L431.157895 754.526316V485.052632h-66.074948c-14.470737 110.645895-44.355368 197.066105-102.696421 260.742736l-39.747368-36.432842C306.526316 617.876211 323.368421 462.901895 323.368421 242.526316V215.578947h377.263158v53.894737H377.182316c-0.404211 58.260211-2.209684 112.128-6.359579 161.684211H700.631579v53.894737h-122.152421a481.172211 481.172211 0 0 0 76.826947 119.70021l66.479158-39.855158 27.728842 46.214737-54.460631 32.687158c29.507368 24.953263 63.757474 45.675789 102.80421 58.098526l-16.303158 51.361684c-134.224842-42.711579-222.773895-167.073684-261.551158-268.207157H485.052632v221.857684l68.985263-41.391158 27.728842 46.214737-109.783579 65.886316zM646.736842 377.263158h-215.578947v-53.894737h215.578947v53.894737z" fill="#231F20"></path></symbol><symbol id="icon-dog_xu" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#D6B196"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-375.592421 150.393263c33.684211 44.544 75.210105 74.698105 124.739369 90.812632l11.425684 3.718737 10.401684-6.009264C781.204211 727.740632 808.421053 622.565053 808.421053 592.842105h-53.894737c0 22.069895-19.132632 80.869053-33.711158 103.504842-34.816-14.605474-64.538947-39.262316-89.249684-74.13221 48.316632-55.269053 92.079158-117.328842 120.535579-179.900632l-49.044211-22.285473c-23.767579 52.250947-59.742316 104.717474-100.055579 152.656842-24.010105-50.930526-41.148632-115.927579-51.658105-195.395369H700.631579v-53.894737h-155.189895A1848.050526 1848.050526 0 0 1 538.947368 161.684211h-53.894736c0 58.206316 2.155789 112.074105 6.494315 161.68421H323.368421v26.947368c0 216.549053-13.177263 263.545263-100.702316 359.046737l39.747369 36.432842c63.326316-69.093053 92.806737-118.272 105.714526-206.848H485.052632v-53.894736h-111.319579a1742.147368 1742.147368 0 0 0 3.449263-107.789474h120.158316c12.611368 98.250105 35.031579 177.475368 67.395368 238.187789-61.978947 65.536-128.053895 117.975579-173.298526 142.282106l25.519158 47.481263c47.589053-25.573053 114.095158-77.446737 177.55621-142.821053z m125.170526-411.971368l-80.842105-80.842106-38.103579 38.103579 80.842105 80.842106 38.103579-38.103579z" fill="#231F20"></path></symbol><symbol id="icon-dog" viewBox="0 0 1024 1024"><path d="M894.814316 904.434526l83.240421-183.134315-13.824-13.204211c-0.485053-0.458105-45.648842-47.589053-47.939369-185.263158-0.134737-7.922526-0.134737-33.953684-0.134736-55.996631-30.693053 15.306105-70.090105 19.887158-106.09179 19.887157-92.752842 0-163.624421-23.983158-210.647579-71.275789a192.512 192.512 0 0 1-27.944421-36.513684H377.263158v377.263158c342.662737 0 403.105684 51.092211 494.592 128.377263 7.922526 6.682947 15.521684 13.312 22.959158 19.86021z" fill="#85C3DE"></path><path d="M326.063158 282.947368c0 34.250105-13.231158 44.463158-29.642105 44.463158s-29.642105-10.213053-29.642106-44.463158c0-34.223158 13.231158-44.463158 29.642106-44.463157s29.642105 10.24 29.642105 44.463157zM269.473684 430.295579v311.646316L190.275368 916.210526h59.203369L323.368421 753.637053V377.263158h-26.947368c-119.403789 0-172.732632-53.382737-185.505685-107.789474h35.624421c51.092211 0 68.581053-15.764211 120.535579-62.544842 12.773053-11.506526 28.079158-25.276632 47.023158-41.741474l18.351158-15.952842-69.658947-99.139368-44.085895 30.989474 41.768421 59.472842c-11.183158 9.862737-20.884211 18.593684-29.480421 26.327579C180.736 212.156632 176.235789 215.578947 146.539789 215.578947H53.894737v26.947369c0 88.710737 66.910316 178.149053 215.578947 187.769263z m216.710737-161.414737c2.290526 71.733895 28.698947 136.326737 75.048421 182.918737C618.711579 509.628632 702.437053 538.947368 810.091789 538.947368c18.593684 0 36.190316-1.158737 52.628211-3.449263 3.745684 111.265684 33.630316 170.334316 51.496421 196.015158l-38.507789 84.722526C782.174316 742.049684 688.774737 700.631579 377.263158 700.631579v53.894737c34.277053 0 65.697684 0.512 94.639158 1.509052L374.595368 970.105263h59.203369l96.013474-211.240421c66.182737 4.338526 117.005474 11.829895 157.911578 22.016L626.229895 916.210526h59.176421l54.16421-119.134315c47.616 18.405053 79.737263 42.091789 113.125053 69.739789L805.753263 970.105263h59.203369l113.071157-248.778105-13.824-13.204211c-0.485053-0.458105-45.648842-47.589053-47.939368-185.263158C985.168842 498.553263 1024 447.811368 1024 377.263158c0-95.205053-66.506105-161.684211-161.684211-161.684211v53.894737c65.482105 0 107.789474 42.307368 107.789474 107.789474 0 89.088-87.013053 107.789474-160.013474 107.789474-92.752842 0-163.624421-23.983158-210.647578-71.27579-30.315789-30.504421-45.891368-65.832421-53.35579-98.735158 11.210105 6.952421 22.932211 13.338947 35.274105 19.186527l23.04-48.720843c-92.106105-43.654737-148.992-128.646737-219.243789-243.981473l-46.026105 28.05221c49.448421 81.246316 92.968421 148.506947 147.051789 199.302737z" fill="#231F20"></path></symbol><symbol id="icon-goat" viewBox="0 0 1024 1024"><path d="M548.378947 646.736842a952.32 952.32 0 0 1 140.90779-161.68421H107.789474c0 107.600842 0 107.600842-63.649685 169.283368l-13.069473 12.665263L66.721684 754.526316h417.172211c20.345263-41.472 43.654737-77.446737 64.485052-107.789474z" fill="#F7C768"></path><path d="M608.256 144.734316C555.762526 115.577263 506.098526 107.789474 485.052632 107.789474V53.894737c32.579368 0 91.270737 11.452632 149.369263 43.735579 75.290947 41.822316 130.694737 94.531368 171.385263 150.878316C755.873684 288.013474 697.101474 323.368421 646.736842 323.368421h-107.789474v-53.894737h107.789474c20.506947 0 48.424421-11.210105 80.437895-31.285895a471.04 471.04 0 0 0-118.918737-93.453473zM832.673684 342.231579c-16.384 0-29.642105 10.24-29.642105 44.463158 0 34.250105 13.231158 44.463158 29.642105 44.463158s29.642105-10.213053 29.642105-44.463158c0-34.223158-13.231158-44.463158-29.642105-44.463158zM1024 619.789474C1024 347.109053 901.066105 122.448842 686.753684 3.395368l-26.165895 47.104C914.324211 191.461053 964.688842 440.400842 969.647158 592.842105h-84.506947c-17.92-35.624421-45.352421-69.12-87.013053-101.995789l-16.788211-13.285053-16.734315 13.392842c-66.128842 52.897684-134.629053 127.083789-187.311158 209.677474H102.965895l-8.272842-20.318316C159.043368 617.013895 161.684211 603.109053 161.684211 485.052632v-53.894737h485.052631v-53.894737H161.684211c0-80.384 14.309053-110.026105 66.586947-137.916632l-25.384421-47.535158C123.365053 234.226526 107.789474 291.920842 107.789474 377.263158v107.789474c0 107.600842 0 107.600842-63.649685 169.283368l-13.069473 12.665263L110.618947 862.315789h58.206316l-43.897263-107.789473h103.477895l43.897263 107.789473h58.206316l-43.897263-107.789473h259.47621C508.981895 824.939789 485.052632 899.152842 485.052632 970.105263h53.894736c0-68.688842 27.270737-144.060632 68.958316-215.578947H687.157895c7.410526 0 13.473684 6.063158 13.473684 13.473684V862.315789h53.894737v-94.315789c0-37.160421-30.208-67.368421-67.368421-67.368421h-44.65179c40.771368-58.017684 89.438316-111.427368 138.913684-153.626947C841.512421 600.037053 862.315789 655.225263 862.315789 754.526316h53.894737c0-38.912-2.748632-74.482526-11.102315-107.789474H1024v-26.947368z" fill="#231F20"></path></symbol><symbol id="icon-goat_wei" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#D6B196"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-431.157895 50.202947c52.304842 70.925474 136.973474 152.144842 232.528843 190.383158l19.994947-50.041263c-109.271579-43.708632-202.805895-152.629895-238.780632-217.49221H808.421053v-53.894737H538.947368v-53.894737h215.578948v-53.894737h-215.578948V161.684211h-53.894736v161.68421h-215.578948v53.894737h215.578948v53.894737H215.578947v53.894737h255.757474c-35.974737 64.862316-129.536 173.783579-238.807579 217.49221l20.021895 50.041263c95.528421-38.238316 180.197053-119.484632 232.501895-190.383158V808.421053h53.894736v-246.218106z" fill="#231F20"></path></symbol><symbol id="icon-dragon" viewBox="0 0 1024 1024"><path d="M366.376421 344.441263l152.980211-152.98021c43.142737-43.142737 141.204211-9.216 270.201263 115.738947-15.225263 9.835789-25.114947 15.818105-44.13979 32.256s-38.076632 35.489684-59.418947 56.832c-4.203789 4.203789-51.173053 53.221053-78.740211 82.027789-10.805895-12.126316-22.743579-24.171789-34.654315-36.082526L493.136842 362.792421l-54.218105 54.218105-72.542316-72.569263zM862.315789 512c0 46.834526-45.352421 80.842105-107.789473 80.842105-108.948211 0-189.359158-28.806737-267.129263-56.697263C414.100211 509.871158 344.872421 485.052632 258.182737 485.052632 80.788211 485.052632 0 588.126316 0 683.897263h53.894737C73.216 659.779368 135.302737 646.736842 177.340632 646.736842c77.338947 0 223.124211 23.282526 291.893894 47.912421C547.462737 722.701474 615.989895 754.526316 734.315789 754.526316 862.315789 754.526316 916.210526 670.315789 916.210526 512h-53.894737z" fill="#FF8787"></path><path d="M552.421053 1024c-69.766737 0-113.825684-13.958737-156.402527-27.459368-54.487579-17.273263-110.807579-35.004632-232.421052-26.516211l-3.826527-53.733053c131.718737-9.458526 195.934316 10.967579 252.52379 28.887579 42.226526 13.365895 78.686316 24.926316 140.126316 24.926316 92.752842 0 148.210526-57.936842 148.210526-113.960421 0-16.949895-5.524211-101.618526-114.634105-101.618526-64.970105 0-112.747789 23.336421-163.328 48.02021C365.325474 830.571789 300.301474 862.315789 204.288 862.315789 85.908211 862.315789 0 787.294316 0 683.897263 0 588.126316 80.788211 485.052632 258.182737 485.052632c86.689684 0 155.917474 24.818526 229.214316 51.09221 45.810526 16.410947 92.564211 33.172211 145.488842 44.166737 9.000421-7.033263 13.850947-16.276211 13.850947-26.758737 0-37.187368-37.672421-74.859789-74.13221-111.265684l-3.287579-3.287579 38.103579-38.103579 3.260631 3.287579C652.853895 446.275368 700.631579 494.026105 700.631579 553.552842c0 12.719158-2.802526 24.926316-7.976421 36.109474A594.997895 594.997895 0 0 0 754.526316 592.842105c62.437053 0 107.789474-34.007579 107.789473-80.842105 0-58.853053-52.870737-110.268632-108.840421-164.702316l-8.057263-7.841684c-19.024842 16.437895-38.076632 35.489684-59.418947 56.832l-38.103579-38.103579c74.805895-74.832842 134.898526-134.898526 268.314947-141.931789V55.619368c-63.407158 7.787789-120.993684 39.424-121.667368 39.801264l-15.818105 8.811789-14.120421-11.344842C731.701895 66.452211 709.712842 53.894737 673.684211 53.894737c-41.418105 0-74.347789 25.869474-109.190737 53.301895-26.624 20.911158-54.137263 42.549895-86.851369 53.194105L469.342316 161.684211h-69.093053l-105.525895 105.525894-38.103579-38.130526L324.015158 161.684211H161.684211V107.789474h303.104c22.231579-8.272842 43.708632-25.168842 66.398315-42.981053C569.829053 34.438737 613.618526 0 673.684211 0c48.909474 0 81.408 17.946947 110.888421 40.097684C813.702737 26.300632 877.729684 0 943.157895 0h26.947368v323.368421h-53.894737v-53.167158c-54.164211 3.098947-92.914526 15.845053-127.002947 36.675369l1.832421 1.778526C852.587789 368.505263 916.210526 430.376421 916.210526 512c0 60.928-43.708632 109.945263-107.789473 127.622737V700.631579h53.894736v-53.894737h53.894737v53.894737h53.894737v53.894737h-53.894737v53.894737h-53.894737v-53.894737h-53.894736c-29.722947 0-53.894737-24.171789-53.894737-53.894737v-53.894737c-118.325895 0-207.063579-31.797895-285.318737-59.877053C400.437895 562.229895 335.494737 538.947368 258.182737 538.947368 117.059368 538.947368 53.894737 611.732211 53.894737 683.897263 53.894737 757.221053 115.738947 808.421053 204.288 808.421053c11.910737 0 23.228632-0.538947 34.034526-1.536C248.454737 796.321684 269.473684 770.640842 269.473684 739.166316c0-33.118316-43.088842-70.979368-58.152421-81.596632l30.935579-44.139789c8.299789 5.793684 81.111579 58.664421 81.111579 125.736421 0 19.429053-4.527158 37.052632-10.994526 52.304842 30.773895-10.051368 58.314105-23.498105 86.662737-37.349053C452.877474 727.848421 508.577684 700.631579 585.997474 700.631579 702.410105 700.631579 754.526316 778.725053 754.526316 856.144842 754.526316 938.657684 678.912 1024 552.421053 1024z m-21.180632-623.104L493.136842 362.792421l137.889684-137.889684 38.103579 38.103579-137.889684 137.889684z m-126.760421-18.351158l-38.103579-38.103579 152.980211-152.98021 38.103579 38.103579-152.980211 152.98021z m282.004211-218.624c15.494737-9.754947 43.331368-31.447579 43.331368-31.447579-25.734737-27.809684-49.556211-33.333895-67.368421-29.07621-19.240421 4.608-37.753263 24.602947-37.753263 24.602947s42.253474 22.447158 61.790316 35.920842z" fill="#231F20"></path></symbol><symbol id="icon-horse" viewBox="0 0 1024 1024"><path d="M776.003368 646.736842c16.599579-99.947789 43.439158-181.086316 83.213474-256.538947l6.817684-12.934737H269.473684c-36.756211 0-53.894737 54.945684-53.894737 92.05221 0 46.753684 6.656 77.527579 70.278737 176.074106l84.533895 128.269473L498.876632 646.736842h277.126736z" fill="#FFAF6E"></path><path d="M1024 0v404.210526c0 33.333895 0 134.736842-92.079158 134.736842h-13.824l-78.362947-109.056c-22.743579 49.906526-40.340211 103.046737-53.490527 162.950737h115.092211C937.310316 592.842105 970.105263 625.637053 970.105263 661.638737c0 60.631579-69.389474 154.300632-77.312 164.75621l-43.008-32.471579C875.466105 759.861895 916.210526 693.813895 916.210526 661.638737c0-5.982316-8.919579-14.901895-14.901894-14.901895h-125.332211C761.128421 736.121263 754.526316 840.569263 754.526316 970.105263h-53.894737c0-283.971368 31.097263-453.605053 110.888421-605.049263l20.318316-38.534737 112.801684 156.995369c14.443789-4.419368 25.465263-20.938105 25.465263-79.306106V0h53.894737z m-161.684211 161.684211h53.894737V0h-53.894737v80.842105c-17.381053-14.955789-38.184421-26.947368-80.842105-26.947368h-134.736842v53.894737h134.736842c37.672421 0 80.842105 40.906105 80.842105 53.894737z m-107.789473 0h-215.578948v53.894736h161.684211l53.894737-53.894736zM300.894316 766.544842L400.680421 916.210526h64.754526l-95.043368-142.551579L498.876632 646.736842h167.855157a1212.631579 1212.631579 0 0 1 9.431579-53.894737h-199.383579l-175.885473 173.702737z m109.97221-184.400842l-37.861052-38.319158-132.419369 130.802526C173.729684 571.095579 161.684211 529.812211 161.684211 469.315368 161.684211 398.578526 199.464421 323.368421 269.473684 323.368421h323.368421l53.894737-53.894737H269.473684c-6.709895 0-13.258105 0.565895-19.698526 1.482105C234.927158 249.451789 204.638316 215.578947 160.633263 215.578947 65.967158 215.578947 0 349.291789 0 469.315368c0 70.170947 16.141474 136.650105 49.232842 202.671158L6.197895 723.833263l41.472 34.41179 66.128842-79.737264-8.704-16.033684C83.105684 622.133895 53.894737 558.214737 53.894737 469.315368 53.894737 368.451368 106.765474 269.473684 160.633263 269.473684c13.231158 0 25.815579 9.889684 35.43579 20.533895C142.874947 321.967158 107.789474 388.500211 107.789474 469.315368c0 78.201263 19.698526 130.937263 93.642105 243.981474l-55.296 54.622316L280.899368 970.105263h64.754527l-130.048-195.072 195.260631-192.889263z" fill="#231F20"></path></symbol><symbol id="icon-monkey_shen" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#BBC4C9"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-431.157895 134.736842h161.684211v53.894737h53.894737V269.473684h-215.578948V161.684211h-53.894736v107.789473h-215.578948v431.157895h53.894737v-53.894737h161.684211v215.578947h53.894736v-215.578947z m0-161.68421h161.684211v107.789473h-161.684211v-107.789473z m-215.578947 0h161.684211v107.789473h-161.684211v-107.789473z m215.578947-161.684211h161.684211v107.789474h-161.684211v-107.789474z m-215.578947 0h161.684211v107.789474h-161.684211v-107.789474z" fill="#231F20"></path></symbol><symbol id="icon-ox_chou" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#D6B196"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-161.68421 188.631579h-159.555369c13.985684-172.813474 43.115789-357.429895 70.817684-385.158737L700.631579 269.473684H323.368421v53.894737h107.169684c-1.940211 45.756632-8.192 103.962947-15.76421 161.684211H323.368421v53.894736h83.968c-9.862737 68.446316-20.264421 130.128842-25.734737 161.684211H215.578947v53.894737h592.842106v-53.894737z m-346.543158-161.684211h149.800421a3313.717895 3313.717895 0 0 0-16.842105 161.684211h-158.477474c6.036211-35.247158 16.114526-95.636211 25.519158-161.684211z m22.608842-215.578947h171.735579c-15.198316 41.121684-27.405474 100.594526-36.890948 161.684211h-150.123789c7.383579-57.505684 13.419789-115.361684 15.279158-161.684211z" fill="#231F20"></path></symbol><symbol id="icon-monkey" viewBox="0 0 1024 1024"><path d="M757.733053 485.052632H565.894737a80.842105 80.842105 0 0 0-80.842105 80.842105v215.578947c0 40.96 43.546947 99.678316 77.446736 139.210105C596.426105 960.215579 603.055158 970.105263 603.055158 970.105263H754.526316s15.144421-18.674526 45.891368-58.071579S862.315789 809.984 862.315789 717.608421c0-89.573053-47.993263-166.346105-104.582736-232.555789z" fill="#C3D686"></path><path d="M538.947368 1024h-53.894736c0-32.794947 25.869474-87.417263 77.446736-103.316211C528.599579 881.152 485.052632 822.433684 485.052632 781.473684c0-44.570947 36.271158-80.842105 80.842105-80.842105h80.842105v53.894737h-80.842105a26.947368 26.947368 0 0 0-26.947369 26.947368c0 19.725474 36.675368 77.473684 92.133053 134.736842h88.602947c20.210526-14.147368 88.737684-71.464421 88.737685-198.602105 0-108.382316-93.237895-202.967579-168.151579-278.986105-49.502316-50.202947-88.576-89.842526-98.735158-128.61979-11.749053-44.732632-21.584842-112.586105-26.327579-148.318315H377.263158c-45.136842 0-89.519158 8.434526-121.802105 53.894736H431.157895v53.894737c-97.28 0-107.789474 113.071158-107.789474 161.684211v53.894737h53.894737v161.68421h-53.894737v-107.789474h-26.947368c-170.253474 0-188.631579-94.234947-188.631579-134.736842 0-31.043368 35.220211-72.326737 55.727158-93.722947 2.694737-14.686316 5.847579-28.348632 9.431579-41.013895H161.684211V215.578947h31.528421C239.642947 120.993684 317.224421 107.789474 377.263158 107.789474h185.640421l2.802526 23.794526c0.134737 1.050947 12.719158 106.657684 27.944421 164.756211 6.494316 24.872421 44.624842 63.514947 84.965053 104.448C760.481684 483.813053 862.315789 587.129263 862.315789 717.608421c0 92.375579-31.124211 155.028211-61.898105 194.425263C904.919579 892.146526 970.105263 803.004632 970.105263 673.684211c0-91.405474-42.819368-154.381474-84.237474-215.255579C847.791158 402.458947 808.421053 344.576 808.421053 269.473684c0-119.349895 87.093895-161.684211 161.68421-161.68421v53.894737c-32.417684 0-107.789474 10.509474-107.789474 107.789473 0 58.502737 31.555368 104.933053 68.096 158.639158C974.282105 492.597895 1024 565.679158 1024 673.684211c0 177.286737-108.301474 296.421053-269.473684 296.421052h-161.684211c-37.672421 0-53.894737 40.906105-53.894737 53.894737zM229.214316 269.473684a384.808421 384.808421 0 0 0-14.012632 58.341053l-1.401263 8.488421-6.090105 6.117053c-22.878316 22.932211-44.813474 52.601263-46.026105 62.275368 0 56.805053 53.76 75.264 107.789473 79.386947V431.157895c0-58.691368 13.473684-119.619368 46.511158-161.684211h-86.770526zM323.368421 1024h-53.894737c0-32.794947 25.869474-87.417263 77.446737-103.316211C313.020632 881.152 269.473684 822.433684 269.473684 781.473684c0-44.570947 36.271158-80.842105 80.842105-80.842105h45.16379A188.847158 188.847158 0 0 1 565.894737 592.842105h134.736842v53.894737h-134.736842c-74.293895 0-134.736842 60.442947-134.736842 134.736842v26.516211l-53.894737 0.377263V781.473684c0-9.162105 0.646737-18.135579 1.913263-26.947368H350.315789c-14.848 0-26.947368 12.072421-26.947368 26.947368 0 19.725474 36.675368 77.473684 92.133053 134.736842H431.157895v53.894737h-53.894737c-37.672421 0-53.894737 40.906105-53.894737 53.894737z" fill="#231F20"></path></symbol><symbol id="icon-horse_wu" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#FF8787"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-431.157895 26.947368h269.473685v-53.894736H538.947368v-161.684211h161.684211v-53.894737H411.001263c12.045474-33.28 20.156632-69.793684 20.156632-107.789473h-53.894737c0 121.963789-105.364211 233.391158-106.415158 234.496l38.858105 37.349052c2.883368-3.018105 43.816421-46.133895 77.392842-110.160842H485.052632v161.684211H215.578947v53.894736h269.473685v323.368421h53.894736V538.947368z" fill="#231F20"></path></symbol><symbol id="icon-ox" viewBox="0 0 1025 1024"><path d="M540.294737 754.526316h215.578947c20.210526 0 35.112421 1.374316 53.894737 4.581052 91.863579 15.656421 145.354105 67.691789 161.684211 86.069895V916.210526h53.894736V635.580632l-7.895579-7.895579c-9.269895-9.269895-36.513684-49.232842-44.032-196.527158H540.294737a161.684211 161.684211 0 0 0-161.684211 161.68421v131.098948c43.304421 20.210526 97.28 30.585263 161.684211 30.585263z" fill="#FFAF6E"></path><path d="M1025.347368 635.580632V916.210526h-53.894736v-71.033263c-16.330105-18.405053-69.820632-70.413474-161.684211-86.069895V916.210526h-53.894737v-161.68421h-107.789473v215.578947h-53.894737V700.631579h161.68421c100.998737 0 172.570947 38.669474 215.578948 71.868632v-115.738948c-33.684211-43.627789-51.712-137.458526-53.706106-279.498105H701.978947c-76.934737 0-127.218526-26.219789-175.804631-51.550316a1556.048842 1556.048842 0 0 0-26.839579-13.743158c-26.839579 26.004211-66.209684 44.921263-115.738948 55.511579 24.441263 22.986105 60.874105 52.116211 106.469053 72.838737l-22.312421 49.044211c-76.584421-34.816-129.589895-88.926316-150.824421-113.125053-10.644211 0.619789-21.477053 1.024-32.687158 1.024a473.734737 473.734737 0 0 1-123.365053-15.952842l-93.022315 186.314105 68.581052 53.86779C167.882105 579.557053 237.891368 538.947368 324.715789 538.947368v53.894737c-95.986526 0-170.361263 62.490947-171.088842 63.137684l-16.78821 14.282106-136.838737-107.358316 109.729684-219.809684C46.430316 314.448842 1.347368 267.371789 1.347368 199.868632 1.347368 89.815579 121.586526 53.894737 163.031579 53.894737v53.894737c-14.120421 0-107.789474 17.165474-107.789474 92.079158C55.242105 290.465684 192.188632 323.368421 284.240842 323.368421c67.907368 0 122.421895-12.988632 157.696-35.624421-42.711579-14.336-95.097263-23.120842-169.337263-18.324211l-3.503158-53.786947c95.878737-6.117053 160.148211 8.515368 211.429053 28.833684C484.244211 235.439158 486.4 225.818947 486.4 215.578947c0-48.855579-57.829053-76.288-58.394947-76.557473l22.393263-49.017263C454.063158 91.648 540.294737 131.826526 540.294737 215.578947c0 18.566737-3.422316 35.84-9.997474 51.631158 7.060211 3.584 13.985684 7.168 20.776421 10.698106C597.854316 302.322526 638.248421 323.368421 701.978947 323.368421h269.473685v26.947368c0 214.689684 35.220211 266.590316 45.999157 277.369264l7.895579 7.895579z m-729.384421 25.141894l-98.789052 118.541474 86.797473 137.835789 45.594948-28.725894-65.913263-104.690527 37.052631-44.43621C358.642526 785.192421 439.080421 808.421053 540.294737 808.421053v-53.894737c-99.893895 0-175.077053-24.549053-223.474526-72.946527l-20.857264-20.857263z" fill="#231F20"></path></symbol><symbol id="icon-rabbit_mao" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#7DD47F"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-377.263158-188.631579h107.789474v323.368421c-20.48 0-39.936-11.264-40.016842-11.317895l-27.728842 46.214737c3.206737 1.940211 32.660211 18.997895 67.745684 18.997895 30.746947 0 53.894737-23.147789 53.894737-53.894737V269.473684h-215.578948v538.947369h53.894737V323.368421z m-107.789473 242.526316v-242.526316h-53.894737v196.904421l-107.789474 40.421053v-243.927579l169.094737-48.316632-14.821053-51.819789L269.473684 276.102737v304.801684l-36.405895 13.662316 18.917053 50.472421 178.741895-67.018105c-5.039158 69.928421-55.269053 106.981053-165.133474 122.933894l7.733895 53.328842C325.712842 746.657684 485.052632 723.536842 485.052632 565.894737z" fill="#231F20"></path></symbol><symbol id="icon-rabbit" viewBox="0 0 1024 1024"><path d="M680.96 488.744421a1666.667789 1666.667789 0 0 0-54.433684-23.95621c-16.006737 12.234105-33.899789 20.264421-60.631579 20.264421h-80.842105c-36.810105 0-83.644632 30.396632-104.394106 67.772631-42.819368 77.123368-53.409684 117.813895-11.021473 201.701053C397.096421 808.879158 431.157895 876.409263 431.157895 970.105263h338.539789l68.338527-138.859789c20.129684-40.96 24.252632-73.701053 24.252631-110.349474 0.026947-57.397895-25.061053-159.717053-181.328842-232.151579z" fill="#FFBDD8"></path><path d="M862.315789 720.896c0 36.621474-4.122947 69.389474-24.252631 110.349474L769.697684 970.105263H485.052632v-53.894737h48.370526C507.877053 880.074105 485.052632 833.509053 485.052632 781.473684c0-59.418947 24.171789-113.313684 63.218526-152.360421l38.103579 38.103579A161.091368 161.091368 0 0 0 538.947368 781.473684c0 54.784 35.381895 104.043789 63.514948 134.736842h133.712842l53.490526-108.759579c15.710316-31.851789 18.755368-55.834947 18.755369-86.554947 0-80.976842-63.434105-150.096842-178.607158-195.503158-17.542737 8.138105-38.292211 13.554526-63.919158 13.554526h-80.842105c-13.958737 0-43.924211 15.979789-57.290106 40.016843l-47.104-26.165895C401.408 515.449263 448.242526 485.052632 485.052632 485.052632h80.842105c37.268211 0 57.478737-15.440842 79.090526-36.45979C625.367579 336.195368 549.753263 269.473684 485.052632 269.473684h-107.789474a21.288421 21.288421 0 0 0-5.955369 2.021053A683.762526 683.762526 0 0 0 302.187789 194.021053c-35.84-34.223158-61.763368-58.933895-94.908631-79.440842A42.442105 42.442105 0 0 0 185.478737 107.789474a22.824421 22.824421 0 0 0-17.381053 7.194947c-10.913684 11.425684-6.063158 28.240842 1.428211 39.181474 21.989053 32.121263 47.912421 56.858947 83.752421 91.109052 20.614737 19.671579 49.259789 43.169684 77.392842 63.08379C281.007158 367.400421 215.578947 484.432842 215.578947 592.842105c0 74.482526 24.791579 124.065684 51.065264 176.586106C294.534737 825.209263 323.368421 882.903579 323.368421 970.105263h-53.894737c0-74.482526-24.791579-124.065684-51.065263-176.586105C190.517895 737.738105 161.684211 680.043789 161.684211 592.842105c0-90.866526 42.226526-197.685895 93.453473-274.485894a803.759158 803.759158 0 0 1-39.046737-34.115369C177.852632 247.754105 150.231579 221.399579 125.035789 184.616421c-24.441263-35.759158-22.797474-78.686316 4.069053-106.819368 26.300632-27.567158 70.898526-31.043368 106.522947-9.000421 37.941895 23.444211 65.562947 49.798737 103.774316 86.258526 9.970526 9.512421 33.037474 32.309895 56.93979 60.550737h68.634947c-27.621053-37.780211-60.416-72.730947-88.522105-99.543579-28.833684-27.540211-54.730105-52.116211-84.533895-74.024421L326.305684 0.296421c31.232 23.228632 57.802105 48.532211 87.309474 76.719158 53.840842 51.388632 94.450526 100.594526 121.74821 146.83621 82.836211 26.650947 150.042947 116.870737 165.025685 230.750316l1.724631 13.177263-9.404631 9.404632c-3.772632 3.772632-7.706947 7.653053-11.802948 11.587368C837.227789 561.178947 862.315789 663.498105 862.315789 720.896zM309.463579 754.526316c3.934316 8.057263 7.895579 16.087579 11.991579 24.144842C348.887579 832.970105 377.263158 889.128421 377.263158 970.105263h53.894737c0-93.696-34.061474-161.226105-61.520842-215.578947h-60.173474z m597.90821 53.894737c-3.422316 9.404632-7.814737 19.806316-13.770105 31.959579L829.790316 970.105263h60.065684l52.143158-105.957052c10.778947-21.935158 17.515789-40.016842 21.90821-55.727158h-56.535579zM514.694737 390.736842c0-34.223158-13.231158-44.463158-29.642105-44.463158s-29.642105 10.24-29.642106 44.463158c0 34.250105 13.231158 44.463158 29.642106 44.463158s29.642105-10.213053 29.642105-44.463158z" fill="#231F20"></path></symbol><symbol id="icon-rat_zi" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#85C3DE"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-431.157895 188.631579v-215.578947h269.473685v-53.894737H538.947368v-39.585684c26.543158-18.081684 94.585263-65.050947 177.852632-127.488L700.631579 215.578947H323.368421v53.894737h295.316211a4221.008842 4221.008842 0 0 1-121.640421 85.369263l-11.991579 8.003369V431.157895H242.526316v53.894737h242.526316v215.578947c0 48.343579-13.850947 53.894737-134.736843 53.894737v53.894737c105.391158 0 188.631579 0 188.631579-107.789474z" fill="#231F20"></path></symbol><symbol id="icon-rat" viewBox="0 0 1024 1024"><path d="M727.659789 431.157895c-132.581053 0-220.348632 47.454316-285.803789 154.354526-19.779368 32.309895-15.845053 76.503579-9.404632 96.579368 3.260632 10.159158 7.760842 18.647579 12.422737 25.546106C464.761263 737.010526 499.927579 754.526316 538.947368 754.526316h66.829474c1.158737 17.893053-1.967158 34.762105-15.144421 53.975579-12.692211 18.539789-37.807158 40.151579-56.32 54.810947 25.249684-0.673684 52.709053-0.997053 83.240421-0.997053C877.487158 862.315789 970.105263 711.922526 970.105263 571.176421 936.421053 512 882.364632 431.157895 727.659789 431.157895z" fill="#85C3DE"></path><path d="M210.432 1012.897684l-43.573895-31.690105c106.954105-147.051789 185.317053-171.196632 423.828211-172.705684 21.396211-31.258947 16.249263-56.266105 9.377684-89.70779-3.557053-17.138526-7.221895-34.842947-7.221895-54.433684 0-68.958316 25.330526-104.636632 63.407158-136.973474l34.896842 41.040842c-29.453474 25.061053-44.409263 46.780632-44.409263 95.932632 0 14.093474 2.937263 28.402526 6.063158 43.546947 5.901474 28.510316 12.8 62.032842-1.131789 99.462737 166.373053-10.24 264.542316-96.902737 264.542315-236.193684C916.210526 418.330947 827.580632 323.368421 684.921263 323.368421c-83.644632 0-153.303579 29.696-174.187789 39.612632a224.875789 224.875789 0 0 1-20.533895 31.339789l-41.741474-34.115368 20.884211 17.057684-20.911158-16.976842C448.781474 359.828211 485.052632 314.287158 485.052632 262.736842c0-34.816-8.946526-60.766316-26.570106-77.069474-17.515789-16.249263-44.786526-24.602947-81.219368-24.953263V323.368421h-53.894737V109.783579l24.872421-1.913263c64.700632-4.931368 114.095158 7.895579 146.863158 38.238316C524.207158 173.056 538.947368 212.291368 538.947368 262.736842c0 11.102316-1.131789 21.908211-3.072 32.202105 37.268211-12.584421 89.842526-25.465263 149.045895-25.465263C858.165895 269.473684 970.105263 387.907368 970.105263 571.176421 970.105263 711.922526 877.487158 862.315789 617.552842 862.315789c-258.667789 0-311.942737 19.698526-407.120842 150.581895z m19.105684-256.835368c-12.045474 0-24.387368-0.565895-37.025684-1.64379l-22.096842-1.859368-2.425263-22.016C167.747368 728.144842 161.684211 672.444632 161.684211 631.026526c0-103.585684 21.450105-178.903579 53.894736-259.045052V107.789474h53.894737v274.782315l-2.021052 4.904422C235.439158 465.758316 215.578947 533.800421 215.578947 631.026526c0 22.878316 2.101895 51.442526 3.826527 70.979369 99.678316 2.802526 172.813474-35.408842 222.450526-116.493474l48.020211 24.090947c-11.237053 28.133053-11.371789 51.577263-0.377264 67.853474 9.701053 14.282105 28.645053 23.174737 49.448421 23.174737v53.894737c-39.019789 0-74.186105-17.515789-94.073263-46.888421a100.244211 100.244211 0 0 1-12.422737-25.546106c-53.221053 49.178947-121.128421 73.943579-202.913684 73.970527zM379.957895 525.473684c0-34.223158-13.231158-44.463158-29.642106-44.463158s-29.642105 10.24-29.642105 44.463158c0 34.250105 13.231158 44.463158 29.642105 44.463158s29.642105-10.213053 29.642106-44.463158z" fill="#231F20"></path></symbol><symbol id="icon-rooster_you" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#BBC4C9"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-215.578947-188.631579h-161.684211v-26.947368h161.684211V242.526316H269.473684v53.894737h161.684211v26.947368h-161.684211v485.052632h53.894737v-53.894737h377.263158v53.894737h53.894737V323.368421zM323.368421 646.736842h377.263158v53.894737H323.368421v-53.894737z m0-269.473684h107.789474c0 103.316211-72.784842 107.654737-81.084632 107.789474L350.315789 538.947368c46.592 0 134.736842-33.792 134.736843-161.68421h53.894736v107.789474c0 29.722947 24.171789 53.894737 53.894737 53.894736h107.789474v53.894737H323.368421v-215.578947z m377.263158 0v107.789474h-107.789474v-107.789474h107.789474z m-215.578947-80.842105h53.894736v26.947368h-53.894736v-26.947368z" fill="#231F20"></path></symbol><symbol id="icon-rooster" viewBox="0 0 1024 1024"><path d="M891.688421 506.421895C877.244632 455.033263 862.315789 401.893053 862.315789 323.368421V116.224l-323.368421 195.745684V323.368421c0 78.524632 14.928842 131.664842 29.372632 183.053474 12.611368 44.894316 24.522105 87.282526 24.522105 140.314947 0 101.618526-77.931789 176.693895-168.286316 203.991579l5.416422 11.587368h215.578947c24.333474 0 43.385263-0.242526 58.556631-2.128842C811.52 846.821053 916.210526 764.550737 916.210526 646.736842c0-53.032421-11.910737-95.420632-24.522105-140.314947z" fill="#FF8787"></path><path d="M673.684211 354.357895c-16.384 0-29.642105-10.213053-29.642106-44.463158 0-34.223158 13.231158-44.463158 29.642106-44.463158s29.642105 10.24 29.642105 44.463158c0 34.250105-13.258105 44.463158-29.642105 44.463158zM540.106105 970.105263l-50.58021-107.789474h156.05221l50.607158 107.789474h59.553684l-51.60421-109.918316C811.52 846.821053 916.210526 764.550737 916.210526 646.736842c0-53.032421-11.910737-95.420632-24.522105-140.314947C877.244632 455.033263 862.315789 401.893053 862.315789 323.368421V107.789474c0-59.445895-48.343579-107.789474-107.789473-107.789474a107.924211 107.924211 0 0 0-107.789474 106.172632 100.890947 100.890947 0 0 0-24.117895-3.314527 88.710737 88.710737 0 0 0-88.602947 88.602948c0 20.668632 5.227789 39.720421 10.671158 53.921684l-99.489684 59.688421 93.749894 14.470737V377.263158c0 14.416842-5.901474 21.692632-33.360842 49.152l-11.129263 11.129263C398.228211 326.521263 324.985263 269.473684 215.740632 269.473684 96.768 269.473684 0 366.241684 0 485.214316V646.736842h53.894737v-161.522526A162.007579 162.007579 0 0 1 215.740632 323.368421c82.081684 0 140.422737 36.244211 240.64 152.252632l-38.615579 38.615579C367.804632 461.285053 323.098947 431.157895 259.584 431.157895A151.983158 151.983158 0 0 0 107.789474 582.952421V754.526316h53.894737v-171.573895A98.007579 98.007579 0 0 1 259.584 485.052632c46.322526 0 79.629474 20.911158 137.027368 86.016l18.970948 21.530947 128.080842-128.080842C572.200421 435.981474 592.842105 415.366737 592.842105 377.263158v-97.926737l23.309474-14.120421-13.662316-23.04c-0.161684-0.242526-14.578526-24.899368-14.578526-50.688 0-19.132632 15.575579-34.708211 34.70821-34.708211 5.093053 0 26.785684 3.179789 39.558737 18.647579l26.327579 46.026106 39.774316-24.090948-20.372211-49.367579C704.754526 140.449684 700.631579 117.517474 700.631579 107.789474c0-29.722947 24.171789-53.894737 53.894737-53.894737s53.894737 24.171789 53.894737 53.894737v215.578947c0 85.935158 16.680421 145.300211 31.366736 197.632C851.887158 564.008421 862.315789 601.141895 862.315789 646.736842c0 95.285895-99.408842 161.684211-188.631578 161.684211h-209.461895l-68.419369-145.704421C375.242105 618.954105 338.108632 592.842105 296.448 592.842105A80.976842 80.976842 0 0 0 215.578947 673.711158V862.315789h53.894737v-188.604631c0-14.874947 12.099368-26.974316 26.974316-26.974316 20.533895 0 38.965895 14.147368 50.553263 38.858105L480.579368 970.105263h59.526737z" fill="#231F20"></path></symbol><symbol id="icon-snake_si" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#FF8787"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-242.041263 180.762947l-52.116211-13.797052C657.219368 749.864421 651.425684 754.526316 619.789474 754.526316h-242.526316V485.052632h269.473684v53.894736h53.894737V215.578947H323.368421v538.947369c0 29.722947 24.171789 53.894737 53.894737 53.894737h242.526316c77.689263 0 91.189895-51.065263 108.274526-115.658106zM377.263158 269.473684h269.473684v161.684211H377.263158v-161.684211z" fill="#231F20"></path></symbol><symbol id="icon-tiger_yin" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#7DD47F"></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-257.42821 299.250526l-107.789474-53.894737-24.117895 48.208843 107.789474 53.894736 24.117895-48.208842z m-269.473685-5.658947l-24.117894-48.208842-107.789474 53.894737 24.117895 48.208842 107.789473-53.894737zM700.631579 431.157895h-161.684211v-53.894737h107.789474v-53.894737H377.263158v53.894737h107.789474v53.894737h-161.684211v323.368421h53.894737v-53.894737h269.473684v53.894737h53.894737V431.157895z m-161.684211 161.68421h107.789474v53.894737h-107.789474v-53.894737z m-161.68421 0h107.789474v53.894737h-107.789474v-53.894737z m161.68421-107.789473h107.789474v53.894736h-107.789474v-53.894736z m-161.68421 0h107.789474v53.894736h-107.789474v-53.894736zM754.526316 215.578947h-223.097263l-20.803369-62.410105-51.119158 17.057684L474.624 215.578947H269.473684v107.789474h53.894737v-53.894737h377.263158v53.894737h53.894737V215.578947z" fill="#231F20"></path></symbol><symbol id="icon-snake" viewBox="0 0 1024 1024"><path d="M107.789474 790.474105c0-72.434526 67.880421-91.513263 121.451789-91.513263 74.401684 0 153.815579 34.438737 237.891369 70.925474 50.580211 21.935158 104.609684 45.325474 162.250105 63.083789-52.412632 44.786526-118.784 74.347789-195.152842 83.078737-143.171368 16.357053-326.440421 7.006316-326.440421-125.574737zM377.263158 215.578947c-15.575579 0-30.288842 3.449263-43.654737 9.377685A250.691368 250.691368 0 0 0 323.368421 296.421053c0 115.550316 76.422737 169.391158 137.83579 212.614736 8.138105 5.712842 16.141474 11.371789 23.848421 17.057685V323.368421a107.789474 107.789474 0 0 0-107.789474-107.789474z" fill="#C3D686"></path><path d="M671.528421 788.857263c44.328421 11.964632 89.626947 19.563789 136.892632 19.56379 89.168842 0 161.684211-60.442947 161.68421-134.736842s-72.515368-134.736842-161.68421-134.736843c-19.078737 0-37.025684 1.509053-54.218106 4.015158-0.754526-101.402947-38.211368-172.355368-79.413894-219.648L673.684211 323.368421a1749.962105 1749.962105 0 0 1-79.036632-1.751579c45.702737 35.866947 108.705684 107.870316 105.984 232.367158 0 0.431158-0.080842 0.808421-0.10779 1.239579-34.923789 10.994526-66.155789 26.731789-95.097263 45.190737a163.085474 163.085474 0 0 0-15.845052-42.388211c-21.557895-39.639579-60.065684-66.775579-97.360842-93.022316C433.098105 423.343158 377.263158 384 377.263158 296.421053c0-130.290526 108.274526-188.631579 215.578947-188.631579 64.134737 0 132.715789 12.045474 214.366316 37.807158C802.330947 180.250947 780.099368 209.381053 700.631579 214.635789V161.684211h-53.894737v53.679157c-63.272421-1.024-104.528842-5.200842-104.986947-5.254736l-5.578106 53.598315C538.408421 263.949474 592.357053 269.473684 673.684211 269.473684c125.170526 0 188.631579-48.128 188.631578-143.063579V106.981053l-18.432-6.144C747.789474 68.823579 668.025263 53.894737 592.842105 53.894737c-158.666105 0-269.473684 99.732211-269.473684 242.526316 0 115.550316 76.422737 169.391158 137.83579 212.614736 33.684211 23.713684 65.509053 46.106947 81.003789 74.698106 9.539368 17.542737 13.285053 33.414737 12.341895 47.750737 21.153684 9.108211 42.118737 17.839158 62.949052 25.977263C671.151158 620.193684 729.977263 592.842105 808.421053 592.842105c59.445895 0 107.789474 36.271158 107.789473 80.842106s-48.343579 80.842105-107.789473 80.842105c-105.472 0-203.237053-42.388211-297.768421-83.429053-94.800842-41.094737-184.346947-79.952842-281.411369-79.952842C122.718316 591.171368 53.894737 644.715789 53.894737 727.578947c0 79.063579 67.098947 136.434526 159.555368 136.434527 142.174316 0 230.426947-66.883368 306.79579-129.886316 31.420632 13.419789 62.787368 26.058105 94.450526 37.133474-47.077053 49.637053-110.969263 82.566737-186.610526 91.270736l5.066105 53.625264c93.453474-7.006316 143.144421 9.350737 195.718737 26.543157 46.457263 15.225263 94.127158 30.854737 169.822316 30.854737 19.994947 0 41.957053-1.077895 66.344421-3.557052l-5.416421-53.625263c-105.283368 10.778947-158.100211-6.548211-213.935158-24.872422-22.150737-7.275789-44.624842-14.632421-70.305684-20.345263a334.848 334.848 0 0 0 96.14821-82.297263z m-458.078316 21.261474C162.573474 810.118737 107.789474 784.276211 107.789474 727.578947c0-60.847158 62.733474-82.539789 121.451789-82.539789 77.850947 0 154.731789 30.288842 235.250526 64.943158-66.263579 52.924632-139.722105 100.136421-251.041684 100.136421z" fill="#231F20"></path></symbol><symbol id="icon-tiger" viewBox="0 0 1024 1024"><path d="M431.157895 162.250105V134.736842c0-41.552842-39.289263-80.842105-80.842106-80.842105-28.833684 0-57.128421 4.661895-58.314105 4.850526L269.473684 62.490947v83.887158C144.788211 223.824842 89.222737 346.839579 66.991158 431.157895h266.051368c240.747789 0 415.851789 107.789474 415.85179 269.473684-14.848-25.114947-43.924211-53.894737-88.68379-53.894737-67.988211 0-121.263158 71.033263-121.263158 161.684211 0 66.802526 30.477474 119.888842 60.712421 156.16 12.638316 15.171368 36.055579 37.726316 59.014737 58.88 5.066105 0.107789 9.781895 0.538947 15.009685 0.538947 219.297684 0 350.315789-191.811368 350.315789-377.263158C1024 327.545263 679.855158 172.813474 431.157895 162.250105z" fill="#F7C768"></path><path d="M673.684211 1024c-114.768842 0-188.820211-33.333895-254.167579-62.787368-53.625263-24.144842-99.974737-45.002105-161.28-45.002106-40.448 0-83.590737 23.255579-103.639579 45.16379l-39.747369-36.432842C142.497684 894.787368 199.168 862.315789 258.236632 862.315789c68.392421 0 119.861895 21.288421 172.921263 45.056V673.684211c0-35.166316-17.542737-64.107789-30.639158-80.815158-15.198316 9.835789-32.067368 18.890105-50.741895 26.947368l-21.342316-49.475368C469.800421 509.413053 485.052632 377.317053 485.052632 323.368421V221.642105A597.827368 597.827368 0 0 0 404.210526 215.578947h-26.947368V134.736842c0-12.099368-14.848-26.947368-26.947369-26.947368-9.377684 0-18.836211 0.592842-26.947368 1.347368V269.473684h-53.894737V211.671579c-136.030316 102.912-158.450526 266.886737-161.306947 295.882105 9.135158 9.108211 38.992842 25.061053 71.976421 38.669474l38.103579-59.365053 12.449684-1.589894C321.212632 473.653895 377.263158 392.192 377.263158 323.368421h53.894737c0 88.333474-68.796632 192.242526-180.870737 213.342316l-48.397474 75.398737-20.291368-7.437474C53.894737 557.756632 53.894737 523.317895 53.894737 512c0-50.041263 37.025684-254.733474 215.578947-365.621895V62.490947l22.528-3.745684C293.187368 58.556632 321.482105 53.894737 350.315789 53.894737c41.552842 0 80.842105 39.289263 80.842106 80.842105v27.513263c248.697263 10.563368 592.842105 165.295158 592.842105 484.486737 0 185.451789-131.018105 377.263158-350.315789 377.263158z m-13.473685-323.368421c-36.513684 0-67.368421 49.367579-67.368421 107.789474 0 85.746526 68.096 145.084632 89.465263 161.549473 91.540211-2.533053 164.378947-45.487158 213.827369-107.654737H700.631579v-53.894736h230.238316c8.919579-17.273263 16.357053-35.354947 22.285473-53.894737h-239.885473l-6.467369-17.650527C706.290526 735.582316 692.439579 700.631579 660.210526 700.631579zM485.052632 931.112421c33.926737 14.066526 70.521263 26.597053 114.607157 33.468632C569.424842 928.309895 538.947368 875.223579 538.947368 808.421053c0-90.650947 53.274947-161.684211 121.263158-161.684211 44.759579 0 73.835789 28.779789 88.68379 53.894737h217.007158c2.775579-17.866105 4.203789-35.920842 4.203789-53.894737 0-38.938947-5.658947-74.752-15.925895-107.627789l-126.706526 126.679579-38.103579-38.103579L932.001684 485.052632a367.939368 367.939368 0 0 0-57.775158-81.596632l-154.543158 154.543158-38.103579-38.103579 153.573053-153.573053a537.869474 537.869474 0 0 0-82.593684-56.751158l-140.665263 140.638316-38.103579-38.103579 128.134737-128.134737A794.731789 794.731789 0 0 0 538.947368 231.046737V323.368421c0 50.149053-11.102316 156.698947-95.932631 236.328421 18.378105 23.417263 42.037895 63.407158 42.037895 113.987369v257.42821zM215.578947 431.157895v-53.894737c39.774316 0 53.894737-29.022316 53.894737-53.894737h53.894737c0 53.571368-37.025684 107.789474-107.789474 107.789474z" fill="#231F20"></path></symbol><symbol id="icon-boar" viewBox="0 0 1024 1024"><path d="M732.079158 377.263158c-107.789474 0-186.421895 31.393684-281.869474 126.841263L180.331789 773.982316C257.724632 807.909053 348.725895 808.421053 485.052632 808.421053h96.013473c55.834947-34.411789 133.551158-53.894737 227.354948-53.894737h121.344L970.105263 680.555789V572.631579c0-94.315789-130.236632-195.368421-238.026105-195.368421z" fill="#FFBDD8"></path><path d="M808.421053 700.631579v53.894737c-196.446316 0-323.368421 84.641684-323.368421 215.578947h-53.894737c0-163.705263 148.075789-269.473684 377.263158-269.473684z m-323.368421 107.789474v-53.894737c-158.342737 0-245.598316 0-319.649685-49.367579L158.612211 700.631579H80.842105c-21.692632 0-26.624-14.821053-26.947368-26.947368v-82.620632c84.156632-11.183158 161.684211-74.913684 161.68421-186.853053V215.578947H161.684211v161.684211H134.736842c-66.964211 0-134.736842 37.025684-134.736842 107.789474h53.894737c0-42.630737 52.870737-53.894737 80.842105-53.894737h24.629895C147.132632 504.912842 85.153684 538.947368 26.947368 538.947368H0v134.736843c0 32.498526 21.530947 80.842105 80.842105 80.842105h61.682527c32.687158 20.506947 67.125895 33.145263 105.957052 41.013895A232.879158 232.879158 0 0 0 215.578947 916.210526h53.894737c0-41.930105 14.012632-80.303158 39.424-112.505263C358.885053 808.151579 415.959579 808.421053 485.052632 808.421053z m-72.946527-342.420211L323.368421 554.738526V431.157895h-53.894737v253.682526l180.736-180.736-38.103579-38.103579zM323.368421 161.684211h-53.894737v190.032842a769.536 769.536 0 0 1 53.894737-49.098106V161.684211z m323.368421-53.894737c-72.623158 0-146.809263 23.336421-215.578947 58.637473V107.789474h-53.894737v154.138947C458.832842 205.392842 555.331368 161.684211 646.736842 161.684211c148.587789 0 269.473684 120.885895 269.473684 269.473684v235.654737L809.579789 862.315789h61.359158L970.105263 680.555789V431.157895c0-178.310737-145.057684-323.368421-323.368421-323.368421z" fill="#231F20"></path></symbol><symbol id="icon-boar_hai" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#85C3DE"></path><path d="M309.975579 804.756211l-27.136-46.592c103.073684-60.011789 183.026526-132.473263 241.475368-219.24379H350.315789l-13.473684-50.283789c58.88-33.980632 99.435789-117.571368 118.703158-165.295158H242.526316v-53.894737h538.947368v53.894737h-268.18021c-12.395789 34.088421-42.469053 106.603789-90.435369 161.68421h134.009263a680.555789 680.555789 0 0 0 46.349474-107.708631l51.092211 17.057684c-58.421895 175.265684-171.034947 309.490526-344.333474 410.381474z m192.350316-2.937264L467.806316 760.454737c88.414316-73.728 154.516211-158.773895 202.105263-259.907369l48.801684 22.959158a797.372632 797.372632 0 0 1-82.351158 137.781895c32.741053 15.009684 83.456 44.867368 137.647158 101.591579l-38.938947 37.268211c-57.236211-59.877053-109.325474-85.557895-133.766737-95.178106a850.997895 850.997895 0 0 1-98.977684 96.848842z m48.613052-536.872421l-80.842105-53.894737 29.884632-44.840421 80.842105 53.894737-29.884632 44.840421zM512 53.894737C259.395368 53.894737 53.894737 259.395368 53.894737 512s205.500632 458.105263 458.105263 458.105263c9.081263 0 17.973895-0.835368 26.947368-1.374316v-53.894736c-8.946526 0.619789-17.866105 1.374316-26.947368 1.374315-222.881684 0-404.210526-181.328842-404.210526-404.210526S289.118316 107.789474 512 107.789474s404.210526 181.328842 404.210526 404.210526c0 195.206737-139.075368 358.507789-323.368421 396.045474v54.460631c214.096842-38.346105 377.263158-225.549474 377.263158-450.533052C970.105263 259.395368 764.604632 53.894737 512 53.894737z" fill="#231F20"></path></symbol><symbol id="icon-bilibili1" viewBox="0 0 1129 1024"><path d="M234.909 9.656a80.468 80.468 0 0 1 68.398 0 167.374 167.374 0 0 1 41.843 30.578l160.937 140.82h115.07l160.936-140.82a168.983 168.983 0 0 1 41.843-30.578A80.468 80.468 0 0 1 930.96 76.445a80.468 80.468 0 0 1-17.703 53.914 449.818 449.818 0 0 1-35.406 32.187 232.553 232.553 0 0 1-22.531 18.508h100.585a170.593 170.593 0 0 1 118.289 53.109 171.397 171.397 0 0 1 53.914 118.288v462.693a325.897 325.897 0 0 1-4.024 70.007 178.64 178.64 0 0 1-80.468 112.656 173.007 173.007 0 0 1-92.539 25.75H212.377a341.186 341.186 0 0 1-72.421-4.024A177.835 177.835 0 0 1 28.91 939.065a172.202 172.202 0 0 1-27.36-92.539V388.662a360.498 360.498 0 0 1 0-66.789A177.03 177.03 0 0 1 162.487 178.64h105.414c-16.899-12.07-31.383-26.555-46.672-39.43a80.468 80.468 0 0 1-25.75-65.984 80.468 80.468 0 0 1 39.43-63.57M216.4 321.873a80.468 80.468 0 0 0-63.57 57.937 108.632 108.632 0 0 0 0 30.578v380.615a80.468 80.468 0 0 0 55.523 80.469 106.218 106.218 0 0 0 34.601 5.632h654.208a80.468 80.468 0 0 0 76.444-47.476 112.656 112.656 0 0 0 8.047-53.109v-354.06a135.187 135.187 0 0 0 0-38.625 80.468 80.468 0 0 0-52.304-54.719 129.554 129.554 0 0 0-49.89-7.242H254.22a268.764 268.764 0 0 0-37.82 0z m0 0" fill="#20B0E3"></path><path d="M348.369 447.404a80.468 80.468 0 0 1 55.523 18.507 80.468 80.468 0 0 1 28.164 59.547v80.468a80.468 80.468 0 0 1-16.094 51.5 80.468 80.468 0 0 1-131.968-9.656 104.609 104.609 0 0 1-10.46-54.719v-80.468a80.468 80.468 0 0 1 70.007-67.593z m416.02 0a80.468 80.468 0 0 1 86.102 75.64v80.468a94.148 94.148 0 0 1-12.07 53.11 80.468 80.468 0 0 1-132.773 0 95.757 95.757 0 0 1-12.875-57.133V519.02a80.468 80.468 0 0 1 70.007-70.812z m0 0" fill="#20B0E3"></path></symbol><symbol id="icon-yinle" viewBox="0 0 1024 1024"><path d="M512.2976 0a531.2 531.2 0 0 0-512 548.48V960h128V548.48a398.72 398.72 0 0 1 384-411.52 398.72 398.72 0 0 1 384 411.52V960h128V548.48A531.2 531.2 0 0 0 512.2976 0z" fill="#5c8add"></path><path d="M64.2976 576l256 0 0 448-256 0 0-448Z" fill="#5c8add"></path><path d="M704.2976 576l256 0 0 448-256 0 0-448Z" fill="#5c8add"></path></symbol><symbol id="icon-icon-test-copy" viewBox="0 0 1024 1024"><path d="M512 512m-229.517241 0a229.517241 229.517241 0 1 0 459.034482 0 229.517241 229.517241 0 1 0-459.034482 0Z" fill="#5c8add"></path><path d="M512 1024A512 512 0 1 1 1024 512 512 512 0 0 1 512 1024z m0-141.241379A370.758621 370.758621 0 1 0 141.241379 512 370.758621 370.758621 0 0 0 512 882.758621z" fill="#5c8add"></path></symbol><symbol id="icon-V" viewBox="0 0 1024 1024"><path d="M1012.47774251 492.58192592L544.94137566 87.22962963a49.96686561 49.96686561 0 0 0-65.88275132 0L11.63784127 492.6975097c-21.03624691 18.26223633-23.3479224 49.93219048-5.08568606 70.96843739 18.03106878 21.03624691 49.93219048 23.3479224 70.96843738 5.08568607L512 191.83294532l434.71057495 376.91868784c9.47786949 8.20644797 21.26741446 12.25188008 32.82579189 12.13629629 14.10122046 0 27.97127337-5.77918871 38.02706173-17.33756613 18.14665256-20.92066314 15.95056084-52.70620106-5.08568606-70.9684374z" fill="#5c8add"></path><path d="M109.30613051 567.59579541V896.89396825c0 42.53482892 34.90629982 77.44112875 77.44112875 77.44112875h220.76500882V666.30433862c0-25.54401411 20.92066314-46.46467725 46.46467724-46.46467724h116.16169313c25.54401411 0 46.46467725 20.92066314 46.46467725 46.46467724V974.335097h220.76500882c42.53482892 0 77.44112875-34.90629982 77.44112874-77.44112875l0.11558377-329.29817284L512 218.18604586 109.30613051 567.59579541zM848.00203175 197.49655027h-63.91782716c-12.82979894 0-23.23233862 10.40253968-23.23233863 23.23233862v24.27259259l110.49808818 95.70336508V220.72888889h-0.11558377c0-12.82979894-10.40253968-23.23233862-23.23233862-23.23233862zM905.44716754 83.18419754s-34.90629982 56.86721693-89.11508994 100.32671603c152.68616579 13.98563668 127.83565432-133.26809171 127.83565432-133.2680917-134.07717813-10.28695591-132.92134039 102.29164021-131.072 127.83565432 20.92066314-20.92066314 49.70102293-62.64640564 92.35143562-94.89427865zM798.53217637 174.61096297c-19.64924162-16.52847972-40.56990476-43.45949912-51.203612-53.97762258 0 0 32.94137566 20.57391182 56.40488184 49.3542716 2.42725926-18.37782011 6.47269135-93.3916896-93.16052205-85.3008254 0 0-13.98563668 104.71889947 87.95925221 89.92417638z" fill="#5c8add"></path></symbol><symbol id="icon-zhifeiji" viewBox="0 0 1167 1024"><path d="M41.201759 463.52493L1110.665064 30.117647c10.32605-4.159104 21.942857 0.860504 26.101961 11.043137 1.434174 3.728852 1.864426 7.744538 1.003921 11.616807L949.033691 978.823529c-2.151261 10.89972-12.764146 17.927171-23.663865 15.632493-2.72493-0.573669-5.306443-1.721008-7.601121-3.298599L634.80624 789.79944l-163.065546 133.951821c-16.492997 13.62465-40.87395 11.186555-54.498599-5.306443-3.011765-3.728852-5.306443-7.887955-6.884034-12.477311l-102.973669-313.080112-265.178712-91.787115c-10.469468-3.585434-16.062745-15.058824-12.333893-25.528291 1.864426-5.44986 6.023529-9.895798 11.329972-12.047059z" fill="#FCFDFC"></path><path d="M929.385512 1023.569748c-3.155182 0-6.453782-0.286835-9.752381-1.003922-6.740616-1.434174-12.907563-4.015686-18.50084-8.031372L635.953579 825.940616l-146.142297 120.040336c-13.911485 11.473389-31.408403 16.779832-49.335574 15.058824-17.927171-1.721008-34.133333-10.32605-45.463305-24.237535-5.306443-6.453782-9.322129-13.768067-11.903642-21.79944l-98.527731-299.598879-251.697479-87.19776c-12.333894-4.302521-22.229692-13.05098-27.966386-24.811204s-6.453782-24.954622-2.151261-37.288515c4.589356-13.337815 14.771989-23.9507 27.82297-29.257143L1099.908761 3.585434c24.954622-10.039216 53.351261 2.007843 63.533894 26.819048 3.585434 8.891877 4.445938 18.644258 2.581513 28.109804L977.143495 984.560224c-4.732773 23.090196-25.098039 39.009524-47.757983 39.009524z m-294.579272-233.770308l282.962465 201.357983c2.294678 1.577591 4.87619 2.72493 7.601121 3.298599 10.89972 2.151261 21.512605-4.87619 23.663865-15.632493L1137.914364 52.777591c0.860504-3.872269 0.430252-7.887955-1.003922-11.616807-4.159104-10.32605-15.919328-15.202241-26.101961-11.043137L41.201759 463.52493c-5.306443 2.151261-9.465546 6.597199-11.47339 12.047059-1.721008 5.019608-1.434174 10.469468 0.860505 15.345658 2.294678 4.87619 6.453782 8.461625 11.473389 10.182633l265.178711 91.787115L410.214644 905.967507c1.434174 4.589356 3.872269 8.748459 6.884033 12.477311 6.597199 8.031373 15.919328 12.907563 26.101961 13.911485 10.32605 1.003922 20.365266-2.007843 28.396639-8.605042l163.208963-133.951821z" fill="#4A4A4A"></path><path d="M307.097557 592.743978l105.698599 316.091876c6.310364 18.787675 26.532213 28.970308 45.319888 22.659944 4.159104-1.434174 7.887955-3.442017 11.186555-6.166946l164.786555-133.951821-165.360224-118.892997c297.017367-287.982073 447.462185-433.980952 451.191036-437.853222 0.573669-0.573669 2.581513-3.442017 0.430252-7.027451-1.290756-1.577591-3.298599-3.298599-7.027451-2.15126-202.218487 120.327171-404.293557 242.805602-606.22521 367.291877z" fill="#CAE0EE"></path><path d="M446.786072 934.794398c-5.736695 0-11.329972-1.290756-16.636414-3.872269-8.891877-4.445938-15.632493-12.047059-18.787675-21.512605L305.376549 592.313725l1.003921-0.573669C507.308201 467.684034 711.391114 344.058263 912.60568 224.161345l0.286835-0.143418c3.585434-1.147339 6.310364-0.286835 8.605042 2.581513l0.143417 0.143417c2.438095 4.015686 0.573669 7.457703-0.573669 8.74846-3.872269 4.015686-155.177591 150.87507-450.043698 436.705882l165.503642 119.036414-166.220728 135.09916c-3.442017 2.868347-7.457703 5.019608-11.760225 6.453782-3.728852 1.290756-7.744538 2.007843-11.760224 2.007843z m-137.967507-341.333334l105.268348 314.944538c2.868347 8.748459 9.035294 15.77591 17.210084 19.935014 8.17479 4.159104 17.496919 4.732773 26.245378 1.864426 3.872269-1.290756 7.60112-3.298599 10.756302-5.880112l163.352381-132.804482L466.434252 672.627451l1.290756-1.147339C763.308201 384.932213 915.043775 237.642577 918.772627 233.626891c0 0 2.007843-2.294678 0.286835-5.306443-1.003922-1.290756-2.438095-2.438095-5.306443-1.577591-200.784314 119.610084-404.293557 242.94902-604.934454 366.718207z" fill="#CAE0EE"></path><path d="M460.840974 924.898599l7.457703-253.561904 165.933894 119.896918-168.658824 135.959664c-1.290756 1.003922-3.011765 0.860504-4.015686-0.430252-0.430252-0.430252-0.717087-1.147339-0.717087-1.864426z" fill="#94C3E2"></path><path d="M463.709322 929.344538c-1.290756 0-2.438095-0.573669-3.2986-1.577591-0.573669-0.860504-1.003922-1.864426-1.003921-2.868348l7.60112-256.286834 169.519328 122.621848-1.434174 1.147339-168.658823 135.959664c-0.860504 0.717087-1.721008 1.003922-2.72493 1.003922z m6.023529-255.282913l-7.457703 250.836974c0 0.286835 0.143417 0.717087 0.286835 1.003922 0.430252 0.573669 1.434174 0.717087 2.007843 0.286835l167.22465-134.812325-162.061625-117.315406z" fill="#94C3E2"></path></symbol><symbol id="icon-lianjie" viewBox="0 0 1079 1024"><path d="M695.355535 432.666896c-0.553495-1.10699-0.885592-2.186305-1.383737-3.265619-0.193723-0.193723-0.193723-0.359772-0.359771-0.719543-12.508983-26.318678-39.436506-43.366319-69.325226-41.013966-39.076734 3.265619-68.439634 39.021384-65.312388 79.841627 0.857917 10.516401 3.653066 20.147211 7.998 28.83708 19.78744 46.659613 11.097571 103.448181-25.377737 141.750022l-191.094085 199.950001a118.088119 118.088119 0 0 1-171.998513 0c-47.434506-49.537786-47.434506-130.098956 0-179.636742l71.234782-74.389703-0.52582-0.553494a75.911814 75.911814 0 0 0 24.326097-61.880721c-3.127246-40.820243-37.3609-71.51153-76.437634-68.24591a69.463599 69.463599 0 0 0-46.908685 23.966325l-0.166049-0.193723-72.618519 75.856464c-103.226783 107.793115-103.226783 282.36538 0 390.158495 103.171433 107.793115 270.299193 107.793115 373.498301 0l191.619904-200.1714c80.256748-83.992838 97.636485-208.307773 52.83108-310.289193z" fill="#5c8add"></path><path d="M1002.047012 80.865592c-103.226783-107.82079-270.382217-107.82079-373.581325 0l-191.619905 200.199075c-80.284423 83.854464-97.66416 208.197074-52.997128 310.233843 0.52582 1.079315 0.857917 2.15863 1.383737 3.26562 0.166048 0.166048 0.166048 0.359772 0.332097 0.719543 12.536658 26.291004 39.46418 43.366319 69.3529 41.013966 39.076734-3.265619 68.439634-39.021384 65.312388-79.869302a78.679288 78.679288 0 0 0-7.998-28.864755c-19.78744-46.631938-11.097571-103.448181 25.377737-141.750022l191.287808-199.839302a118.088119 118.088119 0 0 1 172.026188 0c47.434506 49.537786 47.434506 130.126631 0 179.692091l-71.234782 74.417378 0.52582 0.553495a75.939489 75.939489 0 0 0-24.353772 61.88072c3.15492 40.847917 37.3609 71.51153 76.465309 68.245911a69.463599 69.463599 0 0 0 46.908685-23.938651l0.166049 0.166048 72.646194-75.856464c103.03306-107.82079 103.03306-282.642127 0-390.269194z" fill="#5c8add"></path></symbol><symbol id="icon-liaotian" viewBox="0 0 1171 1024"><path d="M1068.71699 0.243751H102.193768C46.228437 0.243751 0.500666 45.045267 0.500666 99.74309v696.251622c0 54.697824 45.727771 99.450589 101.693102 99.450589h329.113198l120.851966 114.465677a48.652788 48.652788 0 0 0 66.641644 0l120.851966-114.465677h329.064448c55.965331 0 101.741852-44.752765 101.741852-99.450589V99.74309C1170.458842 45.045267 1124.682321 0.243751 1068.71699 0.243751z m-439.776354 596.849784h-370.989696c-27.933915 0-50.846551-22.425133-50.846551-49.774045 0-27.348912 22.912636-49.725294 50.846551-49.725294h370.989696c27.933915 0 50.846551 22.376382 50.846551 49.725294 0 27.348912-22.912636 49.774045-50.846551 49.774045z m287.18795-211.381252H254.782171a50.456549 50.456549 0 0 1-50.846551-49.725294c0-27.397662 22.912636-49.774045 50.846551-49.774045h661.346415c27.933915 0 50.846551 22.376382 50.846551 49.774045 0 27.348912-22.912636 49.725294-50.846551 49.725294z" fill="#5C8ADD"></path></symbol><symbol id="icon-xinfeng" viewBox="0 0 1400 1024"><path d="M1301.63733163 214.78520234a207.81921797 207.81921797 0 0 1 7.02423018 52.42036465v489.73590176a205.10753818 205.10753818 0 0 1-205.05853125 205.05853125H283.05853124A205.15654424 205.15654424 0 0 1 77.99999999 756.79444971V267.20556699a201.36672685 201.36672685 0 0 1 7.02423106-52.42036465L586.24393329 562.1905874c69.44187217 51.96297217 146.36536612 49.13694404 214.1736961 0zM1103.60303056 62.0000167H283.05853124A204.50312753 204.50312753 0 0 0 106.37462518 163.41030547l489.71956641 335.75823018c62.43397646 50.77048623 127.85733457 50.31309463 194.62019765 0L1280.28693749 163.41030547A204.68281729 204.68281729 0 0 0 1103.60303056 62.0000167z m0 0" fill="#5c8add"></path></symbol><symbol id="icon-QQ1" viewBox="0 0 1024 1024"><path d="M0 512a512 512 0 1 0 1024 0A512 512 0 1 0 0 512z" fill="#18ACFC"></path><path d="M500.113 228.39c118.396-1.518 178.924 61.004 201 156 3.497 15.048 0.15 34.807 0 50 27.143 5.682 33.087 60.106 10 75v1h1c8.26 14.33 19.04 28.125 26 44 7.332 16.723 9.306 35.16 14 55 4.024 17.01-2.287 51.505-10 57-0.771 0.683-2.231 1.312-3 2-14.601-3.016-30.377-16.865-38-27-3.065-4.074-5.275-9.672-10-12-0.395 21.568-12.503 41.15-22 55-3.514 5.123-14.073 13.217-14 18 3.691 2.836 8.305 2.956 13 5 10.513 4.577 25.449 13.168 32 22 2.334 3.146 5.548 7.555 7 11 16.193 38.414-36.527 48.314-63 54-27.185 5.839-77.818-10.224-92-19-8.749-5.414-16.863-18.573-29-19-3.666 2.389-14.438 1.132-20 1-16.829 32.804-101.913 47.868-148 31-14.061-5.146-43.398-17.695-38-40 4.437-18.327 19.947-29.224 35-37 5.759-2.975 18.915-4.419 22-10-13.141-8.988-24.521-28.659-31-44-3.412-8.077-4.193-25.775-9-32-7.789 12.245-32.097 36.91-52 33-3.071-4.553-7.213-9.097-9-15-4.792-15.835-1.81-40.379 2-54 8.117-29.02 16.965-50.623 32-72 4.672-6.643 11.425-12.135 16-19-8.945-9.733-6.951-37.536-1-49 4.002-7.709 9.701-7.413 10-20-1.92-3.022-0.071-8.604-1-13-4.383-20.75 3.273-47.552 9-63 19.8-53.421 53.712-90.466 105-112 11.986-5.033 25.833-7.783 39-11 5.322-1.3 11.969 0.518 16-2z" fill="#FFFFFF"></path></symbol><symbol id="icon-rss" viewBox="0 0 1024 1024"><path d="M749.61196492 908.06119793C749.61196492 560.41848146 463.58151854 274.36328126 115.93880207 274.36328126V115.93880207c434.50388795 0 792.12239584 357.61850789 792.12239586 792.12239586zM224.55858562 690.72261555a108.91682943 108.91682943 0 0 1 108.69404499 108.74355267C333.25263061 859.29616292 284.24005737 908.06119793 224.31104736 908.06119793 164.48105265 908.06119793 115.96355592 859.41993206 115.96355592 799.46616822s48.69077351-108.71879883 108.61978351-108.74355267zM641.01693522 908.06119793h-153.96879069c0-203.60020956-167.50913289-371.13409627-371.10934246-371.13409629v-153.96879068c288.03550619 0 525.07813313 237.11688843 525.07813315 525.10288697z" fill="#FFA500"></path></symbol><symbol id="icon-youxiang" viewBox="0 0 1024 1024"><path d="M583.60666667 972h-68.08c-8.43333333 0-15.33333333-6.9-15.33333334-15.33333333V609.52c0-8.43333333 6.9-15.33333333 15.33333334-15.33333333h68.08c8.43333333 0 15.33333333 6.9 15.33333333 15.33333333V956.66666667c0 8.43333333-6.9 15.33333333-15.33333333 15.33333333z" fill="#629FF9"></path><path d="M294.42 167c-113.62 0-205.77333333 92-205.77333333 205.31333333v336.72h411.39333333V372.31333333c0.15333333-113.31333333-92-205.31333333-205.62-205.31333333z" fill="#2166CC"></path><path d="M519.97333333 627H216.98666667c-25.45333333 0-46-20.54666667-46-46V393.78c0-25.45333333 20.54666667-46 46-46h302.98666666c25.45333333 0 46 20.54666667 46 46V581c0 25.45333333-20.54666667 46-46 46z" fill="#D2E4FF"></path><path d="M565.97333333 397a49.22 49.22 0 0 0-49.37333333-49.22H220.36c-27.29333333 0-49.37333333 22.08-49.37333333 49.22v10.27333333l179.4 94.60666667c11.34666667 5.98 24.84 5.98 36.18666666 0l179.4-94.60666667v-10.27333333z" fill="#FFFFFF"></path><path d="M730.5 167h-427.8v0.46c109.78666667 4.29333333 197.49333333 94.3 197.49333333 205.00666667v336.72h411.39333334c27.29333333 0 49.37333333-22.08 49.37333333-49.22V397c0-126.96-103.19333333-230-230.46-230z" fill="#4E8DF6"></path><path d="M845.80666667 52H681.12666667c-9.04666667 0-16.40666667 7.36-16.40666667 16.40666667v336.72a24.67133333 24.67133333 0 1 0 49.37333333 0V134.18666667h131.71333334c9.04666667 0 16.40666667-7.36 16.40666666-16.40666667V68.40666667c0-9.04666667-7.36-16.40666667-16.40666666-16.40666667z" fill="#2166CC"></path><path d="M896.25333333 659.81333333h-35.11333333c-8.43333333 0-15.33333333-6.9-15.33333333-15.33333333v-35.11333333c0-8.43333333 6.9-15.33333333 15.33333333-15.33333334h35.11333333c8.43333333 0 15.33333333 6.9 15.33333334 15.33333334v35.11333333c0 8.58666667-6.9 15.33333333-15.33333334 15.33333333z" fill="#FFFFFF"></path><path d="M88.8 709.18666667l-24.22666667 131.40666666c-9.66 54.43333333 26.83333333 98.59333333 81.26666667 98.59333334h213.9c54.58666667 0 106.56666667-44.16 116.22666667-98.59333334l23.15333333-131.40666666H88.8z" fill="#2974CE"></path></symbol><symbol id="icon-gitHub" viewBox="0 0 1049 1024"><path d="M523.6581816 52C262.83923907 52 52 262.8401375 52 523.6581816c0 208.49703047 135.09433812 384.97758117 322.50789391 447.44906532 23.42658172 4.68531653 32.01647887-10.15136894 32.01647796-22.64584583 0-10.93210574-0.78163433-48.41463703-0.78163433-87.45953855-131.18885996 28.11189824-158.5200223-56.22379738-158.52002231-56.22379739-21.08437312-54.66232469-52.3201152-68.71827336-52.3201152-68.71827335-42.94858371-28.89353348 3.12384382-28.89353348 3.12384384-28.89353348 47.63479867 3.12384382 72.62285398 48.41643391 72.62285398 48.4164339 42.16784782 71.84121875 110.10538527 51.53758242 137.43654672 39.04400399 3.90457972-30.45500618 16.3990566-51.5393793 29.67427028-63.25222094-104.64023039-10.93300418-214.74561566-51.53848086-214.74561657-232.70524742 0-51.53848086 18.74126609-93.70632867 48.4164339-126.50444187-4.68621496-11.71284164-21.08527156-60.12837711 4.6844181-124.94207075 0 0 39.82563922-12.49447688 129.62738726 48.41463704 37.48253129-10.15136894 78.08980484-15.61742227 117.91454562-15.61742137s80.43201433 5.46605242 117.91454473 15.61742137c89.80264648-60.90911391 129.62828571-48.41463703 129.62828571-48.41463704 25.76879122 64.81369363 9.37063305 113.22922911 4.68531651 124.94207075 30.45410773 32.79721477 48.41463703 74.96506258 48.41463703 126.50444187 0 181.16676656-110.10538527 220.99150644-215.52545401 232.70524742 17.1797934 14.83668547 32.01647887 42.94858371 32.01647886 87.45953946 0 63.25222094-0.78163433 114.009965-0.78163523 129.62738636 0 12.49447688 8.59079468 27.33116234 32.01737731 22.64584583 187.41265734-62.4705866 322.50699547-238.95203574 322.50699546-447.44996375C995.31636231 262.8401375 783.69369203 52 523.6581816 52z" fill="#663399"></path><path d="M230.82365863 729.03136735c-0.7807359 2.34310703-4.68531653 3.12384382-7.80916035 1.56237113s-5.46605242-4.68531653-3.90368129-7.02842356c0.7807359-2.34220859 4.68531653-3.12384382 7.80826192-1.56147269s4.68531653 4.68531653 3.90457972 7.02752512z m18.7412661 21.08437312c-2.34220859 2.34220859-7.02752512 0.78163433-9.37063305-2.34310703-3.12294539-3.12294539-3.90457972-7.80826192-1.5614727-10.15136894 2.34220859-2.34220859 6.24678922-0.7807359 9.37063305 2.34310702 3.12384382 3.90457972 3.90457972 8.58899782 1.5614727 10.15136895zM268.30618992 777.44690281c-3.12294539 2.34220859-7.80826192 0-10.15136895-3.90457972-3.12384382-3.90457972-3.12384382-9.37063305 0-10.93210574 3.12384382-2.34310703 7.80916035 0 10.15226739 3.90457972 3.12294539 3.90368129 3.12294539 8.58899782 0 10.93210574z m25.76968965 26.55042555c-2.34220859 3.12294539-7.80916035 2.34220859-12.49447688-1.56237113-3.90457972-3.90368129-5.46605242-9.37063305-2.34220859-11.71284164 2.34220859-3.12384382 7.80826192-2.34310703 12.49447687 1.56147269 3.90368129 3.12384382 4.68531653 8.58989625 2.3422086 11.71374008z m35.1403227 14.83668637c-0.78163433 3.90457972-6.24768766 5.46605242-11.71374008 3.90457972-5.46605242-1.5614727-8.58899782-6.24768766-7.80916036-9.37063305 0.78163433-3.90457972 6.24768766-5.46605242 11.71374009-3.90457972 5.46605242 1.5614727 8.58899782 5.46605242 7.80916035 9.37063305z m38.26416562 3.12384382c0 3.90457972-4.68621496 7.02752512-10.15226738 7.02752512-5.46605242 0-10.15226738-3.12294539-10.15226739-7.02752512s4.68621496-7.02842356 10.15226739-7.02842445c5.46605242 0 10.15226738 3.12384382 10.15226738 7.02842445z m35.92016106-6.24768766c0.78163433 3.90457972-3.12384382 7.80916035-8.58899872 8.58989625-5.46695086 0.78163433-10.15226738-1.5614727-10.93390172-5.46605241-0.77983747-3.90457972 3.12384382-7.80916035 8.5907947-8.58899872 5.46605242-0.78163433 10.15136894 1.56057426 10.93210574 5.46515488z m0 0" fill="#663399"></path></symbol><symbol id="icon-bilibili" viewBox="0 0 1024 1024"><path d="M832.61667555 181.33447111h-164.32545185l74.45617778-74.45617778c12.84020148-12.84020148 12.84020148-30.8140563 0-43.65425778-12.84020148-12.84020148-30.8140563-12.84020148-43.65425778 0L573.2882963 189.04101925H450.04420741L324.2272237 63.23617185c-10.26730667-12.84020148-25.68040297-15.40096-41.08136295-7.70654815-2.57289482 0-2.57289482 2.57289482-5.13365334 5.13365333-12.84020148 12.84020148-12.84020148 30.8140563 0 43.65425779l77.02907259 77.02907259h-164.32545185c-89.86927408 0-164.32545185 74.45617778-164.32545185 164.32545184v408.24073483c0 87.29637925 74.45617778 161.75255703 164.32545185 161.75255703h25.68040296c0 30.8140563 25.68040297 53.92156445 53.92156444 53.92156444s53.92156445-25.68040297 53.92156445-53.92156444H704.23893333c2.57289482 30.8140563 28.24116148 53.92156445 59.05521778 51.34866964 28.24116148-2.57289482 48.78791111-23.10750815 51.34866964-51.34866964h20.53461333c89.86927408 0 164.32545185-74.45617778 164.32545184-164.32545186V343.09916445c-2.56075852-89.86927408-77.02907259-161.76469333-166.88621037-161.76469334z m-5.13365333 634.19429926H200.99527111c-33.37481482 0-59.05521778-28.24116148-61.61597629-61.61597629l-2.57289482-415.94728297c0-33.37481482 28.24116148-61.6159763 61.6159763-61.61597629h626.48775111c33.37481482 0 59.05521778 28.24116148 61.61597629 61.61597629l2.57289482 415.94728297c-2.57289482 35.93557333-28.24116148 61.6159763-61.6159763 61.61597629z" fill="#ff7299"></path><path d="M403.82919111 417.55534222l15.40096 77.0290726-205.40681481 38.50846815-15.40096-77.0290726 205.40681481-38.50846815z m197.70026667 77.0290726l15.40096-77.0290726 205.40681481 38.50846815-15.40096 77.0290726-205.40681481-38.50846815z m41.08136297 161.75255703c0 2.57289482 0 7.70654815-2.57289483 10.26730667-12.84020148 28.24116148-41.08136297 46.2150163-74.45617777 48.78791111-20.53461333 0-41.08136297-10.26730667-53.92156445-25.68040296-15.40096 15.40096-33.37481482 25.68040297-53.92156445 25.68040296-30.8140563-2.57289482-59.05521778-20.53461333-74.45617777-48.78791111 0-2.57289482-2.57289482-5.13365333-2.57289481-10.26730667 0-10.26730667 7.70654815-17.97385482 17.97385481-20.53461333h2.57289482c7.70654815 0 12.84020148 2.57289482 15.40096 10.26730666 0 0 20.53461333 28.24116148 38.50846815 28.24116149 35.94770963 0 35.94770963-30.8140563 56.48232296-53.92156445 23.10750815 25.68040297 23.10750815 53.92156445 56.48232296 53.92156445 23.10750815 0 38.50846815-28.24116148 38.50846815-28.24116149 2.57289482-5.13365333 10.26730667-10.26730667 15.40096-10.26730666 10.26730667-2.57289482 17.97385482 5.13365333 20.53461333 15.40096v5.13365333h0.0364089z" fill="#ff7299"></path></symbol></svg>',    o = (o = document.getElementsByTagName("script"))[o.length - 1].getAttribute("data-injectcss"),    p = function (c, l) {      l.parentNode.insertBefore(c, l);    };  if (o &amp;&amp; !c.__iconfont__svg__cssinject__) {    c.__iconfont__svg__cssinject__ = !0;    try {      document.write(        "<style>.svgfont {display: inline-block;width: 1em;height: 1em;fill: currentColor;vertical-align: -0.1em;font-size:16px;}</style>"      );    } catch (c) {      console &amp;&amp; console.log(c);    }  }  function d() {    i || ((i = !0), a());  }  function m() {    try {      t.documentElement.doScroll("left");    } catch (c) {      return void setTimeout(m, 50);    }    d();  }  (l = function () {    var c,      l = document.createElement("div");    (l.innerHTML = v),      (v = null),      (l = l.getElementsByTagName("svg")[0]) &amp;&amp;        (l.setAttribute("aria-hidden", "true"),        (l.style.position = "absolute"),        (l.style.width = 0),        (l.style.height = 0),        (l.style.overflow = "hidden"),        (l = l),        (c = document.body).firstChild ? p(l, c.firstChild) : c.appendChild(l));  }),    document.addEventListener      ? ~["complete", "loaded", "interactive"].indexOf(document.readyState)        ? setTimeout(l, 0)        : ((h = function () {            document.removeEventListener("DOMContentLoaded", h, !1), l();          }),          document.addEventListener("DOMContentLoaded", h, !1))      : document.attachEvent &amp;&amp;        ((a = l),        (t = c.document),        (i = !1),        m(),        (t.onreadystatechange = function () {          "complete" == t.readyState &amp;&amp; ((t.onreadystatechange = null), d());        }));})(window);]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>友情链接</title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>标签</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
