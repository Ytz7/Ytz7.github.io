<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>七号zz の Blog</title>
  
  <subtitle>愿为未来做牛马。</subtitle>
  <link href="https://ytz7.github.io/atom.xml" rel="self"/>
  
  <link href="https://ytz7.github.io/"/>
  <updated>2023-05-11T13:20:37.582Z</updated>
  <id>https://ytz7.github.io/</id>
  
  <author>
    <name>七号zz</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="https://ytz7.github.io/posts/16107.html"/>
    <id>https://ytz7.github.io/posts/16107.html</id>
    <published>2023-05-11T13:20:37.582Z</published>
    <updated>2023-05-11T13:20:37.582Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>deep-learning-some-questions</title>
    <link href="https://ytz7.github.io/posts/b6e97dad.html"/>
    <id>https://ytz7.github.io/posts/b6e97dad.html</id>
    <published>2023-05-11T11:47:01.000Z</published>
    <updated>2023-05-11T13:20:37.582Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于深度学习的一些问题与知识盲区"><a href="#关于深度学习的一些问题与知识盲区" class="headerlink" title="关于深度学习的一些问题与知识盲区"></a>关于深度学习的一些问题与知识盲区</h1><p>此篇用于记录在阅读论文以及博客时遇到的一些问题和知识盲区，方便日后进行复盘。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="为什么需要激活函数？"><a href="#为什么需要激活函数？" class="headerlink" title="为什么需要激活函数？"></a><strong>为什么需要激活函数？</strong></h3><p>通常激活函数都是非线性的，它能够帮助我们引入非线性因素，使得神经网络能够更好地解决更加复杂地问题，在简单的二分类问题中，如果不使用激活函数，使用简单的<strong><u>逻辑回归</u></strong>，那么该模型只能作简单的线性分类，而不能作复杂的非线性划分，如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511195806300.png" alt="image-20230511195806300"></p><p>值得一提的是，如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，效果与单个神经元无异。另外，如果是拟合问题而不是分类问题，输出层的激活函数可以使用线性函数。</p><h3 id="Sigmoid-函数饱和带来的问题"><a href="#Sigmoid-函数饱和带来的问题" class="headerlink" title="Sigmoid 函数饱和带来的问题"></a>Sigmoid 函数饱和带来的问题</h3><p>Sigmoid 函数的取值范围在 (0,1) 之间，单调连续，求导容易，一般用于二分类神经网络的输出层。</p><p>Sigmoid 函数饱和区范围广，容易造成梯度消失。饱和区如下图所示，图中红色椭圆标注的饱和区曲线平缓，梯度的值很小，近似为零，且 Sigmoid 函数的饱和区范围很广，除了 [-5,5]，其余区域都可以认为是饱和区，这种情况很容易造成梯度消失，梯度消失会增大神经网络训练难度，影响神经网络模型的性能。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511200333796.png" alt="image-20230511200333796" style="zoom:50%;"></p><h2 id="降噪自编码器-Denoising-Auto-Encoder"><a href="#降噪自编码器-Denoising-Auto-Encoder" class="headerlink" title="降噪自编码器 Denoising Auto-Encoder"></a>降噪自编码器 Denoising Auto-Encoder</h2><p>在神经网络模型训练阶段开始前，通过 Auto-encoder 对模型进行预训练可确定编码器 W 的初始参数值。然而，受模型复杂度、训练集数据量以及数据噪音等问题的影响，通过 Auto-encoder 得到的初始模型往往存在过拟合的风险。</p><p>简单理解，在人类的感知过程中，某些模态的信息对结果的判断影响并不大。举个例子，一块圆形的饼干和一块方形的饼干，在认知中同属于饼干这一类，因此形状对我们判断是否是饼干没有太大作用，也就是<strong>噪声</strong>。如果不能将形状数据去除掉，可能会产生“圆饼干是饼干，方饼干就不是饼干”的问题（过拟合）。</p><p>当采用无监督的方法分层预训练深度网络的权值时，为了学习到较鲁棒的特征，可以<strong><u>在网络的可视层（即数据的输入层）引入随机噪声</u></strong>，这种方法称为降噪自编码器（<a href="https://dl.acm.org/doi/abs/10.1145/1390156.1390294">Denoising Auto-Encoder， DAE</a>）。</p><blockquote><p>降噪自编码器：一个模型，能够从有噪音的原始数据作为输入，而能够恢复出真正的原始数据。这样的模型，更具有鲁棒性。</p></blockquote><p>以下是以经典的 MNIST 手写数字识别为例，对于输入的数据引入了变换角度、随机噪点、添加背景图像等噪音。模型通过训练后可以对有噪音图像更加鲁棒，而这也更符合实际使用的需求。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-2b0c0b5855f0c98b44fbeaf75d6a72dd_r.jpg" alt="img"></p><p>对于有噪音的输入数据，区别于一般自编码机，降噪自编码机要做的就是数据的降噪。关于降噪的过程如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-9ab89078a4630eb52347244a61410c2f_r.jpg" alt="img"></p><p>对于输入层 $x$，以一定概率将其节点置 0，得到 $\hat{x}$,用 $\hat{x}$ 去计算 $y$,计算 $z$ ，并将 $z$ 与原始 $x$ 做误差迭代，对结果误差较小的节点可以认为是噪声。每层处理重复上述工作。</p><p>自编码器的本质是学习一个相等函数，即网络的输入和重构后的输出相等，这种相等函数的表示有个缺点就是当测试样本和训练样本不符合同一分布，效果不好，而降噪自编码器在这方面的处理有所进步。</p><h2 id="随机深度-Stochastic-Depth"><a href="#随机深度-Stochastic-Depth" class="headerlink" title="随机深度 Stochastic Depth"></a>随机深度 Stochastic Depth</h2><p>针对于残差模块的优化，由于很深的 ResNet 通常需要很长时间的训练(也就是训练很慢)，作者引入了一种类似于 dropout 的方法，在训练过程中随机丢弃子图层（randomly drop a subset of layers），而在推断时正常使用完整的图。</p><p>ResNet 网络是由一个接一个的残差模块(ResBlock)串联起来的，可以视为ResBlock的集合。在训练时，对每个 ResBlock 随机 drop（按伯努利分布），drop 就是将上一个 ResBlock 直接输出到下一个 ResBlock，被 drop 的 ResBlock 什么都不做也不更新。另外，网络的输入被视为第一层，是不会 drop 的。</p><p>与 Dropout 的不同之处在于，该方法 drop 整个 ResBlock，而 Dropout 在训练期间只 drop 一部分神经元节点。这种方法大大降低了训练时间，甚至在训练完成后删除部分layer，还能不影响精度。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;关于深度学习的一些问题与知识盲区&quot;&gt;&lt;a href=&quot;#关于深度学习的一些问题与知识盲区&quot; class=&quot;headerlink&quot; title=&quot;关于深度学习的一些问题与知识盲区&quot;&gt;&lt;/a&gt;关于深度学习的一些问题与知识盲区&lt;/h1&gt;&lt;p&gt;此篇用于记录在阅读论文以及博</summary>
      
    
    
    
    <category term="深度学习" scheme="https://ytz7.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://ytz7.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器学习" scheme="https://ytz7.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>test-blog</title>
    <link href="https://ytz7.github.io/posts/467ec823.html"/>
    <id>https://ytz7.github.io/posts/467ec823.html</id>
    <published>2023-05-11T07:42:17.000Z</published>
    <updated>2023-05-11T13:20:37.582Z</updated>
    
    <content type="html"><![CDATA[<h1 id="这是一篇测试博客"><a href="#这是一篇测试博客" class="headerlink" title="这是一篇测试博客"></a>这是一篇测试博客</h1><p>修改一下在上传试试…</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;这是一篇测试博客&quot;&gt;&lt;a href=&quot;#这是一篇测试博客&quot; class=&quot;headerlink&quot; title=&quot;这是一篇测试博客&quot;&gt;&lt;/a&gt;这是一篇测试博客&lt;/h1&gt;&lt;p&gt;修改一下在上传试试…&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="test" scheme="https://ytz7.github.io/tags/test/"/>
    
  </entry>
  
  <entry>
    <title>半监督学习学习过程</title>
    <link href="https://ytz7.github.io/posts/52ee1a8c.html"/>
    <id>https://ytz7.github.io/posts/52ee1a8c.html</id>
    <published>2023-05-11T07:08:40.000Z</published>
    <updated>2023-05-11T13:20:37.582Z</updated>
    
    <content type="html"><![CDATA[<h1 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h1><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>学习了快一个学期的 Java 程序开发，让我感觉开发的过程真的好空洞…但是我并不知道下一步到底要做什么，又处于想找实习又怕找不到的情况下，刷了几天 LeetCode，然后又变成了“小🐏人”，紧接着被隔离😅，真的挺痛苦，所幸在第三天就“出狱”了，在蹲“监狱”期间，受到一个“狱友”的启发，“还是要做好职业规划的，若是没有做好职业规划，像一只无头苍蝇一样，很可能在毕业之际还是找不到工作。”在思考了一天后，我决定重新开始在“半监督学习”这个领域开始零基础学习了，为什么会有这个想法？是因为我觉得我起初考研的目的是为了能够在人工智能这个领域继续深入了解，目的很单纯，那为什么我不坚持以下呢？于是我决定重新开始这个计划，管他呢，起码是自己喜欢做的事情，哪怕未来找不到工作什么的，起码现在是由试错的资本的。</p><p>以下将会记录我在学习时候遇到的问题/解决方式/新思路/感想。</p><h2 id="1-Self-Training"><a href="#1-Self-Training" class="headerlink" title="1. Self-Training"></a>1. Self-Training</h2><p>自训练方法，模型基于已标记好的训练集进行训练，得到一个基础模型，利用该基础模型对未标记的数据集进行预测一个<strong>伪标签</strong>，然后将两个数据集整合训练，得到一个新的模型，从而迭代更新模型参数，生成一个最优模型。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161304502.png" alt="image-20230511161304502"></p><h3 id="1-1-Pseudo-Label"><a href="#1-1-Pseudo-Label" class="headerlink" title="1.1. Pseudo-Label"></a>1.1. <a href="https://github.com/emintham/Papers/blob/master/Lee- Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks.pdf">Pseudo-Label</a></h3><p>伪标签技术适用于小样本学习，实际上在样本极其珍贵的金融、医疗图像、安全等领域，伪标签学习有时候很有效。</p><p>伪标签的定义来自于半监督学习，半监督学习的核心思想是<u><strong>通过借助无标签的数据来提升有监督过程中的模型性能</strong></u>。</p><p>粗略来讲，伪标签技术就是<u><strong>利用在已标注数据所训练的模型在未标注的数据上进行预测，根据预测结果对样本进行筛选，再次输入模型中进行训练的一个过程</strong></u>。</p><p>如下图所示，利用有标签的数据集训练出一个模型，运用训练出的模型给予无标签数据一个<strong><u>伪标签</u></strong>。如何定义所属类别？利用训练好的模型对无标签数据进行预测，以概率最高的类别作为无标签数据的伪标签。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161425308.png" alt="image-20230511161425308"></p><blockquote><p>entropy regularization：用于防止模型过拟合，通过在损失函数中加入熵（entropy）项来实现</p></blockquote><p>利用 <em>entropy regularization</em> 思想，将无监督数据转为目标数据的正则项，即将拥有伪标签的无标签数据视为有标签的数据，利用交叉熵（与最初训练模型一致）来评估误差大小。</p><p>模型整体的目标函数如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-103a30551a86aa99222f8f35129088d7_r.jpg" alt="img"></p><p>其中左边一项为交叉熵，用来评估有标签数据的误差，右边一项即为 <em>entropy regularization</em> 项，用来从无标签的数据中获取训练信号。</p><p>为了平衡有标签数据和无标签数据的信号强度，如上所示，算法在目标函数中引入了时变参数 α(t)，其数学形式如下，其中 T1 和 T2 都为超参数：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-4d5a8a1d8aa4b9037859ab19dd346c0b_r.jpg" alt="img"></p><p>因此，随着训练时间的增加，α(t) 将会从零开始线性增长至某个饱和值，对应无标签数据的信号也将逐渐释放出来。背后的核心想法也很直观，早期模型预测效果不佳，因此 <em>entropy regularization</em> 产生信号的误差也较大，因而 α(t) 应该从零开始，由小逐渐增大。</p><p><strong>存在不足：</strong>只在训练时间这个维度上，采用了退火思想，即采用时变系数α(t)。而在伪标签这个维度，对于模型给予的预测标签一视同仁，这种方法在实际中存在明显问题。若模型在对伪标签的数据预测后， 10 个类别预测概率值都接近于 0.1，以最大概率这一原则选择对应的标签，那么这个标签对模型的训练会造成一定的<strong><u>副作用</u></strong>。</p><p><strong>设想如何突破这一不足？</strong></p><p>也许可以设定一个阈值，抛弃那些预测最大概率值小于该阈值的未标记样本，将满足条件的未标记样本分配伪标签，并加入模型评估当中，之后再迭代。</p><h3 id="1-2-Noisy-Student"><a href="#1-2-Noisy-Student" class="headerlink" title="1.2. Noisy Student"></a>1.2. <a href="https://arxiv.org/abs/1911.04252">Noisy Student</a></h3><p>论文的关键 idea 是训练两个模型，“teacher”和“student”，在有标签数据中训练“teacher”模型，并利用该模型对未标记数据进行推断伪标签，这些伪标签可以是<strong><u>软标签</u></strong>，也可以取其最大概率的类别将其转换为<strong><u>硬标签</u></strong>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161507462.png" alt="image-20230511161507462"></p><p>然后将标记数据与为标记数据（带有伪标签）置入“student”进行训练，在训练之前<strong><u>数据增强</u></strong>使用 <strong>RandAugment</strong>，待“student”模型训练好后，使用最新的模型作为新的“teacher”，进行下一次迭代，此过程会重复几次（通常为 3 次）。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511211613877.png" alt="image-20230511211613877"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;半监督学习&quot;&gt;&lt;a href=&quot;#半监督学习&quot; class=&quot;headerlink&quot; title=&quot;半监督学习&quot;&gt;&lt;/a&gt;半监督学习&lt;/h1&gt;&lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;</summary>
      
    
    
    
    <category term="半监督学习" scheme="https://ytz7.github.io/categories/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://ytz7.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="个人随笔" scheme="https://ytz7.github.io/tags/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/"/>
    
    <category term="Semi-Suprised-Learning" scheme="https://ytz7.github.io/tags/Semi-Suprised-Learning/"/>
    
  </entry>
  
</feed>
