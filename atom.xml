<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>七号zz の Blog</title>
  
  <subtitle>愿为未来做牛马。</subtitle>
  <link href="https://ytz7.github.io/atom.xml" rel="self"/>
  
  <link href="https://ytz7.github.io/"/>
  <updated>2023-09-13T09:03:09.924Z</updated>
  <id>https://ytz7.github.io/</id>
  
  <author>
    <name>七号zz</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SimMatch</title>
    <link href="https://ytz7.github.io/posts/697723e2.html"/>
    <id>https://ytz7.github.io/posts/697723e2.html</id>
    <published>2023-09-02T12:10:38.000Z</published>
    <updated>2023-09-13T09:03:09.924Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>SimMatch: Semi-supervised Learning with Similarity Matching - CVPR 2022</p></blockquote><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230902201326255.png" alt="image-20230902201326255"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165633855.png" alt="image-20230913165633855"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165645656.png" alt="image-20230913165645656"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165654047.png" alt="image-20230913165654047"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165704315.png" alt="image-20230913165704315"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165712290.png" alt="image-20230913165712290"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165722851.png" alt="image-20230913165722851"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165737444.png" alt="image-20230913165737444"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165746130.png" alt="image-20230913165746130"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165758187.png" alt="image-20230913165758187"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165808546.png" alt="image-20230913165808546"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165818149.png" alt="image-20230913165818149"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165826742.png" alt="image-20230913165826742"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165834876.png" alt="image-20230913165834876"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165843838.png" alt="image-20230913165843838"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165851812.png" alt="image-20230913165851812"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165904250.png" alt="image-20230913165904250"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230913165931934.png" alt="image-20230913165931934"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;SimMatch: Semi-supervised Learning with Similarity Matching - CVPR 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://ytz-blog-buck</summary>
      
    
    
    
    <category term="半监督学习" scheme="https://ytz7.github.io/categories/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="Semi-Surprised-Learning" scheme="https://ytz7.github.io/tags/Semi-Surprised-Learning/"/>
    
    <category term="学习笔记" scheme="https://ytz7.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>MMDetection</title>
    <link href="https://ytz7.github.io/posts/73d20edf.html"/>
    <id>https://ytz7.github.io/posts/73d20edf.html</id>
    <published>2023-06-06T09:02:27.000Z</published>
    <updated>2023-09-13T09:03:09.924Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>MMDetection 踩坑之路</p><p>每天都在配环境，每时每刻都在烦…</p></blockquote><p>参考：<a href="https://mmdetection.readthedocs.io/zh_CN/v2.17.0/get_started.html#id6">依赖 — MMDetection 2.17.0 文档</a></p><h1 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h1><h2 id="安装-Pytorch"><a href="#安装-Pytorch" class="headerlink" title="安装 Pytorch"></a>安装 Pytorch</h2><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">更新 pip</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">python -m pip install --upgrade pip</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html</span></span><br></pre></td></tr></tbody></table></figure><h2 id="安装-mmcv-full"><a href="#安装-mmcv-full" class="headerlink" title="安装 mmcv-full"></a>安装 mmcv-full</h2><p>推荐安装 <code>mmcv-full</code>，包含所有功能，而简版为 <code>mmcv</code>，并且，下载时必须从 openmmlab 官网下载，下载源格式为：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://download.openmmlab.com/mmcv/dist/cu${CUDA_VERSION }/torch{TORCH_VERSION }/index.html</span><br></pre></td></tr></tbody></table></figure><blockquote><p>当 cuda 版本为 10.1 时，则为 <code>cu101</code>；11.1 时，为 <code>cu111</code></p><p>torch 的版本不管为 1.9.5 还是 1.9.1，小版本都归零，即  <code>1.9.0</code></p></blockquote><p>在 CUDA 11.1，torch 版本为 1.9.1 的版本下，下载地址为：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html</span><br></pre></td></tr></tbody></table></figure><p>完整的命令为：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">pip install mmcv-full==1.3.17 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html</span></span><br></pre></td></tr></tbody></table></figure><h3 id="安装-mmdetection"><a href="#安装-mmdetection" class="headerlink" title="安装 mmdetection"></a>安装 mmdetection</h3><p>需要注意依赖，参考首行的官方文档：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">pip install mmdet==2.25</span></span><br></pre></td></tr></tbody></table></figure><h1 id="学习-MMDetection"><a href="#学习-MMDetection" class="headerlink" title="学习 MMDetection"></a>学习 MMDetection</h1><h2 id="对数据做增强"><a href="#对数据做增强" class="headerlink" title="对数据做增强"></a>对数据做增强</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 强增强 - SoftTeacher</span></span><br><span class="line">strong_pipeline = [</span><br><span class="line">    <span class="built_in">dict</span>(</span><br><span class="line">        <span class="built_in">type</span>=<span class="string">"Sequential"</span>,</span><br><span class="line">        transforms=[</span><br><span class="line">            <span class="built_in">dict</span>(</span><br><span class="line">                <span class="built_in">type</span>=<span class="string">"RandResize"</span>,</span><br><span class="line">                img_scale=[(<span class="number">1333</span>, <span class="number">400</span>), (<span class="number">1333</span>, <span class="number">1200</span>)],</span><br><span class="line">                multiscale_mode=<span class="string">"range"</span>,</span><br><span class="line">                keep_ratio=<span class="literal">True</span>,</span><br><span class="line">            ),</span><br><span class="line">            <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandFlip"</span>, flip_ratio=<span class="number">0.5</span>),</span><br><span class="line">            <span class="built_in">dict</span>(</span><br><span class="line">                <span class="built_in">type</span>=<span class="string">"ShuffledSequential"</span>,</span><br><span class="line">                transforms=[</span><br><span class="line">                    <span class="built_in">dict</span>(</span><br><span class="line">                        <span class="built_in">type</span>=<span class="string">"OneOf"</span>,</span><br><span class="line">                        transforms=[</span><br><span class="line">                            <span class="built_in">dict</span>(<span class="built_in">type</span>=k)</span><br><span class="line">                            <span class="keyword">for</span> k <span class="keyword">in</span> [</span><br><span class="line">                                <span class="string">"Identity"</span>,</span><br><span class="line">                                <span class="string">"AutoContrast"</span>,</span><br><span class="line">                                <span class="string">"RandEqualize"</span>,</span><br><span class="line">                                <span class="string">"RandSolarize"</span>,</span><br><span class="line">                                <span class="string">"RandColor"</span>,</span><br><span class="line">                                <span class="string">"RandContrast"</span>,</span><br><span class="line">                                <span class="string">"RandBrightness"</span>,</span><br><span class="line">                                <span class="string">"RandSharpness"</span>,</span><br><span class="line">                                <span class="string">"RandPosterize"</span>,</span><br><span class="line">                            ]</span><br><span class="line">                        ],</span><br><span class="line">                    ),</span><br><span class="line">                    <span class="built_in">dict</span>(</span><br><span class="line">                        <span class="built_in">type</span>=<span class="string">"OneOf"</span>,</span><br><span class="line">                        transforms=[</span><br><span class="line">                            <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandTranslate"</span>, x=(-<span class="number">0.1</span>, <span class="number">0.1</span>)),</span><br><span class="line">                            <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandTranslate"</span>, y=(-<span class="number">0.1</span>, <span class="number">0.1</span>)),</span><br><span class="line">                            <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandRotate"</span>, angle=(-<span class="number">30</span>, <span class="number">30</span>)),</span><br><span class="line">                            [</span><br><span class="line">                                <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandShear"</span>, x=(-<span class="number">30</span>, <span class="number">30</span>)),</span><br><span class="line">                                <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandShear"</span>, y=(-<span class="number">30</span>, <span class="number">30</span>)),</span><br><span class="line">                            ],</span><br><span class="line">                        ],</span><br><span class="line">                    ),</span><br><span class="line">                ],</span><br><span class="line">            ),</span><br><span class="line">            <span class="built_in">dict</span>(</span><br><span class="line">                <span class="built_in">type</span>=<span class="string">"RandErase"</span>,</span><br><span class="line">                n_iterations=(<span class="number">1</span>, <span class="number">5</span>),</span><br><span class="line">                size=[<span class="number">0</span>, <span class="number">0.2</span>],</span><br><span class="line">                squared=<span class="literal">True</span>,</span><br><span class="line">            ),</span><br><span class="line">        ],</span><br><span class="line">        record=<span class="literal">True</span>,</span><br><span class="line">    ),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"Pad"</span>, size_divisor=<span class="number">32</span>),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"Normalize"</span>, **img_norm_cfg),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"ExtraAttrs"</span>, tag=<span class="string">"unsup_student"</span>),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"DefaultFormatBundle"</span>),</span><br><span class="line">    <span class="built_in">dict</span>(</span><br><span class="line">        <span class="built_in">type</span>=<span class="string">"Collect"</span>,</span><br><span class="line">        keys=[<span class="string">"img"</span>, <span class="string">"gt_bboxes"</span>, <span class="string">"gt_labels"</span>],</span><br><span class="line">        meta_keys=(</span><br><span class="line">            <span class="string">"filename"</span>,</span><br><span class="line">            <span class="string">"ori_shape"</span>,</span><br><span class="line">            <span class="string">"img_shape"</span>,</span><br><span class="line">            <span class="string">"img_norm_cfg"</span>,</span><br><span class="line">            <span class="string">"pad_shape"</span>,</span><br><span class="line">            <span class="string">"scale_factor"</span>,</span><br><span class="line">            <span class="string">"tag"</span>,</span><br><span class="line">            <span class="string">"transform_matrix"</span>,</span><br><span class="line">        ),</span><br><span class="line">    ),</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure><p>这段代码中定义了一个强增强的 pipeline，包括以下几个部分：</p><ol><li><p><code>RandResize</code>：随机对图像进行缩放，可以生成不同尺度的图像来增加训练数据的多样性。</p></li><li><p><code>RandFlip</code>：随机对图像进行水平或垂直翻转，可以帮助模型学习不同角度的物体。</p></li><li><p><code>ShuffledSequential</code>：将后续的增强操作随机打乱，以增加数据的多样性。</p></li><li><p><code>OneOf</code>：从一组增强操作中随机选择一个进行操作，包括：</p><ul><li><code>Identity</code>：原图不变</li><li><code>AutoContrast</code>：自动调整图像对比度</li><li><code>RandEqualize</code>：随机增强直方图均衡化</li><li><code>RandSolarize</code>：随机增强 Solarization 效果</li><li><code>RandColor</code>：随机改变图像颜色平衡</li><li><code>RandContrast</code>：随机增强图像对比度</li><li><code>RandBrightness</code>：随机改变图像亮度</li><li><code>RandSharpness</code>：随机增强图像锐利度</li><li><code>RandPosterize</code>：随机降低图像位深度</li></ul></li><li><p><code>OneOf</code>：从一组增强操作中随机选择一个进行操作，包括：</p><ul><li><code>RandTranslate</code>：随机对图像进行平移</li><li><code>RandRotate</code>：随机对图像进行旋转</li><li><code>RandShear</code>：随机对图像进行剪切</li></ul></li><li><p><code>RandErase</code>：随机擦除图像中的像素，增加模型对图像噪声和遮挡的鲁棒性。</p></li><li><p><code>Pad</code>：将图像填充至指定大小的倍数，用于处理不同大小的图像以及特定的 GPU 加速要求。</p></li><li><p><code>Normalize</code>：对图像进行标准化处理，将像素值映射到均值为 0，方差为 1 的分布中。</p></li><li><p><code>ExtraAttrs</code>：添加一个额外的属性，用于标记数据被传入模型的来源（这里标记为 “unsup_student”）。</p></li><li><p><code>DefaultFormatBundle</code>：将图像数据打包为默认格式。</p></li><li><p><code>Collect</code>：将图像及其标注信息收集到一个字典中，以供后续处理。</p></li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 弱增强 - SoftTeacher</span></span><br><span class="line">weak_pipeline = [</span><br><span class="line">    <span class="built_in">dict</span>(</span><br><span class="line">        <span class="built_in">type</span>=<span class="string">"Sequential"</span>,</span><br><span class="line">        transforms=[</span><br><span class="line">            <span class="built_in">dict</span>(</span><br><span class="line">                <span class="built_in">type</span>=<span class="string">"RandResize"</span>,</span><br><span class="line">                img_scale=[(<span class="number">1333</span>, <span class="number">400</span>), (<span class="number">1333</span>, <span class="number">1200</span>)],</span><br><span class="line">                multiscale_mode=<span class="string">"range"</span>,</span><br><span class="line">                keep_ratio=<span class="literal">True</span>,</span><br><span class="line">            ),</span><br><span class="line">            <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"RandFlip"</span>, flip_ratio=<span class="number">0.5</span>),</span><br><span class="line">        ],</span><br><span class="line">        record=<span class="literal">True</span>,</span><br><span class="line">    ),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"Pad"</span>, size_divisor=<span class="number">32</span>),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"Normalize"</span>, **img_norm_cfg),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"ExtraAttrs"</span>, tag=<span class="string">"unsup_teacher"</span>),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">"DefaultFormatBundle"</span>),</span><br><span class="line">    <span class="built_in">dict</span>(</span><br><span class="line">        <span class="built_in">type</span>=<span class="string">"Collect"</span>,</span><br><span class="line">        keys=[<span class="string">"img"</span>, <span class="string">"gt_bboxes"</span>, <span class="string">"gt_labels"</span>],</span><br><span class="line">        meta_keys=(</span><br><span class="line">            <span class="string">"filename"</span>,</span><br><span class="line">            <span class="string">"ori_shape"</span>,</span><br><span class="line">            <span class="string">"img_shape"</span>,</span><br><span class="line">            <span class="string">"img_norm_cfg"</span>,</span><br><span class="line">            <span class="string">"pad_shape"</span>,</span><br><span class="line">            <span class="string">"scale_factor"</span>,</span><br><span class="line">            <span class="string">"tag"</span>,</span><br><span class="line">            <span class="string">"transform_matrix"</span>,</span><br><span class="line">        ),</span><br><span class="line">    ),</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure><p>这是一段弱增强的代码，主要包括以下几个部分：</p><ol><li><p><code>RandResize</code>：随机对图像进行缩放，可以生成不同尺度的图像来增加训练数据的多样性。</p></li><li><p><code>RandFlip</code>：随机对图像进行水平或垂直翻转，可以帮助模型学习不同角度的物体。</p></li><li><p><code>Pad</code>：将图像填充至指定大小的倍数，用于处理不同大小的图像以及特定的 GPU 加速要求。</p></li><li><p><code>Normalize</code>：对图像进行标准化处理，将像素值映射到均值为 0，方差为 1 的分布中。</p></li><li><p><code>ExtraAttrs</code>：添加一个额外的属性，用于标记数据被传入模型的来源（这里标记为 “unsup_teacher”）。</p></li><li><p><code>DefaultFormatBundle</code>：将图像数据打包为默认格式。</p></li><li><p><code>Collect</code>：将图像及其标注信息收集到一个字典中，以供后续处理。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;MMDetection 踩坑之路&lt;/p&gt;
&lt;p&gt;每天都在配环境，每时每刻都在烦…&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;参考：&lt;a href=&quot;https://mmdetection.readthedocs.io/zh_CN/v2.17.0/g</summary>
      
    
    
    
    <category term="环境配置" scheme="https://ytz7.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>Faster-RCNN</title>
    <link href="https://ytz7.github.io/posts/c018b3e5.html"/>
    <id>https://ytz7.github.io/posts/c018b3e5.html</id>
    <published>2023-05-28T09:08:02.000Z</published>
    <updated>2023-09-13T09:03:09.924Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</p></blockquote><blockquote><p>记一次过度饮酒后的第二天…真的痛苦</p><p>看了好几篇在目标检测算法应用半监督学习的文章，感觉好多都是基于 Faster RCNN 作为其主干网络来实现，但是对于目标检测算法，目前只对于 YOLOv3（One-Stage）有所了解，所以说，还是需要对其他目标检测的算法有所了解才好。</p></blockquote><p>论文地址：<a href="https://arxiv.org/abs/1506.01497"> Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p><p>参考文章：</p><ul><li><a href="https://blog.csdn.net/weixin_44791964/article/details/105739918">Pytorch搭建Faster R-CNN目标检测平台</a></li><li><a href="https://blog.csdn.net/STATEABC/article/details/123881710">RCNN、Fast-RCNN、Faster-RCNN网络详解</a></li><li><a href="https://zhuanlan.zhihu.com/p/60895929?utm_id=0">Faster RCNN的一些笔记</a></li><li><a href="https://blog.csdn.net/lanran2/article/details/54376126?spm=1001.2014.3001.5502">RPN 解析</a></li></ul><p>参考视频：</p><ul><li><a href="https://www.bilibili.com/video/BV1af4y1m7iL?p=3&amp;spm_id_from=pageDriver&amp;vd_source=7801e0201bc8ddcb3d951c7388a08c3e">霹雳吧啦Wz</a></li><li><a href="https://www.bilibili.com/video/BV1BK41157Vs?p=1&amp;vd_source=7801e0201bc8ddcb3d951c7388a08c3e">Bubbliiiing</a></li></ul><h1 id="RCNN-算法流程"><a href="#RCNN-算法流程" class="headerlink" title="RCNN 算法流程"></a>RCNN 算法流程</h1><ol><li>一张图片生成1k~2k个候选区域(使用Selective Search方法)；</li><li>对每个候选区域，使用深度网络(即图片分类网络)提取特征；</li><li>特征送入每一类的SVM分类器，判别是否属于该类；</li><li>使用回归器精细修正候选框位置。</li></ol><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/56ef4502976d4a25a8c5db087638bbba.png" alt="在这里插入图片描述"></p><p>Selective Search 方法是一种通过图像分割方法来生成一些原始区域的方法，使用一些合并策略将这些区域合并，得到一个层次化的区域结构，这些结构就包含着可能需要的物体。简而言之，<u><strong>Selective Search 方法用于生成候选框</strong></u>。</p><p><strong>对每个候选区域，使用深度网络提取特征</strong>后，将 2000 个候选区域分别缩放到 <u><strong>227 * 227</strong></u> 的大小（AlexNet CNN 的输入大小），通过 CNN 提取特征。获取 <u><strong>4096 维</strong></u>的特征，得到 <u><strong>2000 * 4096 维</strong></u>特征矩阵：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/353f2eb486744aa1aef24d69a1e8d53d.png" alt="在这里插入图片描述"></p><p>然后将<strong>特征送入每一类的SVM分类器来判定类别</strong>，将 2000 * 4096 维特征与 20 个SVM（Pascal VOC 有 20 个类别）组成的<u><strong>权值矩阵</strong></u> 4096 * 20 相乘，获得 2000 * 20 维矩阵表示每一个建议框是某个目标类别的得分，即在矩阵中 [i, j] 表示第 i 个建议框中是第 j 类的概率大小。</p><p>分别对上述 2000 * 20 维矩阵中每一列即每一类进行<u><strong>非极大值抑制提出重叠建议框</strong></u>，得到该列（即该类中）得分最高的一些建议框。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/c533db30aeb54d288d7ac3ac38e9789b.png" alt="在这里插入图片描述"></p><p><strong>使用回归器精细修正候选框位置</strong>，分别用 20 个回归器对上述 20 个类别中剩余的建议框进行回归操作，最终得到每个类别的修正后得分最高的 bounding box。</p><p>如下图所示，黄色窗口 P 表示建议框 Region Proposal，绿色窗口 G 表示实际框Ground Truth，红色窗口 G^ 表示 Region Proposal 进行回归后的预测窗口。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/25b3a317002b43818e22f4744519c948.png" alt="在这里插入图片描述"></p><h1 id="Fast-RCNN-算法流程"><a href="#Fast-RCNN-算法流程" class="headerlink" title="Fast-RCNN 算法流程"></a>Fast-RCNN 算法流程</h1><p>具体流程如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/f6f2b89d4e7d4a13acc6c068406add95.png" alt="在这里插入图片描述"></p><h2 id="SPPNet-x2F-ROI-pooling-layer"><a href="#SPPNet-x2F-ROI-pooling-layer" class="headerlink" title="SPPNet / ROI pooling layer"></a>SPPNet / ROI pooling layer</h2><p>在 RCNN 的基础上对于提取特征的步骤做了改进：</p><ul><li>将整个图像输入到 CNN 中得到特征图，再将 Selective Search 方法得到的<u><strong>候选框投影到特征图</strong></u>上，得到<u><strong>特征矩阵</strong></u></li></ul><p>即 SPPNet，与 RCNN 的对比如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529172148060.png" alt="image-20230529172148060"></p><p>根据每个候选区域原图与特征图的映射关系，就能在特征图中直接获取特征矩阵，这些候选区域的特征不需要再重复计算。</p><p>对于<u><strong>候选区域的 size 各异的限制</strong></u>，RCNN 通过对候选区域做变换得到，即<u><strong>放缩</strong></u>，在 SPPNet 中由 <code>spatial pyramid pooling</code> 解决，将原始网络中最后一个池化层改进成了空间金字塔池化层，它可以将任意尺寸的特征图转化为固定大小的特征向量从而不同尺寸的候选区域的不同大小特征图样切片也可以转化成相同大小的特征向量以进行后续判决：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/49a24a3963224bf5a3ede9915c0041cd.png" alt="img"></p><h2 id="softmax-分类器"><a href="#softmax-分类器" class="headerlink" title="softmax 分类器"></a>softmax 分类器</h2><p>输出 N+1 个类别的概率（目标总类别 + 背景），共 N+1 个节点：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/6b33b1e4c7f34f7bb10615a41d7410c4.png" alt="在这里插入图片描述"></p><h2 id="bbox-regressor-边界框回归器"><a href="#bbox-regressor-边界框回归器" class="headerlink" title="bbox regressor 边界框回归器"></a>bbox regressor 边界框回归器</h2><p>输出对应 N+1 个类别的候选边界框回归参数（dx, dy, dw, dh），共 （N+1） *  4 个节点：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/dbd6bbb853f34471bb0b6b4344e8e8ee.png" alt="在这里插入图片描述"></p><p>利用回归得到的参数得到边界框：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/d9880b1884e541b99313fb39a10c0896.png" alt="在这里插入图片描述"></p><h1 id="Faster-RCNN-算法流程"><a href="#Faster-RCNN-算法流程" class="headerlink" title="Faster-RCNN 算法流程"></a>Faster-RCNN 算法流程</h1><ol><li>将图像输入网络得到相应的特征图；</li><li>使用 RPN 结构生成候选框，将 RPN 生成的候选框投影到特征图上获得相应的特征矩阵；</li><li>将每个特征矩阵通过 ROI(Region of Interest) pooling 层缩放到 7x7 大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果。</li></ol><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529174255504.png" alt="image-20230529174255504" style="zoom: 67%;"><p>Faster-RCNN 可以理解为 RPN + Fast-RCNN，右半部分即为 Fast-RCNN。</p><h2 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h2><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529192038186.png" alt="image-20230529192038186"></p><blockquote><p>RPN，全称为 Region Proposal Network，Region Proposal 为区域提取，RPN 就是用来提取候选框的网络</p></blockquote><p>利用深度卷积神经网络来计算初始的候选区域，一个特征图通过<u><strong>滑动窗口</strong></u>，得到256 维特征，其中一个 <code>grid</code> 生成 <code>k</code> 个 <code>anchor boxes</code>，通过两次全连接得到 <code>2k</code> 个分数和 <code>4k</code> 个坐标，如下图所示，其中 <code>cls</code> 代表类别预测，分别预测<u><strong>背景概率</strong></u>和<u><strong>前景概率</strong></u>，前景概率即为区域中是否带有对象的概率，可以视作<u><strong>二分类问题</strong></u>，而 <code>reg</code> 则为 k 个 boxes 的四个参数的预测，分别为：中心点坐标和宽高。</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529193318163.png" alt="image-20230529193318163" style="zoom: 67%;"><p>在论文中提出了三种尺度（）和三种比例作为 anchor，每个位置在<u><strong>原图</strong></u>上都对于有 9 个 anchor，对于 RPN 生成的候选框之间存在大量重叠，基于候选框的 <u><strong>cls 得分</strong></u>，采用<u><strong>非极大抑制</strong></u>筛选候选框。最后对应于原图的候选框生成结果可以由下图简略概括：</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529193915964.png" alt="image-20230529193915964" style="zoom:50%;"><p>RPN 的<u><strong>输入特征图</strong></u>指的是哪个特征图？</p><p>RPN 的输入特征图就是 Faster-RCNN 中的公共 feature map，主要用以 RPN 和 RoI Pooling 共享。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529194033385.png" alt="image-20230529194033385"></p><p>其中，<code>p</code> 表示的是 anchor 为一个 object 的概率，<code>p*</code> 为 1 时则表示该 anchor 为正样本（positive），反之，则为负样本（negative）。</p><p>判断 anchor 是否为正样本：</p><ul><li>与 Grounding Truth 的重叠区域 IoU 超过 0.7</li><li>为了防止条件 1 筛选过后不存在正样本，故使 IoU 最大的 anchor 也为正样本</li></ul><h2 id="Training-RPNs"><a href="#Training-RPNs" class="headerlink" title="Training RPNs"></a>Training RPNs</h2><ol><li><p>RPN 通过反向传播和随机梯度下降法进行端到端的训练</p></li><li><p>随机从一张图片中采样 256 个 anchors，计算一个 mini-batch 的损失函数</p></li><li><p>采样的 positive anchors 和 negative anchors 的比例是 1 : 1，若正样本少于 128，则从负样本中采样填充</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529201003539.png" alt="image-20230529201003539"></p></li><li><p>所有层通过 ImageNet 预训练模型进行初始化</p></li></ol><h2 id="Sharing-Features-for-RPN-and-Fast-R-CNN"><a href="#Sharing-Features-for-RPN-and-Fast-R-CNN" class="headerlink" title="Sharing Features for RPN and Fast R-CNN"></a>Sharing Features for RPN and Fast R-CNN</h2><blockquote><p>这一个地方看得不是很懂…</p></blockquote><p><u><strong>RPN和Fast R-CNN共享特征</strong></u>，论文中采用的是：<u><strong>Alternating training</strong></u>：</p><p>首先训练 RPN 层，然后用所得到的 proposals 来训练 Fast-RCNN（此时设置 RPN层不更新），然后得到的 Fast-RCNN，再来初始化并训练 RPN 层，然后再迭代往复。</p><p>目的：为了使两个不同任务能够<u><strong>共享卷积层</strong></u>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529202144896.png" alt="image-20230529202144896"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529202200709.png" alt="image-20230529202200709"></p><p>具体训练细节：</p><ol><li>训练 RPN，使用 ImageNet 预训练权重，并且在 region proposal 任务上进行训练微调</li><li>使用生成的 region proposal 在 Fast R-CNN 上进行训练，同意使用 ImageNet 预训练权重</li><li>使用所训练的目标检测网络（Fast R-CNN）来初始化 RPN 训练权重，但固定共享的卷积层，仅微调 RPN 独有的层</li><li>让共享的卷积层继续固定，训练微调 Fast R-CNN 独有的层</li></ol><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529203002756.png" alt="image-20230529203002756" style="zoom:67%;"><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230529203015920.png" alt="image-20230529203015920" style="zoom:67%;"><blockquote><p>现在是 2023/05/29 22：38，看代码看得有点昏，走咯走咯，下班…</p><p>git push 一哈🤦‍♂️</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;记一次过度饮酒后的第二</summary>
      
    
    
    
    <category term="目标检测" scheme="https://ytz7.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
    <category term="机器学习" scheme="https://ytz7.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Object-Detection" scheme="https://ytz7.github.io/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>2023 年下半年计划</title>
    <link href="https://ytz7.github.io/posts/68092092.html"/>
    <id>https://ytz7.github.io/posts/68092092.html</id>
    <published>2023-05-24T05:48:14.000Z</published>
    <updated>2023-09-13T09:03:09.924Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>打算做半监督目标检测方向，理清以下下半年的计划，该学习什么，该怎样学习，不然像个无头苍蝇一样四处乱撞…</p><p>定一个计划把，看看自己是否能够坚持下来，学完一篇就在这边打卡，但我不知道这样子做会不会太慢？…</p><p>不过还是要处理一下前几天看的 FixMatch 的代码细节还没看完，先把这个东西收尾收掉把。</p><p>2023-05-24 记录：FixMatch 代码收尾结束，果然看代码对于 Coding 也会有些帮助，尤其是在上次看完 YOLOv3 的代码之后，之前感觉还挺害怕看代码的…</p><p>Ps： chatGPT 真好用🤗</p></blockquote><h1 id="Coding-能力"><a href="#Coding-能力" class="headerlink" title="Coding 能力"></a>Coding 能力</h1><p>感觉自己的 Python 基础相当的差啊，对 Pytorch 也不这么熟悉，或者说只会看代码和运行别人的代码🤦‍♂️，那么该如何解决这一个棘手的问题呢？</p><ul><li><a href="https://space.bilibili.com/18161609/channel/collectiondetail?sid=48290">霹雳吧啦Wz的个人空间_哔哩哔哩_bilibili</a></li></ul><p>学习这个博主关于图像分类方面的代码，尽量要试着阅读和手写代码</p><h1 id="目标检测方面"><a href="#目标检测方面" class="headerlink" title="目标检测方面"></a>目标检测方面</h1><p>对于目标检测的基础只是还是需要了解的，具体可以看下面这个博主的视频学习，也是能阅读代码就阅读代码，（手写代码）</p><ul><li><a href="https://space.bilibili.com/472467171/?spm_id_from=333.999.0.0">Bubbliiiing的个人空间_哔哩哔哩_bilibili</a></li></ul><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230525120054286.png" alt="image-20230525120054286"></p><h1 id="半监督目标检测方向"><a href="#半监督目标检测方向" class="headerlink" title="半监督目标检测方向"></a>半监督目标检测方向</h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&amp;mid=2247602689&amp;idx=3&amp;sn=608057273347f3109b0a6e65212ed3c4&amp;chksm=fb54b0edcc2339fb809d96e89c98a6a023d9f4d19185c875ce70f348d2265ddf620d91fba086&amp;scene=27">一文梳理目标检测的半监督学习 (qq.com)</a></p><p>首先肯定是了解先前的相关经典论文，不过不需要精读，需要精度的是这两年以内的论文，然后再去复现论文。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;打算做半监督目标检测方向，理清以下下半年的计划，该学习什么，该怎样学习，不然像个无头苍蝇一样四处乱撞…&lt;/p&gt;
&lt;p&gt;定一个计划把，看看自己是否能够坚持下来，学完一篇就在这边打卡，但我不知道这样子做会不会太慢？…&lt;/p&gt;
&lt;p&gt;不过还是要处理一下前</summary>
      
    
    
    
    <category term="学习计划" scheme="https://ytz7.github.io/categories/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"/>
    
    
  </entry>
  
  <entry>
    <title>Pytorch 学习</title>
    <link href="https://ytz7.github.io/posts/9f542fbf.html"/>
    <id>https://ytz7.github.io/posts/9f542fbf.html</id>
    <published>2023-05-22T14:05:21.000Z</published>
    <updated>2023-09-13T09:03:09.924Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>好久没有写 python了，对于 pytorch 的语法什么的都忘光光了，写着一篇博客来记录我学习的过程，以及途中遇到的 bug 和解决方法，希望我的 coding 能力能够有所提升…</p></blockquote><h1 id="环境安装配置问题"><a href="#环境安装配置问题" class="headerlink" title="环境安装配置问题"></a>环境安装配置问题</h1><h2 id="安装-CUDA（Linux-环境）"><a href="#安装-CUDA（Linux-环境）" class="headerlink" title="安装 CUDA（Linux 环境）"></a>安装 CUDA（Linux 环境）</h2><h3 id="查看-CUDA-状态"><a href="#查看-CUDA-状态" class="headerlink" title="查看 CUDA 状态"></a>查看 CUDA 状态</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="comment"># 查看 cuda</span></span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">naidia-smi</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">nvcc -V</span></span><br></pre></td></tr></tbody></table></figure><h2 id="conda-create-无法使用"><a href="#conda-create-无法使用" class="headerlink" title="conda create 无法使用"></a>conda create 无法使用</h2><p>记录一下 conda create 命令无法使用的问题：</p><p>原因在于配置文件好像解析不了，于是将配置文件 <code>.condarc</code> 中的内容替换为：</p><figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">channel_alias: https://mirrors.tuna.tsinghua.edu.cn/anaconda</span><br><span class="line">default_channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">ssl_verify: true</span><br></pre></td></tr></tbody></table></figure><p>更改完成后，在命令行输入以下命令完成配置的更新：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></tbody></table></figure><h2 id="Linux-中-bash-执行-sh-文件"><a href="#Linux-中-bash-执行-sh-文件" class="headerlink" title="Linux 中 bash 执行 .sh 文件"></a>Linux 中 bash 执行 .sh 文件</h2><p>当出现 <code>$'\r': command not found</code>，是因为 <u><strong>Windows</strong></u> 和 <u><strong>Linux</strong></u> 的 <u><strong>sh</strong></u> 一些文件格式不同：</p><p>以换行为例，Windows 中是 <code>\r\n</code>，而 Linux 中则是 <code>\n</code>，所以才会报 <code>\r’: command not found</code></p><p>如何解决？</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="comment"># 利用 vi 打开文件 xx.sh</span></span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="comment"># 打开分别输入指令</span></span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="comment"># 换行符转换</span></span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">:<span class="built_in">set</span> ff=unix</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="comment"># 保存退出</span></span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">:wq</span></span><br></pre></td></tr></tbody></table></figure><h2 id="安装-Pytorch"><a href="#安装-Pytorch" class="headerlink" title="安装 Pytorch"></a>安装 Pytorch</h2><p>就拿最近看的论文 FixMatch 的环境失手，代码需要：</p><ul><li>python 3.6+</li><li>torch 1.4</li><li>torchvision 0.5</li><li>tensorboard</li><li>numpy</li><li>tqdm</li><li>apex (optional)</li></ul><p>创建虚拟环境 fixmatch，这里我选择 python 版本为 3.7：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n fixmatch python=3.7</span><br><span class="line">conda activate fixmatch</span><br></pre></td></tr></tbody></table></figure><p>访问 Pytorch 的官网选择对应版本的 torch 和 torchvision 即可：</p><p><a href="https://pytorch.org/get-started/previous-versions/">Previous PyTorch Versions | PyTorch</a></p><p>下载 <code>torch==1.4.0</code> 和<code>torchvision==0.5.0</code>，这里需要加上后缀 <code>-f https://download.pytorch.org/whl/torch_stable.html</code>，因为配置下载源为清华源，好像在清华源中找不到这个版本，所以以后都直接从 Pytorch 官网下载即可：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==1.4.0 torchvision==0.5.0 -f https://download.pytorch.org/whl/torch_stable.html</span><br><span class="line">pip install tensorboard</span><br><span class="line">pip install tqdm</span><br></pre></td></tr></tbody></table></figure><p>测试是否可以使用，加入 python 环境，显示以下信息就算安装成功：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; import torch</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; torch.cuda.is_available()</span></span><br><span class="line">True</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; torch.__version__</span></span><br><span class="line">'1.4.0+cu92'</span><br></pre></td></tr></tbody></table></figure><h2 id="安装-Pytorch-和-mmdetection"><a href="#安装-Pytorch-和-mmdetection" class="headerlink" title="安装 Pytorch 和 mmdetection"></a>安装 Pytorch 和 mmdetection</h2><h1 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h1><h2 id="torch-函数使用"><a href="#torch-函数使用" class="headerlink" title="torch 函数使用"></a>torch 函数使用</h2><h3 id="torch-manual-seed"><a href="#torch-manual-seed" class="headerlink" title="torch.manual_seed()"></a>torch.manual_seed()</h3><p>在神经网络中，参数默认是进行随机初始化的。如果不设置的话每次训练时的初始化都是随机的，导致结果不确定。</p><p>如果设置初始化，则每次初始化都是固定的。</p><p>实际上，计算机并不能产生真正的随机数，而是已经编写好的一些无规则排列的数字存储在电脑里，把这些数字划分为若干相等的 N 份，并为每份加上一个编号，编号固定的时候，获得的随机数也是固定的。</p><p><code>torch.manual_seed(1)</code> 用于设置随机初始化的种子，即上述的编号，编号固定，每次获取的随机数固定。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      random.seed(args.seed) </span><br><span class="line">    　torch.manual_seed(args.seed)  <span class="comment">#为CPU设置种子用于生成随机数，以使结果是确定的 </span></span><br><span class="line">    　torch.cuda.manual_seed(args.seed) <span class="comment">#为当前GPU设置随机种子；</span></span><br><span class="line">    　cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    　</span><br><span class="line"><span class="comment"># 如果使用多个GPU，应该使用torch.cuda.manual_seed_all()为所有的GPU设置种子。</span></span><br></pre></td></tr></tbody></table></figure><h3 id="torch-distributed-barrier"><a href="#torch-distributed-barrier" class="headerlink" title="torch.distributed.barrier()"></a>torch.distributed.barrier()</h3><p>这是示例代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.local_rank <span class="keyword">not</span> <span class="keyword">in</span> [-<span class="number">1</span>, <span class="number">0</span>]:</span><br><span class="line">    torch.distributed.barrier()</span><br><span class="line">    </span><br><span class="line">labeled_dataset, unlabeled_dataset, test_dataset = DATASET_GETTERS[args.dataset](</span><br><span class="line">    args, <span class="string">'./data'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.local_rank == <span class="number">0</span>:</span><br><span class="line">    torch.distributed.barrier()</span><br></pre></td></tr></tbody></table></figure><p>关于参数 <code>local_rank</code> 的理解：</p><p>在使用多进程处理任务时，在 Python 中通常设置等级 0 是第一个进程或基本进程，然后对其他进程进行不同的排序，例如 1、2、3 加上基本进程 0，总共 4 个进程。</p><p>使用单进程时，往往会设置进程号为 <code>-1</code>，总而言之，当 <code>local_rank</code> 为 0/-1 时，认为它是主进程。</p><p>对于多进程任务，往往只需要一个进程<strong>预处理数据或者读取数据</strong>，而为了与其他进程共享数据，需要在主进程处理数据的时候让其他进程全部停下来等待。</p><p>上述的代码示例，实际的功能如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 询问当前进程是否为主进程</span></span><br><span class="line"><span class="keyword">if</span> args.local_rank <span class="keyword">not</span> <span class="keyword">in</span> [-<span class="number">1</span>, <span class="number">0</span>]:</span><br><span class="line">    <span class="comment"># 不是主进程，阻塞</span></span><br><span class="line">    <span class="comment"># 让该进程加入 barrier()</span></span><br><span class="line">    torch.distributed.barrier()</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 主进程读取数据</span></span><br><span class="line">labeled_dataset, unlabeled_dataset, test_dataset = DATASET_GETTERS[args.dataset](</span><br><span class="line">    args, <span class="string">'./data'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主进程读取数据完毕，主进程加入 barrier()</span></span><br><span class="line"><span class="keyword">if</span> args.local_rank == <span class="number">0</span>:</span><br><span class="line">    torch.distributed.barrier()</span><br></pre></td></tr></tbody></table></figure><p>当所有的进程都进入了 barrier() 后，Pytorch 就会打开所有的 barrier()，所有的进程都可以继续进行相关操作。</p><p>当需要进行阻塞操作时，模板即为：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.local_rank <span class="keyword">not</span> <span class="keyword">in</span> [-<span class="number">1</span>, <span class="number">0</span>]:</span><br><span class="line">    torch.distributed.barrier()</span><br><span class="line"></span><br><span class="line"><span class="string">'''主进程进行操作</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.local_rank == <span class="number">0</span>:</span><br><span class="line">    torch.distributed.barrier()</span><br></pre></td></tr></tbody></table></figure><p>总结：</p><ul><li>当进程遇到障碍时，它将阻塞</li><li>屏障的位置并不重要（例如，并非所有进程都必须输入相同的 if 语句）</li><li>一个进程被一个屏障阻塞，直到所有进程都遇到一个屏障，在这个屏障上为所有进程解除这些屏障</li></ul><h3 id="model-named-parameters"><a href="#model-named-parameters" class="headerlink" title="model.named_parameters()"></a>model.named_parameters()</h3><p><code>model.named_parameters()</code> 将会打印每一次迭代元素的名字和 <code>param</code>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = DarkNet([<span class="number">1</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">4</span>])</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name,param.requires_grad)</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">conv1.weight <span class="literal">True</span></span><br><span class="line">bn1.weight <span class="literal">True</span></span><br><span class="line">bn1.bias <span class="literal">True</span></span><br><span class="line">layer1.ds_conv.weight <span class="literal">True</span></span><br><span class="line">layer1.ds_bn.weight <span class="literal">True</span></span><br><span class="line">layer1.ds_bn.bias <span class="literal">True</span></span><br><span class="line">layer1.residual_0.conv1.weight <span class="literal">True</span></span><br><span class="line">layer1.residual_0.bn1.weight <span class="literal">True</span></span><br><span class="line">layer1.residual_0.bn1.bias <span class="literal">True</span></span><br><span class="line">layer1.residual_0.conv2.weight <span class="literal">True</span></span><br><span class="line">layer1.residual_0.bn2.weight <span class="literal">True</span></span><br><span class="line">layer1.residual_0.bn2.bias <span class="literal">True</span></span><br><span class="line">layer2.ds_conv.weight <span class="literal">True</span></span><br><span class="line">layer2.ds_bn.weight <span class="literal">True</span></span><br><span class="line">layer2.ds_bn.bias <span class="literal">True</span></span><br><span class="line">layer2.residual_0.conv1.weight <span class="literal">True</span></span><br><span class="line">layer2.residual_0.bn1.weight <span class="literal">True</span></span><br><span class="line">layer2.residual_0.bn1.bias <span class="literal">True</span></span><br><span class="line">....</span><br></pre></td></tr></tbody></table></figure><h3 id="torch-nn-parallel-DistributedDataParallel"><a href="#torch-nn-parallel-DistributedDataParallel" class="headerlink" title="torch.nn.parallel.DistributedDataParallel"></a>torch.nn.parallel.DistributedDataParallel</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""在分布环境中实现数据并行训练</span></span><br><span class="line"><span class="string">    torch.nn.parallel.DistributedDataParallel</span></span><br><span class="line"><span class="string">    - model 需要进行数据并行训练的模型对象</span></span><br><span class="line"><span class="string">    - device_ids=[args.local_rank] 指定在哪些设备上进行数据并行训练</span></span><br><span class="line"><span class="string">    - output_device=args.local_rank 指定输出的设备</span></span><br><span class="line"><span class="string">    - find_unused_parameters=True 指定是否查找未使用的参数。</span></span><br><span class="line"><span class="string">      当模型具有不同分支或子模块具有不同输入形状时，可能会出现未使用的参数。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">model = torch.nn.parallel.DistributedDataParallel(</span><br><span class="line">    model, device_ids=[args.local_rank],</span><br><span class="line">    output_device=args.local_rank, find_unused_parameters=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="tensor-chunk"><a href="#tensor-chunk" class="headerlink" title="tensor.chunk()"></a>tensor.chunk()</h3><p><code>.chunk()</code>是PyTorch张量的一个方法，用于将张量沿着指定维度进行均匀切分成多个子张量。</p><p>具体来说，<code>.chunk()</code>的功能如下：</p><ul><li>语法：<code>chunk(chunks, dim=0)</code></li><li>参数：<ul><li><code>chunks</code>：表示要切分的块数。</li><li><code>dim</code>：表示要在哪个维度上进行切分，默认为0（第一个维度）。</li></ul></li><li>返回值：返回一个元组，包含切分后的子张量。</li></ul><p><code>.chunk()</code>方法会将原始张量沿着指定维度进行均匀切分，每个子张量的大小相等（除非原始张量的大小无法被整除）。返回的结果是一个元组，其中包含切分后的子张量。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logits_u_w, logits_u_s = logits[batch_size:].chunk(<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure><p>在给定的代码中，<code>logits[batch_size:].chunk(2)</code>将张量<code>logits</code>从索引<code>batch_size</code>开始切分成两个子张量。这样可以将模型输出的结果划分为两个部分，分别赋值给<code>logits_u_w</code>和<code>logits_u_s</code>。</p><h3 id="tensor-detach"><a href="#tensor-detach" class="headerlink" title="tensor.detach()"></a>tensor.detach()</h3><p><code>.detach()</code>是PyTorch张量的一个方法，用于创建一个新的张量，其中包含了原始张量的数据，但没有建立与计算图的连接。</p><p>具体来说，<code>.detach()</code>的功能如下：</p><ul><li><code>.detach()</code>会返回一个新的张量，其中包含了原始张量的数据，但<u><strong>没有梯度信息和计算图的连接</strong></u>。这意味着通过<code>.detach()</code>得到的张量<u><strong>不会参与反向传播过程</strong></u>，<u><strong>不会对梯度进行求导</strong></u>。</li></ul><p>常见的使用场景包括：</p><ul><li>当你只对部分张量进行梯度计算时，可以使用<code>.detach()</code>将其与其他需要梯度的张量分离。</li><li>当你需要在不影响原始张量梯度的情况下，对张量进行操作或传递给其他函数。</li></ul><p>总而言之，<code>.detach()</code>方法可以用于生成不需要梯度的张量，并且在某些情况下，可以提高效率并避免梯度传播的影响。</p><h3 id="tensor-flatten-start-dim-x3D-1"><a href="#tensor-flatten-start-dim-x3D-1" class="headerlink" title="tensor.flatten(start_dim=1)"></a>tensor.flatten(start_dim=1)</h3><p><code>x.flatten(start_dim=1)</code> 表示将 <code>x</code> 沿着从 <code>start_dim</code> 开始的所有维度拉平成一个一维张量，返回的结果是一个新的张量。</p><p>具体来说，假设 <code>x</code> 的形状为 <code>(B, C, H, W)</code>，其中 <code>B</code> 表示 batch size，<code>C</code> 表示通道数，<code>H</code> 表示高度，<code>W</code> 表示宽度，那么 <code>x.flatten(start_dim=1)</code> 的结果是一个形状为 <code>(B, C*H*W)</code> 的张量，即将除了 <code>B</code> 以外的所有维度都拉平成一个维度。</p><p>如果 <code>start_dim</code> 是默认值 0，则相当于将整个张量拉平成一个一维张量，即形状为 <code>(B*C*H*W,)</code>。</p><p>如果 <code>start_dim</code> 是 2，假设 <code>x</code> 的形状为 <code>(B, C, H, W)</code>，则表示从第二个维度 <code>C</code> 开始将 <code>x</code> 沿着后面的维度拉平成一个一维张量，返回的结果是一个形状为 <code>(B, C, H*W)</code> 的张量。</p><p>具体来说，对于每个样本数据，张量中第二个维度 <code>C</code> 之后的所有维度都被拉平成为一个维度，而前两个维度 <code>B</code> 和 <code>C</code> 保持不变，因此结果的形状是 <code>(B, C, H*W)</code>。</p><p>举个例子，如果 <code>x</code> 的形状是 <code>(2, 3, 4, 5)</code>，那么 <code>x.flatten(start_dim=2)</code> 的结果将是一个形状为 <code>(2, 3, 20)</code> 的张量。其中，第 1 个样本的维度为 <code>(3, 4, 5)</code>，被拉平成了 <code>(3, 20)</code>；第 2 个样本的维度也是 <code>(3, 4, 5)</code>，被拉平成了 <code>(3, 20)</code>。因此结果的形状是 <code>(2, 3, 20)</code>。</p><h3 id="zip"><a href="#zip" class="headerlink" title="zip()"></a>zip()</h3><p>Python 中使用 <code>zip()</code> 函数同时循环遍历多个列表，例如：</p><p><code>for s, a in zip(self.sizes, self.aspect_ratios)</code>，</p><p>其中：</p><ul><li><code>self.sizes</code> 是一个包含 3 个元素的列表，对应 3 个预定义的尺度（以像素值的平方根为度量）</li></ul><p>  sizes=(128, 256, 512)</p><ul><li><p><code>self.aspect_ratios</code> 是一个包含 3 个元素的列表，对应 3 种预定义的宽高比</p><p>aspect_ratios=(0.5, 1.0, 2.0)</p></li></ul><p><code>zip(self.sizes, self.aspect_ratios)</code> 返回的是一个可迭代对象，每次迭代会输出 <code>self.sizes</code> 和 <code>self.aspect_ratios</code> 中<u><strong>对应位置</strong></u>的元素，即 <code>(s₁, a₁), (s₂, a₂), ...</code>。</p><h3 id="model-parameters-和-model-buffers"><a href="#model-parameters-和-model-buffers" class="headerlink" title="model.parameters() 和 model.buffers()"></a>model.parameters() 和 model.buffers()</h3><p>在 PyTorch 中，<code>model.parameters()</code>和<code>model.buffers()</code>都是用于获取模型中的参数和缓冲区的函数，但是它们获取的内容不同。</p><p><code>model.parameters()</code>函数返回模型中需要训练的参数。这些参数通常对应着网络的权重和偏置值，以及其他可更新的变量。调用<code>model.parameters()</code>函数通常用于定义优化器，从而在训练网络时更新这些参数。</p><p><code>model.buffers()</code>函数返回模型中所有不需要训练的缓冲区，比如BN层和移动平均层的移动平均值和方差。缓冲区也需要在模型定义时被注册，但是它们不需要参与训练，因此也不会影响梯度的计算和更新过程。调用<code>model.buffers()</code>函数通常用于模型的保存和恢复，以及其他需要获取缓冲区的操作。</p><h3 id="nn-BatchNorm2d"><a href="#nn-BatchNorm2d" class="headerlink" title="nn.BatchNorm2d"></a>nn.BatchNorm2d</h3><p>在 PyTorch 中，Batch Normalization 层（BN 层）是通过在模型中添加一个特殊的 nn.BatchNorm2d 类来实现的。其缓冲区包括：</p><ol><li>running_mean：用于计算训练过程中的均值，每次更新一批数据后就会更新。</li><li>running_var：用于计算训练过程中的方差，每次更新一批数据后就会更新。</li><li>weight：会被模型学习到的尺度因子。</li><li>bias：会被模型学习到的偏移参数。</li></ol><p>当模型向前传播时，BN 层会将输入数据按维度计算出均值和方差，然后使用这些均值和方差对数据进行归一化操作。归一化后的数据会受到 weight 和 bias 的影响，然后被传递给下一层作为输入。</p><p>注意到在添加了 BN 层后，我们在<u><strong>传递数据到下一层之前需要使用 ReLU 激活函数</strong></u>。</p><p>nn.BatchNorm2d 的实现细节如下：</p><p>假设输入的形状是 <code>(batch_size, channels, height, width)</code>，则 BatchNorm2d 层会按照 <code>channels</code> 维度计算每一个元素对应的均值和方差，接着使用这些均值和方差对输入数据进行归一化。归一化后的数据会受到权重参数 <code>weight</code>（形状为 <code>(channels,)</code>）和偏置参数 <code>bias</code>（形状也是 <code>(channels,)</code>）的影响，最终输出形状也是 <code>(batch_size, channels, height, width)</code>。</p><h3 id="torch-unique-consecutive-args-kwargs"><a href="#torch-unique-consecutive-args-kwargs" class="headerlink" title="torch.unique_consecutive(*args, **kwargs)"></a>torch.unique_consecutive(*args, **kwargs)</h3><p>参数：</p><ul><li><strong>input</strong>(<a href="https://vimsky.com/cache/index.php?source=https://pytorch.org/docs/stable/tensors.html%23torch.Tensor"><em>Tensor</em></a>) -输入张量</li><li><strong>return_inverse</strong>(<a href="https://vimsky.com/cache/index.php?source=https://docs.python.org/3/library/functions.html%23bool"><em>bool</em></a>) -是否还返回原始输入中的元素在返回的唯一列表中结束的位置的索引。</li><li><strong>return_counts</strong>(<a href="https://vimsky.com/cache/index.php?source=https://docs.python.org/3/library/functions.html%23bool"><em>bool</em></a>) -是否还返回每个唯一元素的计数。</li><li><strong>dim</strong>(<a href="https://vimsky.com/cache/index.php?source=https://docs.python.org/3/library/functions.html%23int"><em>int</em></a>) -要应用唯一的维度。如果 <code>None</code> ，则返回展平输入的唯一性。默认值：<code>None</code></li></ul><p>返回：</p><p>一个张量或一个张量元组包含</p><ul><li>输出(张量)：唯一标量元素的输出列表。</li><li><strong>inverse_indices</strong>(张量)：(可选)如果<code>return_inverse</code>为 True，将有一个额外的返回张量(与输入的形状相同)表示原始输入中的元素映射到输出中的位置的索引；否则，此函数将仅返回一个张量。</li><li><strong>计数</strong>(张量)：(可选)如果<code>return_counts</code>为 True，将有一个额外的返回张量(与 output 或 output.size(dim) 的形状相同，如果指定了 dim )表示每个唯一值或张量的出现次数。</li></ul><p>例子：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = torch.unique_consecutive(x)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, inverse_indices = torch.unique_consecutive(x, return_inverse=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>inverse_indices</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, counts = torch.unique_consecutive(x, return_counts=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>counts</span><br><span class="line">tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></tbody></table></figure><h3 id="torch-cumsum"><a href="#torch-cumsum" class="headerlink" title="torch.cumsum"></a>torch.cumsum</h3><p><code>torch.cumsum</code> 是 PyTorch 中的一个函数，用于计算张量元素的累加和。</p><p>具体来说，对于一个一维张量（长度为 n），<code>torch.cumsum(input, dim=0)</code> 返回一个同样长度的张量，其中每个位置的元素为原张量中该位置及之前位置的元素的累加和。例如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">b = torch.cumsum(a, dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># output: tensor([ 1,  3,  6, 10, 15])</span></span><br></pre></td></tr></tbody></table></figure><p>对于一个多维张量，<code>torch.cumsum(input, dim)</code> 会沿着指定维度 dim 进行累加。例如，对于一个二维张量（shape 为 (m, n)），<code>torch.cumsum(input, dim=0)</code> 会返回一个与原张量尺寸相同的张量，其中每列都沿着行方向进行元素累加。</p><p>具体实现可以参考 PyTorch 文档中的说明：<a href="https://pytorch.org/docs/stable/generated/torch.cumsum.html">torch.cumsum</a>。</p><h2 id="统计模型参数"><a href="#统计模型参数" class="headerlink" title="统计模型参数"></a>统计模型参数</h2><p>首先对于计算模型的FLOPs而言，fvcore是一个易用的工具。fvcore是Facebook开源的一个轻量级的核心库，它提供了各种计算机视觉框架中常见且基本的功能。其中就包括了统计模型的参数以及FLOPs等。fvcore项目的开源地址是：</p><ul><li><a href="https://github.com/facebookresearch/fvcore">facebookresearch/fvcore: Collection of common code that’s shared among different research projects in FAIR computer vision team. (github.com)</a></li></ul><p>如果需要使用fvcore，首先需要安装：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install fvcore</span><br></pre></td></tr></tbody></table></figure><p>对于FlOPs，我们先解释一下其概念：</p><ul><li>FLOPS：注意全大写，是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。</li><li>FLOPs：注意s小写，是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。</li><li><strong>注意，模型的参数量少不代表FLOPs低，理论的FLOPs低也不代表实际的推理速度快</strong>。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet50</span><br><span class="line"><span class="keyword">from</span> fvcore.nn <span class="keyword">import</span> FlopCountAnalysis, parameter_count_table</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_models_param_nums</span>(<span class="params">model</span>):</span><br><span class="line">  <span class="string">"""counts and prints the number of models parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">      model (_type_): model</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  total = <span class="built_in">sum</span>([param.numel() <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters()])</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">'  + Number of params: %.2fM'</span> % (total / <span class="number">1e6</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_models_param_nums</span>(<span class="params">model</span>):</span><br><span class="line">  <span class="string">"""counts and the number of models parameters"""</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()) / <span class="number">1e6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  <span class="comment"># 创建 resnet50 网络</span></span><br><span class="line">  model = resnet50(num_classes=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 创建输入网络的 tensor</span></span><br><span class="line">  tensor = (torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>),)</span><br><span class="line">  <span class="comment"># 分析 FLOPS</span></span><br><span class="line">  flops = FlopCountAnalysis(model, tensor)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">"Flops: {}"</span>.<span class="built_in">format</span>(flops.total()))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 计算模型的参数量</span></span><br><span class="line">  <span class="built_in">print</span>(parameter_count_table(model))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 自定义函数统计模型的参数量</span></span><br><span class="line">  count_models_param_nums(model)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 输出模型的总参数</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">"Total params: {:.2f}M"</span>.<span class="built_in">format</span>(count_models_param_nums(model)))</span><br></pre></td></tr></tbody></table></figure><h2 id="封装数据集"><a href="#封装数据集" class="headerlink" title="封装数据集"></a>封装数据集</h2><h2 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h2><h3 id="多卡训练"><a href="#多卡训练" class="headerlink" title="多卡训练"></a>多卡训练</h3><p>使用多卡训练的方式有很多，当然前提是我们的设备中存在两个及以上的GPU：使用命令 <code>nvidia-smi</code> 查看当前Ubuntu平台的GPU数量，在我们设备中确实存在多卡的条件下，最简单的方法是直接使用 <code>torch.nn.DataParallel</code> 将模型wrap一下即可：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = torch.nn.DataParallel(model)</span><br></pre></td></tr></tbody></table></figure><p>这时，默认所有存在的显卡都会被使用。</p><p>如果机子中有很多显卡，但只想使用0、1、2号显卡，那么可以：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">','</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]))</span><br><span class="line">net = torch.nn.DataParallel(model)</span><br><span class="line"><span class="comment"># CUDA_VISIBLE_DEVICES 表示当前可以被python环境程序检测到的显卡</span></span><br></pre></td></tr></tbody></table></figure><h3 id="权重衰退（weight-decay）"><a href="#权重衰退（weight-decay）" class="headerlink" title="权重衰退（weight_decay）"></a>权重衰退（weight_decay）</h3><blockquote><p>正则化：凡事可以减少泛化误差而不是减少训练误差的方法，都可以称作正则化方法。</p><p>权重衰退是一种最常见的处理过拟合的方法，通常也被称为 L2 正则化</p></blockquote><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/20210709122114223.png" alt="img"></p><p><strong>权值衰减</strong> 是一直以来经常被使用的一种抑制过拟合的方法。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。为损失函数加上权重的平方范数（L2 范数），就可以抑制权重变大，λ 是控制正则化强度的超参数，λ 设置得越大，对大的权重施加的惩罚就越重。</p><p>以下是一段示例代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">no_decay = [<span class="string">'bias'</span>, <span class="string">'bn'</span>]</span><br><span class="line">grouped_parameters = [</span><br><span class="line">    {<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(</span><br><span class="line">        nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">'weight_decay'</span>: args.wdecay},</span><br><span class="line">    {<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> <span class="built_in">any</span>(</span><br><span class="line">        nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">'weight_decay'</span>: <span class="number">0.0</span>}</span><br><span class="line">]</span><br><span class="line">optimizer = optim.SGD(grouped_parameters, lr=args.lr,</span><br><span class="line">                      momentum=<span class="number">0.9</span>, nesterov=args.nesterov)</span><br></pre></td></tr></tbody></table></figure><ol><li><p><code>no_decay</code> 表示不应用权重衰减（weight decay）的参数，其中，偏置（bias）和批归一化（batch normalization）参数不会受到权重衰减的影响</p></li><li><p><code>grouped_parameters</code> 中：</p><ul><li><code>params</code> 一个包含参数（parameter）的列表，根据特定的条件选择模型中的参数，第一个字典中选择的是受权重衰减影响的参数，第二个字典中选择的是不受权重衰减影响的参数</li><li><code>weight_decay</code> 权重衰减的值</li></ul></li><li><p><code>optimizer</code> 是一个使用随机梯度下降（Stochastic Gradient Descent, SGD）算法进行优化的对象，接受以下参数：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SGD</span>(</span><br><span class="line">    params: _params_t,</span><br><span class="line">    lr: <span class="built_in">float</span>,</span><br><span class="line">    momentum: <span class="built_in">float</span> = ...,</span><br><span class="line">    dampening: <span class="built_in">float</span> = ...,</span><br><span class="line">    weight_decay: <span class="built_in">float</span> = ...,</span><br><span class="line">    nesterov: <span class="built_in">bool</span> = ...</span><br><span class="line">)</span><br><span class="line"><span class="string">"""Implements stochastic gradient descent (optionally with momentum).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Nesterov momentum is based on the formula from On the importance of initialization and momentum in deep learning__.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">                parameter groups</span></span><br><span class="line"><span class="string">            lr (float): learning rate</span></span><br><span class="line"><span class="string">            momentum (float, optional): momentum factor (default: 0)</span></span><br><span class="line"><span class="string">            weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string">            dampening (float, optional): dampening for momentum (default: 0)</span></span><br><span class="line"><span class="string">            nesterov (bool, optional): enables Nesterov momentum (default: False)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Example:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.zero_grad()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss_fn(model(input), target).backward()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.step()</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure></li></ol><h3 id="虚拟机后台训练"><a href="#虚拟机后台训练" class="headerlink" title="虚拟机后台训练"></a>虚拟机后台训练</h3><p>只需要在以前训练的指令前增加 nohup 命令，同时在结尾加上 &amp; 符号即可：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nohup python -m visdom.server &amp;  # 后台运行visdom.server</span><br><span class="line">nohup python train.py &amp;  # 使模型在后台训练</span><br></pre></td></tr></tbody></table></figure><p>我们可以通过 jobs -l 来查看进程，在训练结束后通过 kill 指令关闭后台进程：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jobs -l  # 查看进程</span><br><span class="line">kill -9 PID  # 通过进程的PID关闭进程</span><br></pre></td></tr></tbody></table></figure><p>当终端链接断开，重新链接后无法通过 jobs 命令查看后台进程，此时需要通过 ps ux 指令查看所有的进程的 PID，然后通过 kill 指令关闭进程：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps ux  # 查看所有进程的PID</span><br><span class="line">kill -9 PID  # 关闭特定进程</span><br></pre></td></tr></tbody></table></figure><p>另外一点需要注意的时，通过后台运行程序的所有输入都会存储到 nohup.out 的文件中，如果不清理的话，该文件会不断增加，nohup.out 文件默认存放在当前执行脚本所在的目录中，也可以同过指令修改存放位置：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改nohup.out文件存放位置</span></span><br><span class="line">nohup python train.py &gt; /path/to/custom.out &amp;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在不停止进程的情况下清空nohup.out文件的指令（以下两个指令任选一个即可）</span></span><br><span class="line">cp /dev/null nohup.out</span><br><span class="line">cat /dev/null &gt; nohup.out</span><br></pre></td></tr></tbody></table></figure><p>例如：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(fixmatch) root@be6fec86789a:~/Tz/FixMatch-pytorch# nohup python train.py --dataset cifar10 --num-labeled 4000 --arch wideresnet --batch-size 64 --lr 0.03 --expand-labels --seed 5 --out results/cifar10@4000.5 &gt; /root/Tz/FixMatch-pytorch/log/fixmatch_cifar10_4000_5.out &amp;</span><br><span class="line">[1] 3913</span><br><span class="line">nohup: ignoring input and redirecting stderr to stdout</span><br><span class="line">(fixmatch) root@be6fec86789a:~/Tz/FixMatch-pytorch# jobs -l</span><br><span class="line">[1]+  3913 Running                 nohup python train.py --dataset cifar10 --num-labeled 4000 --arch wideresnet --batch-size 64 --lr 0.03 --expand-labels --seed 5 --out results/cifar10@4000.5 &gt; /root/Tz/FixMatch-pytorch/log/fixmatch_cifar10_4000_5.out &amp;</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;好久没有写 python了，对于 pytorch 的语法什么的都忘光光了，写着一篇博客来记录我学习的过程，以及途中遇到的 bug 和解决方法，希望我的 coding 能力能够有所提升…&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;环境安装配</summary>
      
    
    
    
    <category term="环境配置" scheme="https://ytz7.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>FixMatch</title>
    <link href="https://ytz7.github.io/posts/e378f85.html"/>
    <id>https://ytz7.github.io/posts/e378f85.html</id>
    <published>2023-05-22T05:01:14.000Z</published>
    <updated>2023-09-13T09:03:09.924Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Simplifying Semi-Supervised Learning with Consistency and Confidence</p></blockquote><p>会议：NeurIPS 2020</p><p>论文地址：<a href="https://arxiv.org/abs/2001.07685">FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</a></p><p>Pytorch 复现：</p><ul><li><a href="https://github.com/kekmodel/FixMatch-pytorch">kekmodel/FixMatch-pytorch</a></li><li><a href="https://github.com/CoinCheung/fixmatch-pytorch">CoinCheung/fixmatch-pytorch</a></li><li><a href="https://github.com/valencebond/FixMatch_pytorch">valencebond/FixMatch_pytorch:</a></li></ul><p>参考文章：</p><ul><li><a href="https://amitness.com/2020/07/semi-supervised-learning/">Semi-Supervised Learning in Computer Vision</a></li></ul><h2 id="主要工作"><a href="#主要工作" class="headerlink" title="主要工作"></a>主要工作</h2><p>FixMatch 结合了半监督学习之前的工作，结合<u><strong>伪标签（ pseudo-label）</strong></u>和 <u><strong>一致性正则（Consistency Regularization）</strong></u>，并且它更简单，相较于之前在半监督学习的工作，比如：UDA 和 ReMixMatch，如下图所示，FixMatch 在有标签数据上训练，将预测所得到的概率分布与真实标签计算损失 <code>L1</code>；</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522192955482.png" alt="image-20230522192955482"></p><p>对于无标签数据，FixMatch 做了两次数据增强，这是一个相较于其他工作特别的地方，FixMatch 分别对无标签数据做了<code>弱增强（Weakly Augmented）</code>和<code>强增强（Strongly Augmented）</code>，对于弱增强后的数据通过模型预测得到一个概率分布，并且，对这一概率分布做一特殊处理，最概率分布中最大的概率，设置一个阈值，若该概率大于该阈值，那么则将其纳入损失函数之中，并且，使用 <code>arg max</code>使其分布变为 <code>One-Hot</code>形式，接着与强增强后的数据通过模型预测得到的分布作交叉熵得到损失<code>L2</code>，最后通过加权和得到最终的损失。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522192925406.png" alt="image-20230522192925406"></p><p>其中 L1、L2 以及最终的损失如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522193829744.png" alt="image-20230522193829744"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522193843142.png" alt="image-20230522193843142"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522193917729.png" alt="image-20230522193917729"></p><p>这里提一下，在计算有标签数据，将对对其作一些<u><strong>弱增强</strong></u>的操作，一个很有意思的结果，对于权重系数 <code>λ</code>，在先前的工作中都显示对权重系数慢慢提升是一个很重要的部分，但在 FixMatch 中，提升权重系数这一部分以及包含在算法当中，就不需要额外进行一些计算将权重系数慢慢增大：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522194843295.png" alt="image-20230522194843295"></p><p>不需要添加这个权重系数的原因是：在损失函数中 <code>max(qb)</code>在早期训练中绝大部分数据通过模型预测得到的最大概率值都会比阈值<code>τ</code>小，在训练的过程中，模型的预测输出将会越来越“自信”，而将会有越来越多的数据样本的 <code>max(qb) &gt; τ </code>。</p><h2 id="数据增强方式"><a href="#数据增强方式" class="headerlink" title="数据增强方式"></a>数据增强方式</h2><h3 id="弱增强（Weak-Augmentation）"><a href="#弱增强（Weak-Augmentation）" class="headerlink" title="弱增强（Weak Augmentation）"></a>弱增强（Weak Augmentation）</h3><p>论文中使用了简单的<u><strong>翻转和平移（flip-and-shift）</strong></u>策略：</p><ul><li><p><strong>Random Horizontal Flip</strong></p><p><img src="https://amitness.com/images/fixmatch-horizontal-flip-gif" alt="Example of Random Horizontal Flip"></p><p>随机水平翻转，论文中使用了设置其概率为 0.5，对于 SVHN 数据集不适用该增强方式，因为 <u><strong>SVHN 数据集为都是数字</strong></u>，对数字进行翻转，没有意义，以下是基于 Pytorch 实现：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">im = Image.<span class="built_in">open</span>(<span class="string">'dog.png'</span>)</span><br><span class="line">weak_im = transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>)(im)</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>Random Vertical and Horizontal Translation</strong></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-translate.gif" alt="Example of Random Vertical and Horizontal Translation"></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">im = Image.<span class="built_in">open</span>(<span class="string">'dog.png'</span>)</span><br><span class="line">resized_im = transforms.Resize(<span class="number">32</span>)(im)</span><br><span class="line">translated = transforms.RandomCrop(size=<span class="number">32</span>, </span><br><span class="line">                                 padding=<span class="built_in">int</span>(<span class="number">32</span>*<span class="number">0.125</span>), </span><br><span class="line">                                 padding_mode=<span class="string">'reflect'</span>)(resized_im)</span><br></pre></td></tr></tbody></table></figure></li></ul><h3 id="强增强（Strong-Augmentation）"><a href="#强增强（Strong-Augmentation）" class="headerlink" title="强增强（Strong Augmentation）"></a>强增强（Strong Augmentation）</h3><p>FixMatch 对数据样本运用了 <code>RandAugment </code> 和 <code>CTAugment </code>后，在对其使用 <code>CutOut </code>增强方式。</p><ul><li><p>Cutout</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-cutout.gif" alt="Example of Cutout Augmentation"></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># Image of 520*520</span></span><br><span class="line">im = torch.rand(<span class="number">3</span>, <span class="number">520</span>, <span class="number">520</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fill cutout with gray color</span></span><br><span class="line">gray_code = <span class="number">127</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ratio=(1, 1) to set aspect ratio of square</span></span><br><span class="line"><span class="comment"># p=1 means probability is 1, so always apply cutout</span></span><br><span class="line"><span class="comment"># scale=(0.01, 0.01) means we want to get cutout of 1% of image area</span></span><br><span class="line"><span class="comment"># Hence: Cuts out gray square of 52*52</span></span><br><span class="line">cutout_im = transforms.RandomErasing(p=<span class="number">1</span>, </span><br><span class="line">                                 ratio=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">                                 scale=(<span class="number">0.01</span>, <span class="number">0.01</span>), </span><br><span class="line">                                 value=gray_code)(im)</span><br></pre></td></tr></tbody></table></figure></li><li><p>AutoAugment</p></li><li><p>RandAugment</p></li></ul><p>后面两个增强方式具体还没有看他的论文，就不介绍了…</p><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>对有标签的数据使用的 Batch 大小为 <code>B</code>，对于无标签的数据所使用的 Batch 大小为 <code>μB</code>，其中 <code>μ</code> 为超参数，作者使用 <code>μ = 7</code> 用于模型的训练：</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522201443131.png" alt="image-20230522201443131" style="zoom:80%;"><p>有监督学习部分，使用交叉熵计算损失函数：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-supervised-loss.png" alt="Supervised Part of FixMatch"></p><p>对未标记数据制作伪标签：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-pseudolabel.png" alt="Generating Pseudolabels in FixMatch"></p><p>计算一致性损失，所使用的损失函数也为交叉熵：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-strong-aug-loss.png" alt="Consistency Regularization in FixMatch"></p><p>对于上述所提到的权重系数 <code>λ</code>，在训练过程中，应用与训练的无标签数据将会越来越多，如下图所示，模型就好像是小孩子学习一般，从简单在到复杂：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-curriculum-learning.png" alt="Free Curriculum Learning in FixMatch"></p><p>算法如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522201915468.png" alt="image-20230522201915468"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522204054364.png" alt="image-20230522204054364"></p><p>在论文中，作者做了一个很有意思的实验，即只取 10 个有标签的数据（在 CIFAR-10 中，即每一个类别<u><strong>只取 1 张图片</strong></u>），作者随机挑选了 4 次，构建了 4 个不同的数据集，模型的准确率（中位数）为 64.28%（准确率范围为：48.58% - 85.32%）。</p><p>并且，训练数据集之间的差异并不是很大，理应得到的准确率应该相差不大，但作者发现了，4 个模型在第一个数据集中训练得到的准确率都达到了 61% - 67%，而在第二个数据集中准确率在 68% - 75%。</p><p>作者认为：训练的不稳定性与<u><strong>每个数据集中 10 个标记数据的质量</strong></u>有关，采样得到的低质量的标记数据，可能会让模型很难有效的学习信息，为了验证这一个观点，作者构建了 8 个新的训练数据集（各个类别中最有代表性的有标签数据 -&gt; … -&gt; 各个类别中最没有代表性的有标签数据）进行训练，结果如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/fixmatch-1-label-example.png" alt="Learning with just 1 image per class"></p><p>结果和假设一样，基于最有代表性的有标签数据组成的数据集训练达到了 78% 的准确率，而最没有代表性的有标签数据则为 10%，因此，可以验证，<u><strong>标记数据的质量好坏</strong></u>决定了模型的好坏。</p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p>由于 FixMatch 模型很简单，很容易就能实现（指代码量少），作者对与先前的工作未提到的 优化器（Optimizer）和 学习率更新的方式（ Learning Rate Schedule）花了很多的功夫研究。</p><h3 id="Ablation-Study-on-Optimizer"><a href="#Ablation-Study-on-Optimizer" class="headerlink" title="Ablation Study on Optimizer"></a>Ablation Study on Optimizer</h3><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522204944126.png" alt="image-20230522204944126"></p><p>作者在实验中发现：<u><strong>不同的优化器以及优化器中超参数的选择</strong></u>对性能有着很大的影响，并且，Adam 的性能并不比 momentum SGD 好，原因可能在于模型<u><strong>对于学习率的变化很敏感</strong></u>，如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522205017378.png" alt="image-20230522205017378"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522205036867.png" alt="image-20230522205036867"></p><h3 id="Ablation-Study-on-Learning-Rate-Schedule"><a href="#Ablation-Study-on-Learning-Rate-Schedule" class="headerlink" title="Ablation Study on Learning Rate Schedule"></a>Ablation Study on Learning Rate Schedule</h3><p>上文中所提到，作者发现模型对于学习率的变化很敏感，作者发现，在选择合适的衰减方式的同时，选择合适的衰减率也很重要：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522205424281.png" alt="image-20230522205424281"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522205529762.png" alt="image-20230522205529762"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上是我觉得比较有意思的地方，具体可细看论文。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Simplifying Semi-Supervised Learning with Consistency and Confidence&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;会议：NeurIPS 2020&lt;/p&gt;
&lt;p&gt;论文地址：&lt;a hre</summary>
      
    
    
    
    <category term="半监督学习" scheme="https://ytz7.github.io/categories/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="Semi-Surprised-Learning" scheme="https://ytz7.github.io/tags/Semi-Surprised-Learning/"/>
    
    <category term="机器学习" scheme="https://ytz7.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Hybrid Methods" scheme="https://ytz7.github.io/tags/Hybrid-Methods/"/>
    
  </entry>
  
  <entry>
    <title>Unsupervised-Data-Augmentation</title>
    <link href="https://ytz7.github.io/posts/77eb0ac8.html"/>
    <id>https://ytz7.github.io/posts/77eb0ac8.html</id>
    <published>2023-05-18T10:55:32.000Z</published>
    <updated>2023-09-13T09:03:09.924Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Unsupervised Data Augmentation for Consistency Training</p></blockquote><p>会议：NeurIPS 2019</p><p>论文地址：<a href="https://arxiv.org/abs/1904.12848v2"> Unsupervised Data Augmentation for Consistency Training</a></p><p>Pytorch 复现：<a href="https://github.com/sndnyang/vat_pytorch">sndnyang/vat_pytorch</a></p><p>参考文章：</p><ul><li><a href="https://amitness.com/2020/07/semi-supervised-learning/">Semi-Supervised Learning in Computer Vision</a></li><li><a href="https://blog.csdn.net/by6671715/article/details/122706003">码侯烧酒的博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/369023559">Unsupervised Data Augmentation for Consistency Training </a></li></ul><p>参考视频：</p><ul><li><a href="https://www.bilibili.com/video/BV1Ab4y1S7i5/?spm_id_from=333.999.0.0&amp;vd_source=7801e0201bc8ddcb3d951c7388a08c3e">数最后一名</a></li></ul><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>最近的半监督学习方式都是基于一致性学习（consistency training）的，尽管成果不错，但是它们添加噪声的方式都比较初级，大多数都是基于 Gaussian noise， dropout noise 或者是 adversarial noise。因此，作者想引入一些更高级的数据增强方式，尤其是那些在监督学习中已经证明有效的方法，看看它们在半监督学习中是否也是有效的。</p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>少样本标注情况下，充分利用无标注数据，使其尽可能达到有充分标注的效果。</p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>根据一致性训练的原则，认为对一个样本进行较好的数据增强后，预测标签不应该发生变化，因此一般来说会有两个部分的 loss：（1）标注样本的损失，一般采用<code>交叉熵</code>；（2）未标注样本的损失，<code>KL 散度</code>等。</p><p>流程图如下所示，对有标签样本计算损失 L1，对无标签样本执行一次预测得到一个结果，再对改变本做一次特殊的数据增强后在进行预测得到一个结果，将两个结果计算一致性损失 L2，将 L1 与 L2 计算加权和。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519192319207.png" alt="image-20230519192319207"></p><p>数学形式说明该流程，输入 <code>x</code>，计算输入分布 <code>p_θ(y|x)</code> 和带噪声的分布 <code>p_θ(y|x, ɛ)</code>，然后最小化两个分布之间的距离：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519193105506.png" alt="image-20230519193105506"></p><p>上述图片对应的损失函数如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519193157993.png" alt="image-20230519193157993"></p><p>其中，<code>λ</code> 为平衡有监督学习和一致性训练之间的损失，<code>CE</code> 为交叉熵，<code>f*(.)</code> 为索要训练的最终目标模型。</p><p><u><strong>但有一点很奇怪</strong></u>，作者在之前的版本中计算一致性损失所采用的计量方式为 KL 散度，但是在最新版本又换回了 Cross-Entropy，不懂这是为什么…可能看效果而异吧。</p><p>不过有的人说：</p><blockquote><p>MSE 和 KL 散度各有优劣，二者的选取与数据集的实际分布特征关系很大，在实践中不妨进行对比测试</p></blockquote><p>在论文中，作者正对计算机视觉和自然语言任务提出了不同的对应增强方法，其中对于图片的增强方式，作者使用了一个增强方法 <u><strong>RandAugment</strong></u>，（只了解了大概，具体论文没看，好像是从强化学习中的 AutoAugment 衍生而来的）。不同任务的效果如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519193829228.png" alt="image-20230519193829228"></p><h2 id="训练技巧（Tricks）"><a href="#训练技巧（Tricks）" class="headerlink" title="训练技巧（Tricks）"></a>训练技巧（Tricks）</h2><h3 id="数据层面"><a href="#数据层面" class="headerlink" title="数据层面"></a>数据层面</h3><ol><li><p>基于置信度的掩码（Confidence-based masking）</p><p>对预测效果不好的样本（指针对一致性预测的原始样本），即置信度（最大的样本的概率）小于一定阈值数据，不计入一致性损失。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519194350155.png" alt="image-20230519194350155"></p></li><li><p>输出分布锐化（Sharpening Predictions）</p><p>降低预测结果的熵对训练有好处，所以作者对无标签的预测见过做<u><strong>锐化</strong></u>，采用的方法为 <u><strong>low Softmax temperature τ</strong></u>，与方法一相结合，在 batch 大小为 B 的情况下，损失为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519194803301.png" alt="image-20230519194803301"></p></li><li><p>域外相关数据的选择（Domain-relevance Data Filtering）</p><p>无标签数据量大，其中的数据分布往往是不均衡且有大量任务无关数据，为了解决这个问题，论文提出一种通用的无标签数据分布均衡化策略。首先利用有标签数据训练一个初始化模型，然后去预测所有无标签数据，根据置信度均衡选择各类别数据。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519194958974.png" alt="image-20230519194958974"></p></li></ol><h3 id="训练层面"><a href="#训练层面" class="headerlink" title="训练层面"></a>训练层面</h3><p>Training Signal Annealing（训练信号退火）</p><p>主要是针对标签数据与未标签数据不平衡时的场景，由于有大量的未标签数据需要UDA 处理，所以需要一个较大模型，但是由于较大模型很容易在少量标签数据下过拟合，所以，提出了本方法用于解决该问题。</p><p>基本原理就是在训练过程中，随着未标签数据的增加，逐步去除带标签数据，从而避免模型过拟合到带标签的训练数据。具体而言，就是在训练的 <code>t</code> 时刻，设置一个阈值 <code>ηt</code>，当 <code>1/K ≤ ηt ≤ 1</code>，其中，<code>K</code> 是类别数，当某个标签数据计算的 <code>p_θ(y∗| x)</code>大于阈值<code>ηt</code>，将该标签数据移除出计算损失的过程，而只计算 miniBatch 里面的其余数据。</p><p>具体策略有 3 种：</p><ul><li>log-schedule</li><li>linear-schedule</li><li>exp-schedule</li></ul><p>对于 labeled 数据量少，容易过拟合情况，选择最后一种；对于 labeled 数据量较多，过拟合不严重，可视情况选择前两种。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230519195814680.png" alt="image-20230519195814680"></p><h2 id="理论方面的探讨（Theoretical-Analysis）"><a href="#理论方面的探讨（Theoretical-Analysis）" class="headerlink" title="理论方面的探讨（Theoretical Analysis）"></a>理论方面的探讨（Theoretical Analysis）</h2><blockquote><p>理论渣渣只能意会不可用文字整理出来，具体看论文吧…</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>给人的感觉这一篇好“工程”😂，在数据方面做了很多文章，但半监督领域又好像都是在对数据这一方面做很多“动作”，不管了，接着看，接着读。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Unsupervised Data Augmentation for Consistency Training&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;会议：NeurIPS 2019&lt;/p&gt;
&lt;p&gt;论文地址：&lt;a href=&quot;https://ar</summary>
      
    
    
    
    <category term="半监督学习" scheme="https://ytz7.github.io/categories/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="Semi-Surprised-Learning" scheme="https://ytz7.github.io/tags/Semi-Surprised-Learning/"/>
    
    <category term="机器学习" scheme="https://ytz7.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Data Augmentation" scheme="https://ytz7.github.io/tags/Data-Augmentation/"/>
    
    <category term="Consistency Regularization" scheme="https://ytz7.github.io/tags/Consistency-Regularization/"/>
    
  </entry>
  
  <entry>
    <title>Virtual Adversarial Training</title>
    <link href="https://ytz7.github.io/posts/cc87ae2e.html"/>
    <id>https://ytz7.github.io/posts/cc87ae2e.html</id>
    <published>2023-05-17T11:50:25.000Z</published>
    <updated>2023-09-13T09:03:09.924Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>A Regularization Method for Supervised and Semi-Supervised Learning</p></blockquote><p>期刊：IEEE Transactions on Pattern Analysis and Machine Intelligence (2018)</p><p>论文地址：<a href="https://arxiv.org/abs/1704.03976"> Virtual Adversarial Training</a></p><p>Pytorch 复现：<a href="https://github.com/sndnyang/vat_pytorch">sndnyang/vat_pytorch</a></p><p>参考文章：</p><ul><li><a href="https://amitness.com/2020/07/semi-supervised-learning/">Semi-Supervised Learning in Computer Vision</a></li><li><a href="https://blog.csdn.net/by6671715/article/details/122723657"> 码侯烧酒的博客</a></li><li><a href="https://facico.blog.csdn.net/article/details/114634056?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-114634056-blog-119420855.235%5Ev36%5Epc_relevant_default_base&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-114634056-blog-119420855.235%5Ev36%5Epc_relevant_default_base&amp;utm_relevant_index=1">Facico的博客</a></li></ul><p>参考视频：</p><ul><li><a href="https://www.bilibili.com/video/BV1j64y1v7p4?p=1&amp;vd_source=7801e0201bc8ddcb3d951c7388a08c3e">数最后一名</a></li></ul><h2 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h2><p>论文的关键想法同样保持着<u><strong>一致性正则化</strong></u>的思想，只不过将图像的增强方式以生成<u><strong>对抗样本（Adversarial Example）</strong></u>来代替之，有标签的损失采用<u><strong>交叉熵（Cross-Entropy）</strong></u>计算，无标签的损失使用 <u><strong>KL 散度（KL Divergence）</strong></u>来计算，最后施以对应的权重得到最终的损失，大致流程图如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517202628300.png" alt="image-20230517202628300"></p><h2 id="一致性正则化"><a href="#一致性正则化" class="headerlink" title="一致性正则化"></a>一致性正则化</h2><p>具体来说，基于平滑假设和聚类假设，具有不同标签的数据点在低密度区域分离，并且相似的数据点具有相似的输出。如果对一个未标记的数据应用实际的扰动，其预测结果不应该发生显著变化，也就是输出具有一致性。其数学表达如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517203120942.png" alt="image-20230517203120942"></p><p>其中，<code>D</code> 为度量函数，一般采用 <code>KL 散度</code> 或者 <code>JS 散度</code>，当然也可以使用交叉熵或者平方误差等，<code>Augment(·)</code> 是指数据增强函数，当采用数据增强是，视为对样本/模型添加一些噪声扰动，<code>θ</code> 为模型参数。</p><p>常见的数据增强有以下：</p><ul><li>常规的数据增强：平移旋转，随机 dropout等</li><li>时序移动平均：Temporal Ensembling，Mean-Teacher 中使用的方法</li><li>对抗样本扰动：VAT</li><li>高级数据增强：<a href="https://arxiv.org/abs/1904.12848v2">UDA</a></li></ul><h2 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h2><p>对抗训练（Adversarial Training）是增强神经网络鲁棒性的重要方式，在对抗训练的过程中，样本会被混合一些微小的扰动（哪怕改变很小，但很有可能会造成错误分类），使神经网络适应这种改变，从而对对抗样本具有鲁棒性。</p><p>生成对抗样本有以下几种常见方式：</p><ul><li><p>基于梯度法: <a href="https://arxiv.org/abs/1412.6572">Explaining and Harnessing Adversarial Examples</a></p></li><li><p>基于超平面分类: <a href="https://arxiv.org/abs/1511.04599">DeepFool: a simple and accurate method to fool deep neural networks</a></p></li><li><p>对抗攻击（Adversarial Attack）：在模型原始输入上添加对抗扰动</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/2020101817001880.png" alt="img"></p></li></ul><p>Goodfellow 对对抗训练损失函数定义如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517204302784.png" alt="image-20230517204302784"></p><p>其中，<code>D</code> 是衡量两个分布相似度的函数，<code>q(y|xl)</code> 是样本的真实分布，<code>p(y|xl, θ)</code> 是由参数和 <code>xl</code> 生成的预测分布，通过增加扰动 <code>radv</code> 来使得两个分布尽量相似。</p><p>对抗为什么可行？</p><ul><li>因为很多网络被设计得十分“线性”，像 LSTM 这样的，对x的每个维度都做微小扰动，当x的维度变大的时候，会对网络造成较大的影响</li><li>网络的线性，使得高阶导近似0，Taylor 展开后占主导的是线性的部分，所以用来干扰的主要就是对抗样本中线性的部分</li></ul><p>通常，我们无法获得精确对抗性扰动的<u><strong>闭式解</strong></u>，不过可以通过上式中的度量 D 来线性近似 r，使用 <code>L2</code> 正则是，对抗扰动可以通过下面的式子近似：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517205056447.png" alt="image-20230517205056447"></p><p>使用 <code>L∞</code> 正则时，可通过下面的式子近似：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517205330497.png" alt="image-20230517205330497"></p><p>其中 <code>g</code> 为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517205353604.png" alt="image-20230517205353604"></p><p>可通过<u><strong>反向传播</strong></u>进行计算，对抗方法得到的扰动方向，比随机找一个扰动更好。</p><p>对抗训练存在的缺点：仅仅只能适配于有监督学习，当样本没有标签就不能进行，故作者提出了一种可以运用于<u><strong>半监督学习</strong></u>的对抗训练方法。</p><h2 id="Virtual-Adversarial-Training-VAT"><a href="#Virtual-Adversarial-Training-VAT" class="headerlink" title="Virtual Adversarial Training, VAT"></a>Virtual Adversarial Training, VAT</h2><p>Adversarial Direction，对抗性方向，其概念是指能够最大程度减少准确分类的概率的方向，如下图所示，寻找扰动项 <code>radv</code>，其投影点与准确分类的方向为对抗性方向。</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/1684329811138.png" alt="1684329811138" style="zoom: 50%;"><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230517212606646.png" alt="image-20230517212606646"></p><p>local distributional smoothness（LDS），定义为衡量当前基于每个输入数据的模型的平滑度的负函数。</p><h3 id="推导流程"><a href="#推导流程" class="headerlink" title="推导流程"></a>推导流程</h3><blockquote><p>由于不知道为什么公式渲染不出来，我就不手打公式了，等有时间再去把这个问题弄了，我就直接套用现成已有的公式，或是直接手写出来，Sorry…</p></blockquote><p>文中的符号定义如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518145424158.png" alt="image-20230518145424158"></p><p>在上面，我们提到，<u><strong>对抗训练</strong></u>只适用于带标签的有监督问题，在半监督学习中并不是很适用，所以，作者就提出了<u><strong>虚拟对抗训练</strong></u>，<code>VAT</code> 的损失函数如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518145654097.png" alt="image-20230518145654097"></p><p>其中，<code>x⁎</code>可以表示为有标签或者无标签数据，正如上文中所提到的，在实际中，并没有关于 <code>q(y, xul)</code> 的直接信息，因此可以使用<code>p(y|x, θ)</code>来替换之，如果带标签的样本比较多时，<code>p</code>就会逼近<code>q</code>，利用<code>p</code>生成的虚拟标签代替不知道的标签，并根据虚拟标签计算对抗方向，此时的虚拟标签是用上一步训练之后得到的模型进行估计，故损失函数更新如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518150409362.png" alt="image-20230518150409362"></p><p>将损失函数求平均，得到正则项（<strong>regularization term</strong>）：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518150651440.png" alt="image-20230518150651440"></p><p>完整的目标函数为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518150735667.png" alt="image-20230518150735667"></p><p>其中第一项为<u><strong>带标签数据的负对数似然函数</strong></u>（针对有标签数据），VAT 的一个优点是：只有两个超参数：（1）对抗方向的限制参数（norm constraint）<code>ɛ &gt; 0</code>；（2）控制目标函数的两个目标项的相对平衡的正则化系数（regularization coefficient）<code>α &gt; 0</code>，事实上，作者指出，两个超参数的作用大抵相同，于是 VAT 中只微调了超参数 <code>ɛ</code>，而将 <code>α</code> 固定为 1。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518151554425.png" alt="image-20230518151554425"></p><p>对于上述目标函数，观察到，当 r = 0 时，D 永远为 0，所以需要对 D 进行二阶泰勒展开，这里的 D 为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518151854028.png" alt="image-20230518151854028"></p><p>二阶展开后得到：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518151914984.png" alt="image-20230518151914984"></p><p>其中 H 为海森矩阵（Hessian Matrix），具体如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518152200457.png" alt="image-20230518152200457"></p><p>故 <code>rvadv</code> 可以近似为：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518152332268.png" alt="image-20230518152332268"></p><p>其中：u 是 H 的第一特征主向量，上划线表示同方向单位向量（算了我写出来.）</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/1684394786562.png" alt="1684394786562"></p><p>在计算 H 的特征值/特征向量时，时间复杂度需要 <code>O(I³)</code>，论文中提到使用幂迭代法（ power iteration method）和有限差分法（finite difference method）来近似求解，假设 <code>d</code> 为与特征向量不垂直的一个随机单位向量，迭代计算如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153132095.png" alt="image-20230518153132095"></p><p>注意到 <code>Hd</code> 可以被有限差分来近似计算：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153216356.png" alt="image-20230518153216356"></p><p>由于 D 的一介导还是很可能为 0，所以 r 用如下方式迭代更新：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153419621.png" alt="image-20230518153419621"></p><p><u><strong>迭代的次数越多效果越好</strong></u>，甚至很多数据集上迭代次数为 1 都取得了较好的效果，迭代次数如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153804154.png" alt="image-20230518153804154"></p><p>伪代码如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518153918543.png" alt="image-20230518153918543"></p><p>效果如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518154205948.png" alt="image-20230518154205948"></p><p>迭代次数较少的情况下，有大量的<u><strong>未标记点（灰色点）</strong></u>会有较高的<u><strong>LDS（深蓝色）</strong></u>，即不平滑，因为模型一开始对相同类别的 point 预测了不同的 label，VAT 会让 LDS 较高的数据点更大的压力，使得数据点间边界更平滑。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>文章的亮点就是对于二阶导以及之后的各种近似优化，理论方面很充足，实验也很充足，理论方面对于我这个渣硕来说真的挺艰难，我在去年 12 月份就准备看这一篇，结果因为理论被劝退，终于时隔数月，我算是啃下来了…不过也还是云里雾里的，还是得补充一下数学理论知识啊，果然我导说的没错，理论知识都不懂，给我发论文看我都看不懂，那还搞啥呢，哈哈🤦‍♂️，就这样子，如果有机会再复盘这篇论文，有问题在更新。</p><p>最后附上我得手写版公式（惨不忍睹的被公式薄纱了）：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518155255116.png" alt="image-20230518155255116"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518155319698.png" alt="image-20230518155319698"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;A Regularization Method for Supervised and Semi-Supervised Learning&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;期刊：IEEE Transactions on Pattern Ana</summary>
      
    
    
    
    <category term="半监督学习" scheme="https://ytz7.github.io/categories/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="Semi-Surprised-Learning" scheme="https://ytz7.github.io/tags/Semi-Surprised-Learning/"/>
    
    <category term="机器学习" scheme="https://ytz7.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Consistency Regularization" scheme="https://ytz7.github.io/tags/Consistency-Regularization/"/>
    
    <category term="Adversarial Example" scheme="https://ytz7.github.io/tags/Adversarial-Example/"/>
    
  </entry>
  
  <entry>
    <title>关于深度学习中遇到的知识盲区</title>
    <link href="https://ytz7.github.io/posts/eaa3f406.html"/>
    <id>https://ytz7.github.io/posts/eaa3f406.html</id>
    <published>2023-05-11T11:47:01.000Z</published>
    <updated>2023-09-13T09:03:09.924Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于深度学习的一些问题与知识盲区"><a href="#关于深度学习的一些问题与知识盲区" class="headerlink" title="关于深度学习的一些问题与知识盲区"></a>关于深度学习的一些问题与知识盲区</h1><p>此篇用于记录在阅读论文以及博客时遇到的一些问题和知识盲区，方便日后进行复盘。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="为什么需要激活函数？"><a href="#为什么需要激活函数？" class="headerlink" title="为什么需要激活函数？"></a><strong>为什么需要激活函数？</strong></h3><p>通常激活函数都是非线性的，它能够帮助我们引入非线性因素，使得神经网络能够更好地解决更加复杂地问题，在简单的二分类问题中，如果不使用激活函数，使用简单的**<u>逻辑回归</u>**，那么该模型只能作简单的线性分类，而不能作复杂的非线性划分，如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511195806300.png" alt="image-20230511195806300"></p><p>值得一提的是，如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，效果与单个神经元无异。另外，如果是拟合问题而不是分类问题，输出层的激活函数可以使用线性函数。</p><h3 id="Sigmoid-函数饱和带来的问题"><a href="#Sigmoid-函数饱和带来的问题" class="headerlink" title="Sigmoid 函数饱和带来的问题"></a>Sigmoid 函数饱和带来的问题</h3><p>Sigmoid 函数的取值范围在 (0,1) 之间，单调连续，求导容易，一般用于二分类神经网络的输出层。</p><p>Sigmoid 函数饱和区范围广，容易造成梯度消失。饱和区如下图所示，图中红色椭圆标注的饱和区曲线平缓，梯度的值很小，近似为零，且 Sigmoid 函数的饱和区范围很广，除了 [-5,5]，其余区域都可以认为是饱和区，这种情况很容易造成梯度消失，梯度消失会增大神经网络训练难度，影响神经网络模型的性能。</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511200333796.png" alt="image-20230511200333796" style="zoom:50%;"><h2 id="降噪自编码器-Denoising-Auto-Encoder"><a href="#降噪自编码器-Denoising-Auto-Encoder" class="headerlink" title="降噪自编码器 Denoising Auto-Encoder"></a>降噪自编码器 Denoising Auto-Encoder</h2><p>在神经网络模型训练阶段开始前，通过 Auto-encoder 对模型进行预训练可确定编码器 W 的初始参数值。然而，受模型复杂度、训练集数据量以及数据噪音等问题的影响，通过 Auto-encoder 得到的初始模型往往存在过拟合的风险。</p><p>简单理解，在人类的感知过程中，某些模态的信息对结果的判断影响并不大。举个例子，一块圆形的饼干和一块方形的饼干，在认知中同属于饼干这一类，因此形状对我们判断是否是饼干没有太大作用，也就是<strong>噪声</strong>。如果不能将形状数据去除掉，可能会产生“圆饼干是饼干，方饼干就不是饼干”的问题（过拟合）。</p><p>当采用无监督的方法分层预训练深度网络的权值时，为了学习到较鲁棒的特征，可以**<u>在网络的可视层（即数据的输入层）引入随机噪声</u>**，这种方法称为降噪自编码器（<a href="https://dl.acm.org/doi/abs/10.1145/1390156.1390294">Denoising Auto-Encoder， DAE</a>）。</p><blockquote><p>降噪自编码器：一个模型，能够从有噪音的原始数据作为输入，而能够恢复出真正的原始数据。这样的模型，更具有鲁棒性。</p></blockquote><p>以下是以经典的 MNIST 手写数字识别为例，对于输入的数据引入了变换角度、随机噪点、添加背景图像等噪音。模型通过训练后可以对有噪音图像更加鲁棒，而这也更符合实际使用的需求。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-2b0c0b5855f0c98b44fbeaf75d6a72dd_r.jpg" alt="img"></p><p>对于有噪音的输入数据，区别于一般自编码机，降噪自编码机要做的就是数据的降噪。关于降噪的过程如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-9ab89078a4630eb52347244a61410c2f_r.jpg" alt="img"></p><p>对于输入层 $x$，以一定概率将其节点置 0，得到 $\hat{x}$,用 $\hat{x}$ 去计算 $y$,计算 $z$ ，并将 $z$ 与原始 $x$ 做误差迭代，对结果误差较小的节点可以认为是噪声。每层处理重复上述工作。</p><p>自编码器的本质是学习一个相等函数，即网络的输入和重构后的输出相等，这种相等函数的表示有个缺点就是当测试样本和训练样本不符合同一分布，效果不好，而降噪自编码器在这方面的处理有所进步。</p><h2 id="随机深度-Stochastic-Depth"><a href="#随机深度-Stochastic-Depth" class="headerlink" title="随机深度 Stochastic Depth"></a>随机深度 Stochastic Depth</h2><p>针对于残差模块的优化，由于很深的 ResNet 通常需要很长时间的训练(也就是训练很慢)，作者引入了一种类似于 dropout 的方法，在训练过程中随机丢弃子图层（randomly drop a subset of layers），而在推断时正常使用完整的图。</p><p>ResNet 网络是由一个接一个的残差模块(ResBlock)串联起来的，可以视为ResBlock的集合。在训练时，对每个 ResBlock 随机 drop（按伯努利分布），drop 就是将上一个 ResBlock 直接输出到下一个 ResBlock，被 drop 的 ResBlock 什么都不做也不更新。另外，网络的输入被视为第一层，是不会 drop 的。</p><p>与 Dropout 的不同之处在于，该方法 drop 整个 ResBlock，而 Dropout 在训练期间只 drop 一部分神经元节点。这种方法大大降低了训练时间，甚至在训练完成后删除部分layer，还能不影响精度。</p><h2 id="最小化信息熵-Entropy-Minimization"><a href="#最小化信息熵-Entropy-Minimization" class="headerlink" title="最小化信息熵 Entropy Minimization"></a>最小化信息熵 Entropy Minimization</h2><p>参考文章：<a href="https://kexue.fm/archives/5448">最小熵原理（一）</a></p><p>在半监督学习中，有标签（分类完全准确）的数据样本通常相对较少，通过训练模型对未标记数据样本进行预测，选择出高置信度的样本，作为标记样本同有标签样本作为下一次训练的数据样本。</p><p><u><strong>Entropy Minimization</strong></u> 是一种在半监督学习中使用的技术，它的目的是最小化信息熵，从而使模型在分类时的不确定性最小。在半监督学习中，我们希望模型尽可能地利用未标记数据来学习，但是这些数据并不带有正确的标签，因此我们需要利用某些技术来帮助我们学习这些数据。</p><p><u><strong>加快模型学习进度的唯一方法就是降低学习目标的冗余信息量</strong></u>，所提到的“去冗余”，可以理解为“省去没必要的学习成本”。</p><p>也就是通常所使用到的技巧：过滤掉低置信度的未标记样本，保留高置信度样本。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512164646319.png" alt="image-20230512164646319"></p><h2 id="Top-1-and-Top-5-Accuracy"><a href="#Top-1-and-Top-5-Accuracy" class="headerlink" title="Top-1 and Top-5 Accuracy"></a>Top-1 and Top-5 Accuracy</h2><p>Top-1：</p><p>在多分类问题中，一般认为在经过全连接层后得到的概率分布中概率最大的类别为我们模型的预测类别，就判断为正确。</p><p>Top-5：</p><p>在多分类问题中，一般认为在经过全连接层后得到的概率分布中概率最大的全五个类别中有我们模型的预测类别，就判断为正确。</p><h2 id="Dark-Knowledge"><a href="#Dark-Knowledge" class="headerlink" title="Dark Knowledge"></a>Dark Knowledge</h2><p>可以看作是经过 Softmax 函数后得到的各类类别的可能性，其包含着类别之间的相关性，比如，猫和狗的相似性，要远远大于猫和船的相似性，而这种相似性，会在概率值中有所体现，而这部分信息一致没有被很好的利用，所以称之为 Dark Knowledge。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;关于深度学习的一些问题与知识盲区&quot;&gt;&lt;a href=&quot;#关于深度学习的一些问题与知识盲区&quot; class=&quot;headerlink&quot; title=&quot;关于深度学习的一些问题与知识盲区&quot;&gt;&lt;/a&gt;关于深度学习的一些问题与知识盲区&lt;/h1&gt;&lt;p&gt;此篇用于记录在阅读论文以及博</summary>
      
    
    
    
    <category term="深度学习" scheme="https://ytz7.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://ytz7.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="深度学习" scheme="https://ytz7.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>半监督学习学习过程</title>
    <link href="https://ytz7.github.io/posts/52ee1a8c.html"/>
    <id>https://ytz7.github.io/posts/52ee1a8c.html</id>
    <published>2023-05-11T07:08:40.000Z</published>
    <updated>2023-09-13T09:03:09.924Z</updated>
    
    <content type="html"><![CDATA[<h1 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h1><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>学习了快一个学期的 Java 程序开发，让我感觉开发的过程真的好空洞…但是我并不知道下一步到底要做什么，又处于想找实习又怕找不到的情况下，刷了几天 LeetCode，然后又变成了“小🐏人”，紧接着被隔离😅，真的挺痛苦，所幸在第三天就“出狱”了，在蹲“监狱”期间，受到一个“狱友”的启发，“还是要做好职业规划的，若是没有做好职业规划，像一只无头苍蝇一样，很可能在毕业之际还是找不到工作。”在思考了一天后，我决定重新开始在“半监督学习”这个领域开始零基础学习了，为什么会有这个想法？是因为我觉得我起初考研的目的是为了能够在人工智能这个领域继续深入了解，目的很单纯，那为什么我不坚持一下呢？于是我决定重新开始这个计划，管他呢，起码是自己喜欢做的事情，哪怕未来找不到工作什么的，起码现在是由试错的资本的。</p><p>以下将会记录我在学习时候遇到的问题/解决方式/新思路/感想。</p><h2 id="Self-Training"><a href="#Self-Training" class="headerlink" title="Self-Training"></a>Self-Training</h2><p>自训练方法，模型基于已标记好的训练集进行训练，得到一个基础模型，利用该基础模型对未标记的数据集进行预测一个<strong>伪标签</strong>，然后将两个数据集整合训练，得到一个新的模型，从而迭代更新模型参数，生成一个最优模型。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161304502.png" alt="image-20230511161304502"></p><h3 id="Pseudo-Label"><a href="#Pseudo-Label" class="headerlink" title="Pseudo-Label"></a><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.664.3543&amp;rep=rep1&amp;type=pdf">Pseudo-Label</a></h3><p>伪标签技术适用于小样本学习，实际上在样本极其珍贵的金融、医疗图像、安全等领域，伪标签学习有时候很有效。</p><p>伪标签的定义来自于半监督学习，半监督学习的核心思想是<u><strong>通过借助无标签的数据来提升有监督过程中的模型性能</strong></u>。</p><p>粗略来讲，伪标签技术就是<u><strong>利用在已标注数据所训练的模型在未标注的数据上进行预测，根据预测结果对样本进行筛选，再次输入模型中进行训练的一个过程</strong></u>。</p><p>如下图所示，利用有标签的数据集训练出一个模型，运用训练出的模型给予无标签数据一个<u><strong>伪标签</strong></u>。如何定义所属类别？利用训练好的模型对无标签数据进行预测，以概率最高的类别作为无标签数据的伪标签。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161425308.png" alt="image-20230511161425308"></p><blockquote><p>entropy regularization：用于防止模型过拟合，通过在损失函数中加入熵（entropy）项来实现</p></blockquote><p>利用 <em>entropy regularization</em> 思想，将无监督数据转为目标数据的正则项，即将拥有伪标签的无标签数据视为有标签的数据，利用交叉熵（与最初训练模型一致）来评估误差大小。</p><p>模型整体的目标函数如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-103a30551a86aa99222f8f35129088d7_r.jpg" alt="img"></p><p>其中左边一项为交叉熵，用来评估有标签数据的误差，右边一项即为 <em>entropy regularization</em> 项，用来从无标签的数据中获取训练信号。</p><p>为了平衡有标签数据和无标签数据的信号强度，如上所示，算法在目标函数中引入了时变参数 α(t)，其数学形式如下，其中 T1 和 T2 都为超参数：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-4d5a8a1d8aa4b9037859ab19dd346c0b_r.jpg" alt="img"></p><p>因此，随着训练时间的增加，α(t) 将会从零开始线性增长至某个饱和值，对应无标签数据的信号也将逐渐释放出来。背后的核心想法也很直观，早期模型预测效果不佳，因此 <em>entropy regularization</em> 产生信号的误差也较大，因而 α(t) 应该从零开始，由小逐渐增大。</p><p><strong>存在不足：</strong>只在训练时间这个维度上，采用了退火思想，即采用时变系数α(t)。而在伪标签这个维度，对于模型给予的预测标签一视同仁，这种方法在实际中存在明显问题。若模型在对伪标签的数据预测后， 10 个类别预测概率值都接近于 0.1，以最大概率这一原则选择对应的标签，那么这个标签对模型的训练会造成一定的<u><strong>副作用</strong></u>。</p><p><strong>设想如何突破这一不足？</strong></p><p>也许可以设定一个阈值，抛弃那些预测最大概率值小于该阈值的未标记样本，将满足条件的未标记样本分配伪标签，并加入模型评估当中，之后再迭代训练。</p><h3 id="Noisy-Student"><a href="#Noisy-Student" class="headerlink" title="Noisy Student"></a><a href="https://arxiv.org/abs/1911.04252">Noisy Student</a></h3><p>论文的关键 idea 是训练两个模型，“teacher”和“student”，强调的是在student模型中加入噪声，teacher 模型和 student 模型可以用不同的模型训练，也可以使用相同的模型。</p><p>在有标签数据中训练“teacher”模型，并利用该模型对未标记数据进行推断伪标签，这些伪标签可以是<u><strong>软标签</strong></u>，也可以取其最大概率的类别将其转换为<u><strong>硬标签</strong></u>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161507462.png" alt="image-20230511161507462"></p><p>然后将标记数据与为标记数据（带有伪标签）置入“student”进行训练，在训练之前<u><strong>数据增强</strong></u>使用 <strong>RandAugment</strong>，待“student”模型训练好后，使用最新的模型作为新的“teacher”，进行下一次迭代，此过程会重复几次（通常为 3 次）。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511211613877.png" alt="image-20230511211613877"></p><p>总结该篇论文的流程思路：</p><ol><li>首先将在 ImageNet 上训练好的模型作为 Teacher Network 来训练 Student Network</li><li>再使用训练好的 T 网络（无噪音）来对另一个数据集 [JFT dataset] 生成尽可能准确的伪标签</li><li>之后使用生成伪标签的数据集 [JFT dataset] 和 ImageNet 一起训练 Student Network</li><li>Student Network中增加了模型噪音<ul><li>Dropout</li><li>随机深度  Stochastic Depth</li><li>数据噪音：对图片进行数据增广（RandAugment）</li></ul></li></ol><p>对 Student 模型添加噪音的作用：</p><ol><li><strong>数据噪音</strong>：提高泛化能力</li><li><strong>模型噪音</strong>：提高模型鲁棒性和泛化能力</li></ol><p>具体参数设置：</p><ol><li><strong>Stochastic Depth</strong>：幸存概率因子为 0.8</li><li><strong>Dropout</strong>：分类层（final layer）引入 0.5 的丢弃率</li><li><strong>RandAugment</strong>：应用两个随机计算，其震级设置为 27</li></ol><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512204307583.png" alt="image-20230512204307583"></p><p>其他 Tricks：</p><ol><li><strong>数据过滤</strong>：将教师模型中置信度不高的图片过滤，因为这通常代表着域外图像（out-of-domain data）</li><li><strong>数据平衡</strong>：平衡不同类别的图片数量，当一个类别所对应的图片数量不是很充足时，会采取<u>随机复制</u>的方法来扩充样本量</li><li><strong>软标签（Soft Pseudo-Label）</strong>：在消融实验中表示，软标签对域外图像有更强的指导作用</li></ol><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p>1.噪音是否对模型有影响？（The Importance of Noise in Self-training）</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512205434466.png" alt="image-20230512205434466" style="zoom:80%;"><p>从结果可以看出，<u>噪音</u>、<u>随机深度</u>、<u>数据扩充</u>起着重要的作用使学生模型胜过教师模型，对此有人提出是不是对未标记数据加入正则项以防止过拟合来代替噪音，作者在实验中说明这是不对的。因为在去噪的情况下，未标记图像的训练损失并没有下降多少以此说明模型并没有对未标记数据过拟合。</p><p>2.对于迭代训练的消融实验（A Study of Iterative Training）</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512205650571.png" alt="image-20230512205650571" style="zoom:80%;"><p>作者先在<u><strong>标记数据</strong></u>上训练了 EfficientNet-B7 作为 Teacher，然后再训练 EfficientNet-L2 作为 Student，然后让 Student 作为 Teacher 依次迭代三轮，作者表明，迭代训练提高了准确度，并且，给出再最后通过调整未标记图像和标记图像的比为 <strong>1 : 28</strong> 时达到最优 Top-1 Acc.。</p><p>3.能力强的教师模型会不会对学生模型造成的影响</p><p>4.无标签的数据量大小</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512210409208.png" alt="image-20230512210409208" style="zoom:80%;"><p>作者按照比例分别从整个数据集中<u><strong>均匀采样（uniformly sampling）</strong></u>，会发现在数据量减少至 1/16 中，模型的性能大都相似，在数据量达到 1/32 或更小后，模型性能有了显著的下降（可能 .3 个点就可以算是显著的下降了吧…），所以，使用大量未标记的数据会产生更好的性能，但作者指出：<u>对于大模型来说，数据量越多越好，而小模型由于容量限制则很容易饱和</u>。</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512210908139.png" alt="image-20230512210908139" style="zoom:80%;"><p>5.硬标签和软标签对域外图像的影响</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512210946955.png" alt="image-20230512210946955" style="zoom:80%;"><p>作者将预测置信度高（high-confidence）的图像视作域内图像（in-domain images），反之，将预测置信度低（low-confidence）的图像视作域外图像（out-of-domain images），作者表明：对于域内图像，软伪标签（ soft pseudo labels）和硬伪标签（hard pseudo labels）都对模型有一定的帮助；而对于域外图像，软伪标签使得模型对域外图像的判断有着一定的帮助，而硬标签则会对模型的精度有一定的损害。</p><p>剩下的<u><strong>消融实验</strong></u>就不写了，有点过于冗长了。</p><h2 id="Consistency-Regularization"><a href="#Consistency-Regularization" class="headerlink" title="Consistency Regularization"></a>Consistency Regularization</h2><p>说到一致（Consistency），其实很多代价都有这个内涵，如 MSE 代价，最小化预测与标签的差值，也就是希望预测与标签能够一致。其他的代价，如 KL 散度、交叉熵代价也类似。</p><p>所以一致性，是一种非常内在而本质的目标，可以让深度网络进行有效学习。 </p><p>在半监督领域中，未标记数据没有标签，所以需要让模型有个参照，从而通过这个参照从未标记数据中学习。</p><p><strong>Consistency Regularization</strong> 的主要思想是：相同的一张图片，通过模型所预测出来的结果应该是一样的。对于未标记样本，虽然没有对应的标签来使得模型判断预测结果是否准确，但是，却可以在原有的样本中添加一定的噪声，让模型通过比对预测结果来进行学习。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515151137355.png" alt="image-20230515151137355"></p><h3 id="π-model-and-Temporal-Ensembling"><a href="#π-model-and-Temporal-Ensembling" class="headerlink" title="π-model and Temporal Ensembling"></a>π-model and Temporal Ensembling</h3><p>论文地址：<a href="https://arxiv.org/abs/1610.02242">Temporal Ensembling for Semi-Supervised Learning</a> </p><h4 id="π-model"><a href="#π-model" class="headerlink" title="π-model"></a>π-model</h4><p>如下图所示，一个有标签样本（可视作无标签样本），经过随机图像增强输入网络，同时网络也会进行 Dropout 也可以视作噪声，输入两次得到两个结果 Z 和 Z’，将 Z 与图片真正标签 y 进行比对，使用 <code>交叉熵（cross-entropy）</code>计算损失 L1；将 Z 与 Z’ 使用 <code>平方差（squared difference）</code>计算损失 L2，然后将两个过程所得到的损失相加，但需要注意的是，两个损失分别占有一定权重，且占有的权重值会随着训练时间改变，在图 2 的所标记的损失函数公式可以看出。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515154058143.png" alt="image-20230515154058143"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515160459380.png" alt="image-20230515160459380"></p><p>其中 C 表示标记样本的的全部类别数量，w(t) 是随时间变化的加权函数，</p><p>并且，作者指出，第一部分的损失只针对有标签数据进行计算，而第二部分的损失则针对所有数据进行计算，即在算损失的时候，有标签数据两个损失项都用，无标签数据只用第二个损失项：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515161927609.png" alt="image-20230515161927609"></p><p>作者指出，使用 w(t) 时，由于前期模型差不多被有监督损失部分（标记数据样本）所支配，所以，在增长的前期，使权重增长的尽可能慢一点。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515163443077.png" alt="image-20230515163443077"></p><p>总结流程就是，数据集：有标签/无标签图片，分别两次输入模型，其中，L1只在有标签数据参与训练的过程中用于计算损失，L2 则是都有参与计算损失（有标签和无标签）。模型的噪音来自于：输入图像的随机增强，以及模型训练过程中的 Dropout。计算损失公式：L1 使用<u><strong>交叉熵</strong></u>来计算损失，L2 使用<u><strong>平方差公式</strong></u>来计算损失。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516131038965.png" alt="image-20230516131038965"></p><h4 id="Temporal-Ensembling"><a href="#Temporal-Ensembling" class="headerlink" title="Temporal Ensembling"></a>Temporal Ensembling</h4><p>由于以这种方式获得的培训目标是基于对网络的单一评估，因此可以预期它们会很嘈杂。暂时性结合通过将多个先前网络评估的预测汇总为整体预测来减轻这种情况</p><p>在 π-model 中，模型的训练结果都只是对网络进行单一评估，预测结果没有很强的关联性，这样子就造成了模型变得很“嘈杂”，并且，直觉认为，很难不相信过去预测结果与现在的预测结果不存在联系，这就是 π-model 的缺点。</p><p>在 Temporal Ensembling 中就很好的解决了这一个问题，在计算第二部分的损失中，生成的 Z’ 要参与下一个 epoch 的损失计算，（注意是每一个 epoch，而不是每一个 batch，这种改变其实是非常缓慢）。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515154108557.png" alt="image-20230515154108557"></p><p>每训练完一个 epoch 后，就会将 Z 进行更新：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515165329913.png" alt="image-20230515165329913"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515154426776.png" alt="image-20230515154426776"></p><p>总结流程，不同于 π-model，Temporal Ensembling 只进行一次模型预测，π-model 中的第二次训练预测，前一个 epoch 保存下来的 Z 与当前 epoch 的 z 进行指数滑动平均（Exponential Moving Average，EMA）运算得到，其中 z’ 是当前 epoch 的模型预测，参与 L2 的损失计算。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516132020990.png" alt="image-20230516132020990"></p><h3 id="Mean-Teacher"><a href="#Mean-Teacher" class="headerlink" title="Mean Teacher"></a><a href="https://arxiv.org/abs/1703.01780">Mean Teacher</a></h3><p>论文的关键思想是使用两个模型一个叫做<code>Student</code>，另一个叫做<code>Teacher</code>，其中Student 模型是一个带有 <u><strong>Dropout</strong></u> 的标准网络模型，与上文中所提到的 <u><strong>Noise Student</strong></u> 不同的是，该篇论文中的 Teacher 模型的架构与 Student 模型一致，且更新参数时是由 Student 模型的参数作<u><strong>指数滑动平均</strong></u>计算得到。计算损失的方式与 <u><strong>Temporal Ensembling</strong></u>相似/一致，有标签数据使用两个损失项，而无标签数据使用 Student 和 Teacher 模型得到的结果计算一致性损失，最后模型的总损失是由两个部分的损失得到。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516162131724.png" alt="image-20230516162131724"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516165459330.png" alt="image-20230516165459330"></p><p>使用指数滑动平均来更新参数后，模型可以在每一个 step 而不是在每一个 epoch来聚合信息，这大大提高了推断效率。并且，模型的每一层输出都得到了提升，而不是仅仅是在最后一层输出结果上得到改善，所以运用指数滑动平均，使得目标模型有了更好的<code>中间表示（intermediate representations）</code>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516170114727.png" alt="image-20230516170114727"></p><p>Teacher 模型参数 θ 的更新公式如下所示，在论文中将模型参数 θ 视作<u><strong>常量</strong></u>，不参与模型训练更新，而是在每一个 step 中按照公式进行迭代更新。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516183523851.png" alt="image-20230516183523851"></p><p>使用三种类型的噪声：</p><ul><li><p>对输入图像输入图像作随机平移和水平翻转</p><p><code>Translation Randomly {∆x, ∆y} ∼ [−2, 2]</code></p><p><code>Horizontal flip Randomly p = 0.5</code></p></li><li><p>Gaussian noise on the input layer</p><p><code>Gaussian noise σ = 0.15</code></p></li><li><p>Dropout</p><p><code>Dropout p = 0.5</code></p></li></ul><p>并且，计算一致性损失（ consistency cost）与上述所提到的模型不同，使用的是<code>均方误差(mean squared error)</code>，</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516185627093.png" alt="image-20230516185627093"></p><p>所使用的初始网络框架为：</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516195214426.png" alt="image-20230516195214426" style="zoom: 50%;"><h3 id="Virtual-Adversarial-Training"><a href="#Virtual-Adversarial-Training" class="headerlink" title="Virtual Adversarial Training"></a><a href="https://arxiv.org/abs/1704.03976">Virtual Adversarial Training</a></h3><p>由于有相关公式推导，且篇幅可能会很长（这篇论文理论部分好多…），故单独放在另一篇<a href="https://ytz7.github.io/posts/cc87ae2e.html">博客</a>中。</p><h3 id="Unsupervised-Data-Augmentation"><a href="#Unsupervised-Data-Augmentation" class="headerlink" title="Unsupervised Data Augmentation"></a><a href="https://arxiv.org/abs/1904.12848">Unsupervised Data Augmentation</a></h3><p>该篇论文的关键 idea 是使用 <code>AutoAugment</code> 创建一个未标记的图像的增强版本，使用最小化无标签数据增广数据和无标签数据的 KL 散度，如下图所示，具体思想和半监督学习中其他论文所使用的方法很相似，新鲜点就是论文的增强方式，对待不同任务使用不同的增强方式，且效果都还不错。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230518191734446.png" alt="image-20230518191734446"></p><p>具体细节可查阅<a href="https://ytz7.github.io/posts/77eb0ac8.html">博客</a>。</p><blockquote><p>520 这天还在工位，真实咸鱼一条了 (#｀-_ゝ-) ，但今天见证历史了额，居然不看题解 LeetCode AC 了一道困难题，哈哈哈😊，然后就奖励了自己看一下午的视频（堕落了一下午），shit… 看论文看论文~~</p></blockquote><h2 id="混合方法（Hybrid-Methods）"><a href="#混合方法（Hybrid-Methods）" class="headerlink" title="混合方法（Hybrid Methods）"></a>混合方法（Hybrid Methods）</h2><p>看名字就知道，混合混合，就是把前人所提出的一些方法就行合并，以及添加一些其他组件来提高性能。</p><h3 id="MixMatch"><a href="#MixMatch" class="headerlink" title="MixMatch"></a><a href="https://arxiv.org/abs/1905.02249">MixMatch</a></h3><p>主要工作是在生成数据上做了很大的功夫，但感觉还是很工程…生成数据的算法如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520200836920.png" alt="image-20230520200836920"></p><p>对于有标签的数据，对其作一次增强，对于无标签的数据，对其作 K 个不同的增强，生成 K 个对应增强后的数据，并且对 K 个生成后的无标签数据做预测，预测后将其预测分布取平均后的分布作一次<u><strong>锐化</strong></u>，，作为 K 个无标签数据的对应分布，或者可以称之为最终的<u><strong>伪标签</strong></u>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520210753737.png" alt="image-20230520210753737"></p><p>得到有标签数据增强后的结果 <code>X^(x, p)</code>，和无标签数据生成后带有伪标签的数据<code>U^(u, q)</code>，过程如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520201404911.png" alt="image-20230520201404911"></p><p>然后将增强后的数据 <code>X</code> 和  <code>U</code> 混合后做一次 <u><strong>Shuffle</strong></u>，得到样本集 <code>W</code>，其中前 N 项作为 <code>W_L</code>，剩余的 M 项为 <code>W_U</code>：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520202313703.png" alt="image-20230520202313703"></p><p>锐化公式如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520204542849.png" alt="image-20230520204542849"></p><p>然后将 <u><strong>MixUp</strong></u> 算法运用在上述集合之中，作以下操作：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520202419730.png" alt="image-20230520202419730"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520202432527.png" alt="image-20230520202432527"></p><p>其中  <u><strong><a href="https://arxiv.org/abs/1710.09412">MixUp</a></strong></u> 算法为图像增强算法，将两个图像按下面的公式生成新的图像：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520203113608.png" alt="image-20230520203113608"></p><p>为了让 MixUp 方法与损失项更兼容，作者对其做了一点小小的修改：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520212837377.png" alt="image-20230520212837377"></p><p>损失函数如下所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520203414452.png" alt="image-20230520203414452"></p><p>输入：<u><strong>相同 batch 大小</strong></u>有标签数据 <code>X</code> 和无标签数据 <code>U</code></p><p>其中 <code>T</code>，<code>K</code>，<code>α</code> 为模型的超参数，<code>α</code> 在 <u><strong>MixUp</strong></u> 中需要使用，<code>T</code> 在<u><strong>锐化</strong></u>中使用到， <code>K</code> 为 K 个不同的<u><strong>数据增强</strong></u>方法：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520204213042.png"></p><p>输出：增强后的结果 <code>X'</code> 和  <code>U'</code></p><p>然后计算有标签数据的损失 <code>L_X</code>，其中 <code>H</code> 为<u><strong>交叉熵</strong></u>，无标签数据的损失 <code>L_U</code>，使用 <u><strong>L2 损失</strong></u>，其中，无标签数据的损失项中 <code>L</code> 是总的标签数量（也就是输入的标签的维度）：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520204253594.png" alt="image-20230520204253594"></p><p>最后，将有标签数据的损失和无标签数据的损失合并，得到最终的损失，其中无监督的损失会对其施加一个权重 <code>λ_U</code>。</p><p>消融实验的结果：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230520220721107.png" alt="image-20230520220721107"></p><blockquote><p>OK，按照这个<a href="https://amitness.com/2020/07/semi-supervised-learning/">博主</a>关于半监督学习的综述，终于到达最后一篇经典论文了，不过有件事情一直想吐槽，就是 tensorflow 1. 怎么那么难用啊，本来说想复现一下 MixMatch 的实验，今天捣鼓了一整个下午，还是没有成功，气死我了，后面说是和 Cuda 的版本有冲突，tensorflow 1.15 的版本只能用 Cuda 10. ，服务器上又是 11. 的版本，奈何公共服务器都是大家一起用的，我怕搞得服务器崩了…索性算了，看下一篇去了，以后找论文源码的时候，应该要先找 Pytorch 版本的，不然搞这 tensorflow 又要浪费不少时间。</p><p>算了今天先不写了，会去跑步加和爸妈打打电话吧…</p><p>写于 2023/5/21.</p><p>就写了个开头 😂😅</p></blockquote><h3 id="FixMatch"><a href="#FixMatch" class="headerlink" title="FixMatch"></a><a href="https://arxiv.org/abs/2001.07685">FixMatch</a></h3><p>论文的方法是将先前所提到的<u><strong>伪标签（pseudo-labeling）</strong></u>和<u><strong>一致性正则化（consistency regularization）</strong></u>结合在一起，但又大大简化了整体方法，如下图所示，使用<u><strong>交叉熵</strong></u>在有标签数据上训练一个模型，损失记为 <code>L1</code>，对每一张未标记数据，做一次弱<u><strong>弱增强</strong></u>和<u><strong>强增强</strong></u>，对<u><strong>弱增强</strong></u>后的样本做预测，如果预测出的最大概率大于设置的阈值，那么将这个最大概率对于的类别作为<u><strong>弱增强后的样本的伪标签</strong></u>，即最后的输出分布为 <u><strong>One-Hot 编码</strong></u>，然后将弱增强和强增强的无标签数据对应的标签做一次<u><strong>交叉熵</strong></u>，得到损失 <code>L2</code>，最后将两个损失加权得到最终的损失 <code>L</code>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522130017362.png" alt="image-20230522130017362"></p><p>算法步骤：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230522163328704.png" alt="image-20230522163328704"></p><p>具体细节可查阅<a href="https://ytz7.github.io/posts/3680c2d7.html">博客</a>。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;半监督学习&quot;&gt;&lt;a href=&quot;#半监督学习&quot; class=&quot;headerlink&quot; title=&quot;半监督学习&quot;&gt;&lt;/a&gt;半监督学习&lt;/h1&gt;&lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;</summary>
      
    
    
    
    <category term="半监督学习" scheme="https://ytz7.github.io/categories/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="Semi-Surprised-Learning" scheme="https://ytz7.github.io/tags/Semi-Surprised-Learning/"/>
    
    <category term="机器学习" scheme="https://ytz7.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="个人随笔" scheme="https://ytz7.github.io/tags/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
</feed>
