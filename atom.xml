<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>七号zz の Blog</title>
  
  <subtitle>愿为未来做牛马。</subtitle>
  <link href="https://ytz7.github.io/atom.xml" rel="self"/>
  
  <link href="https://ytz7.github.io/"/>
  <updated>2023-05-16T12:28:08.489Z</updated>
  <id>https://ytz7.github.io/</id>
  
  <author>
    <name>七号zz</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="https://ytz7.github.io/posts/16107.html"/>
    <id>https://ytz7.github.io/posts/16107.html</id>
    <published>2023-05-16T12:28:08.489Z</published>
    <updated>2023-05-16T12:28:08.489Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>关于深度学习中遇到的知识盲区</title>
    <link href="https://ytz7.github.io/posts/eaa3f406.html"/>
    <id>https://ytz7.github.io/posts/eaa3f406.html</id>
    <published>2023-05-11T11:47:01.000Z</published>
    <updated>2023-05-16T12:28:08.489Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于深度学习的一些问题与知识盲区"><a href="#关于深度学习的一些问题与知识盲区" class="headerlink" title="关于深度学习的一些问题与知识盲区"></a>关于深度学习的一些问题与知识盲区</h1><p>此篇用于记录在阅读论文以及博客时遇到的一些问题和知识盲区，方便日后进行复盘。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="为什么需要激活函数？"><a href="#为什么需要激活函数？" class="headerlink" title="为什么需要激活函数？"></a><strong>为什么需要激活函数？</strong></h3><p>通常激活函数都是非线性的，它能够帮助我们引入非线性因素，使得神经网络能够更好地解决更加复杂地问题，在简单的二分类问题中，如果不使用激活函数，使用简单的**<u>逻辑回归</u>**，那么该模型只能作简单的线性分类，而不能作复杂的非线性划分，如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511195806300.png" alt="image-20230511195806300"></p><p>值得一提的是，如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，效果与单个神经元无异。另外，如果是拟合问题而不是分类问题，输出层的激活函数可以使用线性函数。</p><h3 id="Sigmoid-函数饱和带来的问题"><a href="#Sigmoid-函数饱和带来的问题" class="headerlink" title="Sigmoid 函数饱和带来的问题"></a>Sigmoid 函数饱和带来的问题</h3><p>Sigmoid 函数的取值范围在 (0,1) 之间，单调连续，求导容易，一般用于二分类神经网络的输出层。</p><p>Sigmoid 函数饱和区范围广，容易造成梯度消失。饱和区如下图所示，图中红色椭圆标注的饱和区曲线平缓，梯度的值很小，近似为零，且 Sigmoid 函数的饱和区范围很广，除了 [-5,5]，其余区域都可以认为是饱和区，这种情况很容易造成梯度消失，梯度消失会增大神经网络训练难度，影响神经网络模型的性能。</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511200333796.png" alt="image-20230511200333796" style="zoom:50%;"><h2 id="降噪自编码器-Denoising-Auto-Encoder"><a href="#降噪自编码器-Denoising-Auto-Encoder" class="headerlink" title="降噪自编码器 Denoising Auto-Encoder"></a>降噪自编码器 Denoising Auto-Encoder</h2><p>在神经网络模型训练阶段开始前，通过 Auto-encoder 对模型进行预训练可确定编码器 W 的初始参数值。然而，受模型复杂度、训练集数据量以及数据噪音等问题的影响，通过 Auto-encoder 得到的初始模型往往存在过拟合的风险。</p><p>简单理解，在人类的感知过程中，某些模态的信息对结果的判断影响并不大。举个例子，一块圆形的饼干和一块方形的饼干，在认知中同属于饼干这一类，因此形状对我们判断是否是饼干没有太大作用，也就是<strong>噪声</strong>。如果不能将形状数据去除掉，可能会产生“圆饼干是饼干，方饼干就不是饼干”的问题（过拟合）。</p><p>当采用无监督的方法分层预训练深度网络的权值时，为了学习到较鲁棒的特征，可以**<u>在网络的可视层（即数据的输入层）引入随机噪声</u>**，这种方法称为降噪自编码器（<a href="https://dl.acm.org/doi/abs/10.1145/1390156.1390294">Denoising Auto-Encoder， DAE</a>）。</p><blockquote><p>降噪自编码器：一个模型，能够从有噪音的原始数据作为输入，而能够恢复出真正的原始数据。这样的模型，更具有鲁棒性。</p></blockquote><p>以下是以经典的 MNIST 手写数字识别为例，对于输入的数据引入了变换角度、随机噪点、添加背景图像等噪音。模型通过训练后可以对有噪音图像更加鲁棒，而这也更符合实际使用的需求。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-2b0c0b5855f0c98b44fbeaf75d6a72dd_r.jpg" alt="img"></p><p>对于有噪音的输入数据，区别于一般自编码机，降噪自编码机要做的就是数据的降噪。关于降噪的过程如下图所示：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-9ab89078a4630eb52347244a61410c2f_r.jpg" alt="img"></p><p>对于输入层 $x$，以一定概率将其节点置 0，得到 $\hat{x}$,用 $\hat{x}$ 去计算 $y$,计算 $z$ ，并将 $z$ 与原始 $x$ 做误差迭代，对结果误差较小的节点可以认为是噪声。每层处理重复上述工作。</p><p>自编码器的本质是学习一个相等函数，即网络的输入和重构后的输出相等，这种相等函数的表示有个缺点就是当测试样本和训练样本不符合同一分布，效果不好，而降噪自编码器在这方面的处理有所进步。</p><h2 id="随机深度-Stochastic-Depth"><a href="#随机深度-Stochastic-Depth" class="headerlink" title="随机深度 Stochastic Depth"></a>随机深度 Stochastic Depth</h2><p>针对于残差模块的优化，由于很深的 ResNet 通常需要很长时间的训练(也就是训练很慢)，作者引入了一种类似于 dropout 的方法，在训练过程中随机丢弃子图层（randomly drop a subset of layers），而在推断时正常使用完整的图。</p><p>ResNet 网络是由一个接一个的残差模块(ResBlock)串联起来的，可以视为ResBlock的集合。在训练时，对每个 ResBlock 随机 drop（按伯努利分布），drop 就是将上一个 ResBlock 直接输出到下一个 ResBlock，被 drop 的 ResBlock 什么都不做也不更新。另外，网络的输入被视为第一层，是不会 drop 的。</p><p>与 Dropout 的不同之处在于，该方法 drop 整个 ResBlock，而 Dropout 在训练期间只 drop 一部分神经元节点。这种方法大大降低了训练时间，甚至在训练完成后删除部分layer，还能不影响精度。</p><h2 id="最小化信息熵-Entropy-Minimization"><a href="#最小化信息熵-Entropy-Minimization" class="headerlink" title="最小化信息熵 Entropy Minimization"></a>最小化信息熵 Entropy Minimization</h2><p>参考文章：<a href="https://kexue.fm/archives/5448">最小熵原理（一）</a></p><p>在半监督学习中，有标签（分类完全准确）的数据样本通常相对较少，通过训练模型对未标记数据样本进行预测，选择出高置信度的样本，作为标记样本同有标签样本作为下一次训练的数据样本。</p><p><u><strong>Entropy Minimization</strong></u> 是一种在半监督学习中使用的技术，它的目的是最小化信息熵，从而使模型在分类时的不确定性最小。在半监督学习中，我们希望模型尽可能地利用未标记数据来学习，但是这些数据并不带有正确的标签，因此我们需要利用某些技术来帮助我们学习这些数据。</p><p><u><strong>加快模型学习进度的唯一方法就是降低学习目标的冗余信息量</strong></u>，所提到的“去冗余”，可以理解为“省去没必要的学习成本”。</p><p>也就是通常所使用到的技巧：过滤掉低置信度的未标记样本，保留高置信度样本。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512164646319.png" alt="image-20230512164646319"></p><h2 id="Top-1-and-Top-5-Accuracy"><a href="#Top-1-and-Top-5-Accuracy" class="headerlink" title="Top-1 and Top-5 Accuracy"></a>Top-1 and Top-5 Accuracy</h2><p>Top-1：</p><p>在多分类问题中，一般认为在经过全连接层后得到的概率分布中概率最大的类别为我们模型的预测类别，就判断为正确。</p><p>Top-5：</p><p>在多分类问题中，一般认为在经过全连接层后得到的概率分布中概率最大的全五个类别中有我们模型的预测类别，就判断为正确。</p><h2 id="Dark-Knowledge"><a href="#Dark-Knowledge" class="headerlink" title="Dark Knowledge"></a>Dark Knowledge</h2><p>可以看作是经过 Softmax 函数后得到的各类类别的可能性，其包含着类别之间的相关性，比如，猫和狗的相似性，要远远大于猫和船的相似性，而这种相似性，会在概率值中有所体现，而这部分信息一致没有被很好的利用，所以称之为 Dark Knowledge。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;关于深度学习的一些问题与知识盲区&quot;&gt;&lt;a href=&quot;#关于深度学习的一些问题与知识盲区&quot; class=&quot;headerlink&quot; title=&quot;关于深度学习的一些问题与知识盲区&quot;&gt;&lt;/a&gt;关于深度学习的一些问题与知识盲区&lt;/h1&gt;&lt;p&gt;此篇用于记录在阅读论文以及博</summary>
      
    
    
    
    <category term="深度学习" scheme="https://ytz7.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://ytz7.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器学习" scheme="https://ytz7.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>test-blog</title>
    <link href="https://ytz7.github.io/posts/467ec823.html"/>
    <id>https://ytz7.github.io/posts/467ec823.html</id>
    <published>2023-05-11T07:42:17.000Z</published>
    <updated>2023-05-16T12:28:08.489Z</updated>
    
    <content type="html"><![CDATA[<h1 id="这是一篇测试博客"><a href="#这是一篇测试博客" class="headerlink" title="这是一篇测试博客"></a>这是一篇测试博客</h1><p>修改一下在上传试试…<br>在测试测试</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;这是一篇测试博客&quot;&gt;&lt;a href=&quot;#这是一篇测试博客&quot; class=&quot;headerlink&quot; title=&quot;这是一篇测试博客&quot;&gt;&lt;/a&gt;这是一篇测试博客&lt;/h1&gt;&lt;p&gt;修改一下在上传试试…&lt;br&gt;在测试测试&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="test" scheme="https://ytz7.github.io/tags/test/"/>
    
  </entry>
  
  <entry>
    <title>半监督学习学习过程</title>
    <link href="https://ytz7.github.io/posts/52ee1a8c.html"/>
    <id>https://ytz7.github.io/posts/52ee1a8c.html</id>
    <published>2023-05-11T07:08:40.000Z</published>
    <updated>2023-05-16T12:28:08.489Z</updated>
    
    <content type="html"><![CDATA[<h1 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h1><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>学习了快一个学期的 Java 程序开发，让我感觉开发的过程真的好空洞…但是我并不知道下一步到底要做什么，又处于想找实习又怕找不到的情况下，刷了几天 LeetCode，然后又变成了“小🐏人”，紧接着被隔离😅，真的挺痛苦，所幸在第三天就“出狱”了，在蹲“监狱”期间，受到一个“狱友”的启发，“还是要做好职业规划的，若是没有做好职业规划，像一只无头苍蝇一样，很可能在毕业之际还是找不到工作。”在思考了一天后，我决定重新开始在“半监督学习”这个领域开始零基础学习了，为什么会有这个想法？是因为我觉得我起初考研的目的是为了能够在人工智能这个领域继续深入了解，目的很单纯，那为什么我不坚持以下呢？于是我决定重新开始这个计划，管他呢，起码是自己喜欢做的事情，哪怕未来找不到工作什么的，起码现在是由试错的资本的。</p><p>以下将会记录我在学习时候遇到的问题/解决方式/新思路/感想。</p><h2 id="Self-Training"><a href="#Self-Training" class="headerlink" title="Self-Training"></a>Self-Training</h2><p>自训练方法，模型基于已标记好的训练集进行训练，得到一个基础模型，利用该基础模型对未标记的数据集进行预测一个<strong>伪标签</strong>，然后将两个数据集整合训练，得到一个新的模型，从而迭代更新模型参数，生成一个最优模型。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161304502.png" alt="image-20230511161304502"></p><h3 id="Pseudo-Label"><a href="#Pseudo-Label" class="headerlink" title="Pseudo-Label"></a><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.664.3543&amp;rep=rep1&amp;type=pdf">Pseudo-Label</a></h3><p>伪标签技术适用于小样本学习，实际上在样本极其珍贵的金融、医疗图像、安全等领域，伪标签学习有时候很有效。</p><p>伪标签的定义来自于半监督学习，半监督学习的核心思想是<u><strong>通过借助无标签的数据来提升有监督过程中的模型性能</strong></u>。</p><p>粗略来讲，伪标签技术就是<u><strong>利用在已标注数据所训练的模型在未标注的数据上进行预测，根据预测结果对样本进行筛选，再次输入模型中进行训练的一个过程</strong></u>。</p><p>如下图所示，利用有标签的数据集训练出一个模型，运用训练出的模型给予无标签数据一个<u><strong>伪标签</strong></u>。如何定义所属类别？利用训练好的模型对无标签数据进行预测，以概率最高的类别作为无标签数据的伪标签。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161425308.png" alt="image-20230511161425308"></p><blockquote><p>entropy regularization：用于防止模型过拟合，通过在损失函数中加入熵（entropy）项来实现</p></blockquote><p>利用 <em>entropy regularization</em> 思想，将无监督数据转为目标数据的正则项，即将拥有伪标签的无标签数据视为有标签的数据，利用交叉熵（与最初训练模型一致）来评估误差大小。</p><p>模型整体的目标函数如下：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-103a30551a86aa99222f8f35129088d7_r.jpg" alt="img"></p><p>其中左边一项为交叉熵，用来评估有标签数据的误差，右边一项即为 <em>entropy regularization</em> 项，用来从无标签的数据中获取训练信号。</p><p>为了平衡有标签数据和无标签数据的信号强度，如上所示，算法在目标函数中引入了时变参数 α(t)，其数学形式如下，其中 T1 和 T2 都为超参数：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/v2-4d5a8a1d8aa4b9037859ab19dd346c0b_r.jpg" alt="img"></p><p>因此，随着训练时间的增加，α(t) 将会从零开始线性增长至某个饱和值，对应无标签数据的信号也将逐渐释放出来。背后的核心想法也很直观，早期模型预测效果不佳，因此 <em>entropy regularization</em> 产生信号的误差也较大，因而 α(t) 应该从零开始，由小逐渐增大。</p><p><strong>存在不足：</strong>只在训练时间这个维度上，采用了退火思想，即采用时变系数α(t)。而在伪标签这个维度，对于模型给予的预测标签一视同仁，这种方法在实际中存在明显问题。若模型在对伪标签的数据预测后， 10 个类别预测概率值都接近于 0.1，以最大概率这一原则选择对应的标签，那么这个标签对模型的训练会造成一定的<u><strong>副作用</strong></u>。</p><p><strong>设想如何突破这一不足？</strong></p><p>也许可以设定一个阈值，抛弃那些预测最大概率值小于该阈值的未标记样本，将满足条件的未标记样本分配伪标签，并加入模型评估当中，之后再迭代训练。</p><h3 id="Noisy-Student"><a href="#Noisy-Student" class="headerlink" title="Noisy Student"></a><a href="https://arxiv.org/abs/1911.04252">Noisy Student</a></h3><p>论文的关键 idea 是训练两个模型，“teacher”和“student”，强调的是在student模型中加入噪声，teacher 模型和 student 模型可以用不同的模型训练，也可以使用相同的模型。</p><p>在有标签数据中训练“teacher”模型，并利用该模型对未标记数据进行推断伪标签，这些伪标签可以是<u><strong>软标签</strong></u>，也可以取其最大概率的类别将其转换为<u><strong>硬标签</strong></u>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511161507462.png" alt="image-20230511161507462"></p><p>然后将标记数据与为标记数据（带有伪标签）置入“student”进行训练，在训练之前<u><strong>数据增强</strong></u>使用 <strong>RandAugment</strong>，待“student”模型训练好后，使用最新的模型作为新的“teacher”，进行下一次迭代，此过程会重复几次（通常为 3 次）。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230511211613877.png" alt="image-20230511211613877"></p><p>总结该篇论文的流程思路：</p><ol><li>首先将在 ImageNet 上训练好的模型作为 Teacher Network 来训练 Student Network</li><li>再使用训练好的 T 网络（无噪音）来对另一个数据集 [JFT dataset] 生成尽可能准确的伪标签</li><li>之后使用生成伪标签的数据集 [JFT dataset] 和 ImageNet 一起训练 Student Network</li><li>Student Network中增加了模型噪音<ul><li>Dropout</li><li>随机深度  Stochastic Depth</li><li>数据噪音：对图片进行数据增广（RandAugment）</li></ul></li></ol><p>对 Student 模型添加噪音的作用：</p><ol><li><strong>数据噪音</strong>：提高泛化能力</li><li><strong>模型噪音</strong>：提高模型鲁棒性和泛化能力</li></ol><p>具体参数设置：</p><ol><li><strong>Stochastic Depth</strong>：幸存概率因子为 0.8</li><li><strong>Dropout</strong>：分类层（final layer）引入 0.5 的丢弃率</li><li><strong>RandAugment</strong>：应用两个随机计算，其震级设置为 27</li></ol><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512204307583.png" alt="image-20230512204307583"></p><p>其他 Tricks：</p><ol><li><strong>数据过滤</strong>：将教师模型中置信度不高的图片过滤，因为这通常代表着域外图像（out-of-domain data）</li><li><strong>数据平衡</strong>：平衡不同类别的图片数量，当一个类别所对应的图片数量不是很充足时，会采取<u>随机复制</u>的方法来扩充样本量</li><li><strong>软标签（Soft Pseudo-Label）</strong>：在消融实验中表示，软标签对域外图像有更强的指导作用</li></ol><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p>1.噪音是否对模型有影响？（The Importance of Noise in Self-training）</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512205434466.png" alt="image-20230512205434466" style="zoom:80%;"><p>从结果可以看出，<u>噪音</u>、<u>随机深度</u>、<u>数据扩充</u>起着重要的作用使学生模型胜过教师模型，对此有人提出是不是对未标记数据加入正则项以防止过拟合来代替噪音，作者在实验中说明这是不对的。因为在去噪的情况下，未标记图像的训练损失并没有下降多少以此说明模型并没有对未标记数据过拟合。</p><p>2.对于迭代训练的消融实验（A Study of Iterative Training）</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512205650571.png" alt="image-20230512205650571" style="zoom:80%;"><p>作者先在<u><strong>标记数据</strong></u>上训练了 EfficientNet-B7 作为 Teacher，然后再训练 EfficientNet-L2 作为 Student，然后让 Student 作为 Teacher 依次迭代三轮，作者表明，迭代训练提高了准确度，并且，给出再最后通过调整未标记图像和标记图像的比为 <strong>1 : 28</strong> 时达到最优 Top-1 Acc.。</p><p>3.能力强的教师模型会不会对学生模型造成的影响</p><p>4.无标签的数据量大小</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512210409208.png" alt="image-20230512210409208" style="zoom:80%;"><p>作者按照比例分别从整个数据集中<u><strong>均匀采样（uniformly sampling）</strong></u>，会发现在数据量减少至 1/16 中，模型的性能大都相似，在数据量达到 1/32 或更小后，模型性能有了显著的下降（可能 .3 个点就可以算是显著的下降了吧…），所以，使用大量未标记的数据会产生更好的性能，但作者指出：<u>对于大模型来说，数据量越多越好，而小模型由于容量限制则很容易饱和</u>。</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512210908139.png" alt="image-20230512210908139" style="zoom:80%;"><p>5.硬标签和软标签对域外图像的影响</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230512210946955.png" alt="image-20230512210946955" style="zoom:80%;"><p>作者将预测置信度高（high-confidence）的图像视作域内图像（in-domain images），反之，将预测置信度低（low-confidence）的图像视作域外图像（out-of-domain images），作者表明：对于域内图像，软伪标签（ soft pseudo labels）和硬伪标签（hard pseudo labels）都对模型有一定的帮助；而对于域外图像，软伪标签使得模型对域外图像的判断有着一定的帮助，而硬标签则会对模型的精度有一定的损害。</p><p>剩下的<u><strong>消融实验</strong></u>就不写了，有点过于冗长了。</p><h2 id="Consistency-Regularization"><a href="#Consistency-Regularization" class="headerlink" title="Consistency Regularization"></a>Consistency Regularization</h2><p>说到一致（Consistency），其实很多代价都有这个内涵，如 MSE 代价，最小化预测与标签的差值，也就是希望预测与标签能够一致。其他的代价，如 KL 散度、交叉熵代价也类似。</p><p>所以一致性，是一种非常内在而本质的目标，可以让深度网络进行有效学习。 </p><p>在半监督领域中，未标记数据没有标签，所以需要让模型有个参照，从而通过这个参照从未标记数据中学习。</p><p><strong>Consistency Regularization</strong> 的主要思想是：相同的一张图片，通过模型所预测出来的结果应该是一样的。对于未标记样本，虽然没有对应的标签来使得模型判断预测结果是否准确，但是，却可以在原有的样本中添加一定的噪声，让模型通过比对预测结果来进行学习。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515151137355.png" alt="image-20230515151137355"></p><h3 id="π-model-and-Temporal-Ensembling"><a href="#π-model-and-Temporal-Ensembling" class="headerlink" title="π-model and Temporal Ensembling"></a>π-model and Temporal Ensembling</h3><p>论文地址：<a href="https://arxiv.org/abs/1610.02242">Temporal Ensembling for Semi-Supervised Learning</a> </p><h4 id="π-model"><a href="#π-model" class="headerlink" title="π-model"></a>π-model</h4><p>如下图所示，一个有标签样本（可视作无标签样本），经过随机图像增强输入网络，同时网络也会进行 Dropout 也可以视作噪声，输入两次得到两个结果 Z 和 Z’，将 Z 与图片真正标签 y 进行比对，使用 <code>交叉熵（cross-entropy）</code>计算损失 L1；将 Z 与 Z’ 使用 <code>平方差（squared difference）</code>计算损失 L2，然后将两个过程所得到的损失相加，但需要注意的是，两个损失分别占有一定权重，且占有的权重值会随着训练时间改变，在图 2 的所标记的损失函数公式可以看出。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515154058143.png" alt="image-20230515154058143"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515160459380.png" alt="image-20230515160459380"></p><p>其中 C 表示标记样本的的全部类别数量，w(t) 是随时间变化的加权函数，</p><p>并且，作者指出，第一部分的损失只针对有标签数据进行计算，而第二部分的损失则针对所有数据进行计算，即在算损失的时候，有标签数据两个损失项都用，无标签数据只用第二个损失项：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515161927609.png" alt="image-20230515161927609"></p><p>作者指出，使用 w(t) 时，由于前期模型差不多被有监督损失部分（标记数据样本）所支配，所以，在增长的前期，使权重增长的尽可能慢一点。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515163443077.png" alt="image-20230515163443077"></p><p>总结流程就是，数据集：有标签/无标签图片，分别两次输入模型，其中，L1只在有标签数据参与训练的过程中用于计算损失，L2 则是都有参与计算损失（有标签和无标签）。模型的噪音来自于：输入图像的随机增强，以及模型训练过程中的 Dropout。计算损失公式：L1 使用<u><strong>交叉熵</strong></u>来计算损失，L2 使用<u><strong>平方差公式</strong></u>来计算损失。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516131038965.png" alt="image-20230516131038965"></p><h4 id="Temporal-Ensembling"><a href="#Temporal-Ensembling" class="headerlink" title="Temporal Ensembling"></a>Temporal Ensembling</h4><p>由于以这种方式获得的培训目标是基于对网络的单一评估，因此可以预期它们会很嘈杂。暂时性结合通过将多个先前网络评估的预测汇总为整体预测来减轻这种情况</p><p>在 π-model 中，模型的训练结果都只是对网络进行单一评估，预测结果没有很强的关联性，这样子就造成了模型变得很“嘈杂”，并且，直觉认为，很难不相信过去预测结果与现在的预测结果不存在联系，这就是 π-model 的缺点。</p><p>在 Temporal Ensembling 中就很好的解决了这一个问题，在计算第二部分的损失中，生成的 Z’ 要参与下一个 epoch 的损失计算，（注意是每一个 epoch，而不是每一个 batch，这种改变其实是非常缓慢）。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515154108557.png" alt="image-20230515154108557"></p><p>每训练完一个 epoch 后，就会将 Z 进行更新：</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515165329913.png" alt="image-20230515165329913"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230515154426776.png" alt="image-20230515154426776"></p><p>总结流程，不同于 π-model，Temporal Ensembling 只进行一次模型预测，π-model 中的第二次训练预测，前一个 epoch 保存下来的 Z 与当前 epoch 的 z 进行指数滑动平均（Exponential Moving Average，EMA）运算得到，其中 z’ 是当前 epoch 的模型预测，参与 L2 的损失计算。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516132020990.png" alt="image-20230516132020990"></p><h3 id="Mean-Teacher"><a href="#Mean-Teacher" class="headerlink" title="Mean Teacher"></a><a href="https://arxiv.org/abs/1703.01780">Mean Teacher</a></h3><p>论文的关键思想是使用两个模型一个叫做<code>Student</code>，另一个叫做<code>Teacher</code>，其中Student 模型是一个带有 <u><strong>Dropout</strong></u> 的标准网络模型，与上文中所提到的 <u><strong>Noise Student</strong></u> 不同的是，该篇论文中的 Teacher 模型的架构与 Student 模型一致，且更新参数时是由 Student 模型的参数作<u><strong>指数滑动平均</strong></u>计算得到。计算损失的方式与 <u><strong>Temporal Ensembling</strong></u>相似/一致，有标签数据使用两个损失项，而无标签数据使用 Student 和 Teacher 模型得到的结果计算一致性损失，最后模型的总损失是由两个部分的损失得到。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516162131724.png" alt="image-20230516162131724"></p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516165459330.png" alt="image-20230516165459330"></p><p>使用指数滑动平均来更新参数后，模型可以在每一个 step 而不是在每一个 epoch来聚合信息，这大大提高了推断效率。并且，模型的每一层输出都得到了提升，而不是仅仅是在最后一层输出结果上得到改善，所以运用指数滑动平均，使得目标模型有了更好的<code>中间表示（intermediate representations）</code>。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516170114727.png" alt="image-20230516170114727"></p><p>Teacher 模型参数 θ 的更新公式如下所示，在论文中将模型参数 θ 视作<u><strong>常量</strong></u>，不参与模型训练更新，而是在每一个 step 中按照公式进行迭代更新。</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516183523851.png" alt="image-20230516183523851"></p><p>使用三种类型的噪声：</p><ul><li><p>对输入图像输入图像作随机平移和水平翻转</p><p><code>Translation Randomly {∆x, ∆y} ∼ [−2, 2]</code></p><p><code>Horizontal flip Randomly p = 0.5</code></p></li><li><p>Gaussian noise on the input layer</p><p><code>Gaussian noise σ = 0.15</code></p></li><li><p>Dropout</p><p><code>Dropout p = 0.5</code></p></li></ul><p>并且，计算一致性损失（ consistency cost）与上述所提到的模型不同，使用的是<code>均方误差(mean squared error)</code>，</p><p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516185627093.png" alt="image-20230516185627093"></p><p>所使用的初始网络框架为：</p><img src="https://ytz-blog-bucket.oss-cn-guangzhou.aliyuncs.com/typora-markdown/images/image-20230516195214426.png" alt="image-20230516195214426" style="zoom: 50%;">]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;半监督学习&quot;&gt;&lt;a href=&quot;#半监督学习&quot; class=&quot;headerlink&quot; title=&quot;半监督学习&quot;&gt;&lt;/a&gt;半监督学习&lt;/h1&gt;&lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;</summary>
      
    
    
    
    <category term="半监督学习" scheme="https://ytz7.github.io/categories/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://ytz7.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="个人随笔" scheme="https://ytz7.github.io/tags/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/"/>
    
    <category term="Semi-Suprised-Learning" scheme="https://ytz7.github.io/tags/Semi-Suprised-Learning/"/>
    
  </entry>
  
</feed>
